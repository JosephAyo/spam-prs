{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babdda8-11f7-485e-bad8-d8c5b8eebb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Read tokens from a text file\n",
    "tokens_file = \"./env/tokens.txt\"\n",
    "with open(tokens_file, \"r\") as file:\n",
    "    tokens = file.read().splitlines()\n",
    "\n",
    "# Create an iterator to cycle through the tokens\n",
    "token_iterator = itertools.cycle(tokens)\n",
    "current_token = next(token_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c281d1-993b-497b-8984-0e32ac6a61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of User-Agents for randomization\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "]\n",
    "\n",
    "# Define headers to authenticate using the first token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {current_token}\",\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "}\n",
    "\n",
    "# Setup GraphQL endpoint and client\n",
    "graphql_url = \"https://api.github.com/graphql\"\n",
    "transport = RequestsHTTPTransport(url=graphql_url, headers=headers, use_json=True)\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837bc28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_activity(activity: str):\n",
    "    log = f\"{datetime.datetime.now()}: {activity}\\n\"\n",
    "    # print(log)\n",
    "    with open(\"non-spam-prs-output.log\", \"a\") as log_file:\n",
    "        log_file.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062f1b0-a9df-4f94-bb18-56f42774bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all tokens to verify their validity\n",
    "def test_all_tokens():\n",
    "    test_query = gql(\n",
    "        \"\"\"\n",
    "        {\n",
    "          viewer {\n",
    "            login\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    for i, token in enumerate(tokens):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"User-Agent\": random.choice(user_agents),\n",
    "        }\n",
    "        transport = RequestsHTTPTransport(\n",
    "            url=graphql_url, headers=headers, use_json=True\n",
    "        )\n",
    "        client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "        try:\n",
    "            response = client.execute(test_query)\n",
    "            log_activity(\n",
    "                f\"Token {i+1}/{len(tokens)} is valid. Logged in as: {response['viewer']['login']}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log_activity(f\"Token {i+1}/{len(tokens)} failed with error: {e}\")\n",
    "\n",
    "\n",
    "# Run the token validation\n",
    "test_all_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd85df0-71b7-46c8-9ed4-eceb724201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphQL query\n",
    "query_template = gql(\n",
    "    \"\"\"\n",
    "    query searchIssues($keyword: String!, $afterCursor: String, $first: Int) {\n",
    "      search(query: $keyword, type: ISSUE, first: $first, after: $afterCursor) {\n",
    "        issueCount\n",
    "        edges {\n",
    "          cursor\n",
    "          node {\n",
    "            ... on PullRequest {\n",
    "              id\n",
    "              number\n",
    "              title\n",
    "              url\n",
    "              comments(first: 100) {\n",
    "                totalCount # it still gives the total count regardless of the first parameter\n",
    "                edges {\n",
    "                  node {   \n",
    "                    author { ... on User { login } }\n",
    "                    editor { ... on User { login } }\n",
    "                    body\n",
    "                    createdAt\n",
    "                    lastEditedAt\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              state\n",
    "              closed\n",
    "              merged\n",
    "              createdAt\n",
    "              updatedAt\n",
    "              mergeCommit {\n",
    "                oid\n",
    "              }\n",
    "              timeline(last: 100) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    __typename\n",
    "                    ... on LabeledEvent {\n",
    "                      actor { ... on User { login } }\n",
    "                      label { ... on Label { name }}\n",
    "                      createdAt\n",
    "                    }\n",
    "                    ... on ClosedEvent { \n",
    "                      actor { ... on User { login } }\n",
    "                      createdAt\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              commits {\n",
    "                totalCount\n",
    "              }\n",
    "              changedFiles\n",
    "              headRefName\n",
    "              baseRefName\n",
    "              repository {\n",
    "                id\n",
    "                nameWithOwner\n",
    "                stargazerCount\n",
    "                description\n",
    "                codeOfConduct {\n",
    "                  body\n",
    "                  id\n",
    "                  name\n",
    "                  url\n",
    "                }\n",
    "                homepageUrl\n",
    "                assignableUsers(first: 100) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      login\n",
    "                      url\n",
    "                      bio\n",
    "                      company\n",
    "                    }\n",
    "                  }\n",
    "                  totalCount\n",
    "                }\n",
    "                mentionableUsers(first: 100) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      login\n",
    "                      url\n",
    "                      bio\n",
    "                      company\n",
    "                    }\n",
    "                  }\n",
    "                  totalCount\n",
    "                }\n",
    "                forkCount\n",
    "                watchers {\n",
    "                  totalCount\n",
    "                }\n",
    "                isFork\n",
    "                languages(first: 20) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      name\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              author {\n",
    "                ... on User {\n",
    "                  login\n",
    "                  url\n",
    "                  createdAt\n",
    "                  repositories {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  followers {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  following {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  repositoryDiscussions {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  repositoryDiscussionComments {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  organizations (first: 20){\n",
    "                    edges {\n",
    "                      node {\n",
    "                        name\n",
    "                        login\n",
    "                        url\n",
    "                        membersWithRole {\n",
    "                          totalCount\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              labels(first: 10) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    name\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              body\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        pageInfo {\n",
    "          endCursor\n",
    "          hasNextPage\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b32e41-7d3f-458b-ad0f-0c13d3146813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_contributor_count(repo_owner, repo_name):\n",
    "#     global current_token\n",
    "#     max_retries = 3\n",
    "#     retries = 0\n",
    "#     while retries < max_retries:\n",
    "#         try:\n",
    "#             # Randomize User-Agent for each query\n",
    "#             headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "#             headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "#             url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contributors?per_page=1&anon=true\"\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             if response.status_code == 200:\n",
    "#                 return int(response.headers.get(\"Link\", \"\").split(\",\")[-1].split(\"&page=\")[-1].split(\">\")[0]) if \"Link\" in response.headers else len(response.json())\n",
    "#             elif response.status_code == 403:\n",
    "#                 log_activity(f\"Rate limit exceeded, switching token... (Attempt {retries + 1}/{max_retries})\")\n",
    "#                 current_token = next(token_iterator)\n",
    "#                 retries += 1\n",
    "#             else:\n",
    "#                 response.raise_for_status()\n",
    "#         except Exception as e:\n",
    "#             log_activity(f\"Error: {e}, retrying... (Attempt {retries + 1}/{max_retries})\")\n",
    "#             retries += 1\n",
    "#     raise Exception(\"Max retries reached. Unable to complete the request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c2427-7797-4088-a144-6045c87ee349",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.headers = headers\n",
    "# Check rate limit before executing the main query\n",
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "rate_limit_response = client.execute(rate_limit_query)\n",
    "log_activity(f\"Rate limit: {rate_limit_response['rateLimit']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acb9fd-7a25-458d-aec7-8f302a653c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def execute_query(keyword, first=100, after_cursor=None):\n",
    "    global current_token\n",
    "    log_activity(\n",
    "        f\"Executing query with keyword: {keyword}, first: {first}, afterCursor: {after_cursor}\"\n",
    "    )\n",
    "    while True:\n",
    "        try:\n",
    "            # Randomize User-Agent for each query\n",
    "            headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "            transport.headers = headers\n",
    "            # Check rate limit before executing the main query\n",
    "            rate_limit_response = client.execute(rate_limit_query)\n",
    "            remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "            if remaining < 100:\n",
    "                log_activity(\n",
    "                    f\"Rate limit remaining ({remaining}) is below threshold. Switching token...\"\n",
    "                )\n",
    "                # Set up to track whether we have cycled through all tokens\n",
    "                all_tokens_checked = False\n",
    "                initial_token = current_token\n",
    "\n",
    "                while not all_tokens_checked:\n",
    "                    # Switch to the next token\n",
    "                    current_token = next(token_iterator)\n",
    "                    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "                    transport.headers = headers\n",
    "\n",
    "                    # Check the rate limit of the new token\n",
    "                    rate_limit_response = client.execute(rate_limit_query)\n",
    "                    remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "\n",
    "                    if remaining >= 100:\n",
    "                        log_activity(\n",
    "                            f\"Switched to a new token with sufficient rate limit ({remaining} remaining).\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "                    # Check if we have cycled through all tokens\n",
    "                    if current_token == initial_token:\n",
    "                        log_activity(\"All tokens are below threshold. Waiting for 1 hour...\")\n",
    "                        time.sleep(3600)\n",
    "                        all_tokens_checked = True\n",
    "\n",
    "                continue\n",
    "            return client.execute(\n",
    "                query_template,\n",
    "                variable_values={\n",
    "                    \"keyword\": keyword,\n",
    "                    \"first\": first,\n",
    "                    \"afterCursor\": after_cursor,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"API rate limit\" in str(e):\n",
    "                log_activity(\n",
    "                    f\"Rate limit reached: {e}, switching token... (Attempt with first {first})\"\n",
    "                )\n",
    "                current_token = next(token_iterator)\n",
    "                headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "            else:\n",
    "                if first > 1:\n",
    "                    first = max(1, first // 2)\n",
    "                    log_activity(\n",
    "                    f\"Error: {e}, reducing number of results and retrying... (Attempt with first {first})\"\n",
    "                    )\n",
    "                else:\n",
    "                    log_activity(f\"Query failed completely after retries: {e}\")\n",
    "                    break\n",
    "    log_activity(\"Max retries reached. Sleeping for 30 minutes and switching token...\")\n",
    "    time.sleep(1800)\n",
    "    current_token = next(token_iterator)\n",
    "    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "    transport.headers = headers\n",
    "    return execute_query(keyword, first, after_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_create_close_max_range_dates(pull_requests):\n",
    "    \"\"\"\n",
    "    Get the maximum range of creation and closing dates for pull requests grouped by repository.\n",
    "\n",
    "    This function processes a list of pull requests and calculates the earliest creation date\n",
    "    and the latest closing date for each repository. It returns a list of tuples containing\n",
    "    the repository name, the earliest creation date, and the latest closing date.\n",
    "\n",
    "    Args:\n",
    "        pull_requests (list): A list of dictionaries, where each dictionary represents a pull request.\n",
    "            Each dictionary should have the keys \"repository_name_with_owner\", \"created_at\", and \"closed_at\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "            - str: The repository name.\n",
    "            - datetime.datetime: The earliest creation date of pull requests in the repository.\n",
    "            - datetime.datetime: The latest closing date of pull requests in the repository.\n",
    "\n",
    "    Example:\n",
    "        pull_requests = [\n",
    "            {\"repository_name_with_owner\": \"repo1\", \"created_at\": \"2023-01-01T12:00:00Z\", \"closed_at\": \"2023-01-02T12:00:00Z\"},\n",
    "            {\"repository_name_with_owner\": \"repo1\", \"created_at\": \"2023-01-03T12:00:00Z\", \"closed_at\": \"2023-01-04T12:00:00Z\"},\n",
    "            {\"repository_name_with_owner\": \"repo2\", \"created_at\": \"2023-01-01T12:00:00Z\", \"closed_at\": \"2023-01-05T12:00:00Z\"},\n",
    "        ]\n",
    "        result = get_create_close_max_range_dates(pull_requests)\n",
    "        # result: [('repo1', datetime.datetime(2023, 1, 1, 12, 0), datetime.datetime(2023, 1, 4, 12, 0)),\n",
    "        #          ('repo2', datetime.datetime(2023, 1, 1, 12, 0), datetime.datetime(2023, 1, 5, 12, 0))]\n",
    "    \"\"\"\n",
    "    if not pull_requests:  # Handle empty or None input\n",
    "        return []\n",
    "\n",
    "    date_ranges = {}\n",
    "    for pr in pull_requests:\n",
    "        repo = pr.get(\"repository_name_with_owner\")\n",
    "        created_at = pr.get(\"created_at\")\n",
    "        closed_at = pr.get(\"closed_at\")\n",
    "\n",
    "        if not repo or not created_at or not closed_at:\n",
    "            continue\n",
    "\n",
    "        created_at = datetime.datetime.strptime(created_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        closed_at = datetime.datetime.strptime(closed_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        if repo not in date_ranges:\n",
    "            date_ranges[repo] = (created_at, closed_at)\n",
    "        else:\n",
    "            current_min, current_max = date_ranges[repo]\n",
    "            date_ranges[repo] = (\n",
    "                min(current_min, created_at),\n",
    "                max(current_max, closed_at),\n",
    "            )\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"repository_name_with_owner\": repo,\n",
    "            \"start_date\": dates[0],\n",
    "            \"end_date\": dates[1],\n",
    "        }\n",
    "        for repo, dates in date_ranges.items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43708fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if os.path.exists(\"non-spam-progress.pkl\"):\n",
    "    with open(\"non-spam-progress.pkl\", \"rb\") as f:\n",
    "        progress_data = pickle.load(f)\n",
    "        df = progress_data[\"df\"]\n",
    "        start_index = progress_data[\"start_index\"]\n",
    "else:\n",
    "    df = []\n",
    "    start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = []\n",
    "# start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "def execute_with_dynamic_date_range(\n",
    "    execute_query,\n",
    "    process_results,\n",
    "    index,\n",
    "    se_fm_repository_data,\n",
    "    repo,\n",
    "    days_offset=31,\n",
    "    max_total_allowed_results=950,\n",
    "    default_days_interval=60,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes a GraphQL query within dynamically adjusted date ranges to handle large datasets.\n",
    "\n",
    "    :param keywords: List of keywords for search queries.\n",
    "    :param execute_query: Function to execute the query.\n",
    "    :param process_results: Function to process the query results.\n",
    "    :param max_total_allowed_results: Max allowed results before reducing date range.\n",
    "    :param default_days_interval: Initial days interval for date range.\n",
    "    \"\"\"\n",
    "    repository_name_with_owner = repo.get(\"repository_name_with_owner\")\n",
    "    earliest_pr_create_date = repo.get(\"start_date\")\n",
    "    latest_pr_close_date = repo.get(\"end_date\")\n",
    "\n",
    "    if (\n",
    "        not repository_name_with_owner\n",
    "        or not earliest_pr_create_date\n",
    "        or not latest_pr_close_date\n",
    "    ):\n",
    "        return\n",
    "\n",
    "    start_date = earliest_pr_create_date - datetime.timedelta(days=days_offset)\n",
    "    end_date = latest_pr_close_date + datetime.timedelta(days=days_offset)\n",
    "    days_interval = default_days_interval\n",
    "\n",
    "    while start_date < end_date:\n",
    "        next_date_candidate = start_date + datetime.timedelta(days=days_interval)\n",
    "        next_date = min(next_date_candidate, end_date)\n",
    "\n",
    "        try:\n",
    "            after_cursor = None\n",
    "            while True:\n",
    "                date_range = f\"{start_date.strftime('%Y-%m-%dT%H:%M')}..{next_date.strftime('%Y-%m-%dT%H:%M')}\"\n",
    "                search_keyword = f\"-label:spam repo:{repository_name_with_owner} is:pr is:public archived:false created:{date_range}\"\n",
    "                response = execute_query(\n",
    "                    search_keyword, first=10, after_cursor=after_cursor\n",
    "                )\n",
    "                log_activity(f'response count: {response[\"search\"][\"issueCount\"]}\\n')\n",
    "\n",
    "                if response[\"search\"][\"issueCount\"] == 0:\n",
    "                    days_interval = default_days_interval  # Reset interval\n",
    "                    break\n",
    "\n",
    "                # Adjust interval if issue count exceeds max allowed\n",
    "                if response[\"search\"][\"issueCount\"] > max_total_allowed_results:\n",
    "                    reduced_interval = (\n",
    "                        max(1, days_interval // 2)\n",
    "                        if days_interval > 1\n",
    "                        else max(0.00069, days_interval / 2)\n",
    "                    )\n",
    "                    log_activity(f\"Reducing interval to {reduced_interval} days...\")\n",
    "                    days_interval = reduced_interval\n",
    "                    next_date = start_date + datetime.timedelta(days=days_interval)\n",
    "                    continue\n",
    "\n",
    "                # Process results\n",
    "                process_results(response)\n",
    "\n",
    "                # Pagination\n",
    "                page_info = response[\"search\"][\"pageInfo\"]\n",
    "                if page_info[\"hasNextPage\"]:\n",
    "                    after_cursor = page_info[\"endCursor\"]\n",
    "                else:\n",
    "                    break\n",
    "            with open(\"non-spam-progress.pkl\", \"wb\") as f:\n",
    "                pickle.dump({\"df\": se_fm_repository_data, \"start_index\": index + 1}, f)\n",
    "\n",
    "            # Reset interval to default after a successful run\n",
    "            days_interval = default_days_interval\n",
    "        except Exception as e:\n",
    "            log_activity(\n",
    "                f\"Error fetching data for '{search_keyword}' in range {date_range}: {e}\"\n",
    "            )\n",
    "            # Save progress before terminating\n",
    "            with open(\"non-spam-progress.pkl\", \"wb\") as f:\n",
    "                pickle.dump({\"df\": df, \"start_index\": index}, f)\n",
    "            raise\n",
    "\n",
    "        start_date = next_date  # Move to the next date interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f6383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_spammed_pr_repositories(pull_requests, max=10):\n",
    "    \"\"\"\n",
    "    Get the repositories with the most spam pull requests.\n",
    "\n",
    "    This function processes a list of pull requests and calculates the number of spam pull requests\n",
    "    for each repository. It returns a list of dictionaries containing the repository name and the\n",
    "    count of spam pull requests, sorted in descending order by the count.\n",
    "\n",
    "    Args:\n",
    "        pull_requests (list): A list of dictionaries, where each dictionary represents a pull request.\n",
    "            Each dictionary should have the keys \"repository_name_with_owner\" and \"labeled_spam_by\".\n",
    "        max (int): The maximum number of repositories to return. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains:\n",
    "            - str: The repository name.\n",
    "            - int: The count of spam pull requests in the repository.\n",
    "\n",
    "    Example:\n",
    "        pull_requests = [\n",
    "            {\"repository_name_with_owner\": \"repo1\", \"labeled_spam_by\": \"user1\"},\n",
    "            {\"repository_name_with_owner\": \"repo1\", \"labeled_spam_by\": \"user2\"},\n",
    "            {\"repository_name_with_owner\": \"repo2\", \"labeled_spam_by\": \"user1\"},\n",
    "        ]\n",
    "        result = get_most_spammed_pr_repositories(pull_requests)\n",
    "        # result: [{'repository_name_with_owner': 'repo1', 'spam_pr_count': 2},\n",
    "        #          {'repository_name_with_owner': 'repo2', 'spam_pr_count': 1}]\n",
    "    \"\"\"\n",
    "    if not pull_requests:  # Handle empty or None input\n",
    "        return []\n",
    "\n",
    "    spam_tracker = {}\n",
    "    for pr in pull_requests:\n",
    "        repo = pr.get(\"repository_name_with_owner\")\n",
    "\n",
    "        if not repo:\n",
    "            continue\n",
    "\n",
    "        if repo not in spam_tracker:\n",
    "            spam_tracker[repo] = {\"count\": 0, \"details\": []}\n",
    "        spam_tracker[repo][\"count\"] += 1\n",
    "        spam_tracker[repo][\"details\"].append(pr)\n",
    "\n",
    "    sorted_spam_counts = sorted(\n",
    "        spam_tracker.items(), key=lambda x: x[1][\"count\"], reverse=1\n",
    "    )\n",
    "    return [\n",
    "        {\"prs\": repo_data[\"details\"], \"spam_pr_count\": repo_data[\"count\"]}\n",
    "        for _, repo_data in sorted_spam_counts[:max]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dcc155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_processor(\n",
    "    response,\n",
    "):\n",
    "    for edge in response[\"search\"][\"edges\"]:\n",
    "        pull_request = edge[\"node\"]\n",
    "\n",
    "        if not pull_request:\n",
    "            continue\n",
    "        timeline = pull_request[\"timeline\"][\"edges\"]\n",
    "        labeled_spam_event = next(\n",
    "            filter(\n",
    "                lambda x: x[\"node\"]\n",
    "                and x[\"node\"][\"__typename\"] == \"LabeledEvent\"\n",
    "                and x[\"node\"][\"label\"][\"name\"]\n",
    "                and (x[\"node\"][\"label\"][\"name\"]).lower()\n",
    "                in [\"ai spam\", \"spam\", \"ai-spam\"],\n",
    "                timeline,\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        labeled_spam_event_node = (\n",
    "            labeled_spam_event[\"node\"] if labeled_spam_event else None\n",
    "        )\n",
    "        closed_event = next(\n",
    "            filter(\n",
    "                lambda x: x[\"node\"] and x[\"node\"][\"__typename\"] == \"ClosedEvent\",\n",
    "                timeline,\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        closed_event_node = closed_event[\"node\"] if closed_event else None\n",
    "        author = pull_request[\"author\"]\n",
    "        comments = [comment[\"node\"] for comment in pull_request[\"comments\"][\"edges\"]]\n",
    "        labeled_spam_at = (\n",
    "            labeled_spam_event_node[\"createdAt\"] if labeled_spam_event_node else None\n",
    "        )\n",
    "\n",
    "        comments_by_spam_labeler = []\n",
    "\n",
    "        closed_by = (\n",
    "            closed_event_node[\"actor\"][\"login\"]\n",
    "            if closed_event_node and closed_event_node[\"actor\"]\n",
    "            else None\n",
    "        )\n",
    "        end_date = closed_event_node[\"createdAt\"] if closed_event_node else None\n",
    "        comments_by_closer = [\n",
    "            {\n",
    "                **closer_comment,\n",
    "                \"commented_before_closing\": (\n",
    "                    closer_comment[\"createdAt\"] < end_date if end_date else False\n",
    "                ),\n",
    "            }\n",
    "            for closer_comment in comments\n",
    "            if (\n",
    "                (\n",
    "                    closer_comment[\"author\"]\n",
    "                    and closer_comment[\"author\"][\"login\"] == closed_by\n",
    "                )\n",
    "                or (not closer_comment[\"author\"] and not closed_by)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        author_organizations = (\n",
    "            [\n",
    "                organization[\"node\"]\n",
    "                for organization in author[\"organizations\"][\"edges\"]\n",
    "                if organization[\"node\"]\n",
    "            ]\n",
    "            if author\n",
    "            and author.get(\"organizations\")\n",
    "            and author[\"organizations\"].get(\"edges\")\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        timestamp_suffix = f\"_as_at_{datetime.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "        df.append(\n",
    "            {\n",
    "                \"id\": pull_request[\"id\"],\n",
    "                \"title\": pull_request[\"title\"],\n",
    "                \"url\": pull_request[\"url\"],\n",
    "                \"state\": pull_request[\"state\"],\n",
    "                \"comments_count\": pull_request[\"comments\"][\"totalCount\"],\n",
    "                \"comments_by_spam_labeler_count\": len(comments_by_spam_labeler),\n",
    "                \"comments_by_spam_labeler\": comments_by_spam_labeler,\n",
    "                \"labeled_spam_by\": (\n",
    "                    labeled_spam_event_node[\"actor\"][\"login\"]\n",
    "                    if labeled_spam_event_node and labeled_spam_event_node[\"actor\"]\n",
    "                    else None\n",
    "                ),\n",
    "                \"is_labeled_spam_by_bot\": False,\n",
    "                \"labeled_spam_at\": labeled_spam_at,\n",
    "                \"comments_by_closer_count\": len(comments_by_closer),\n",
    "                \"comments_by_closer\": comments_by_closer,\n",
    "                \"closed\": pull_request[\"closed\"],\n",
    "                \"is_closed_by_bot\": closed_by is None and pull_request[\"closed\"],\n",
    "                \"closed_by\": closed_by,\n",
    "                \"closed_at\": end_date,\n",
    "                \"merged\": pull_request[\"merged\"],\n",
    "                \"body\": pull_request[\"body\"],\n",
    "                \"created_at\": pull_request[\"createdAt\"],\n",
    "                \"updated_at\": pull_request[\"updatedAt\"],\n",
    "                \"repository\": pull_request[\"repository\"],\n",
    "                \"repository_name_with_owner\": pull_request[\"repository\"][\n",
    "                    \"nameWithOwner\"\n",
    "                ],\n",
    "                \"repository_stargazer_count\": pull_request[\"repository\"][\n",
    "                    \"stargazerCount\"\n",
    "                ],\n",
    "                \"repository_watcher_count\": pull_request[\"repository\"][\"watchers\"][\n",
    "                    \"totalCount\"\n",
    "                ],\n",
    "                \"repository_is_fork\": pull_request[\"repository\"][\"isFork\"],\n",
    "                \"repository_languages\": [\n",
    "                    language[\"node\"][\"name\"]\n",
    "                    for language in pull_request[\"repository\"][\"languages\"][\"edges\"]\n",
    "                ],\n",
    "                \"merge_commit\": (\n",
    "                    pull_request[\"mergeCommit\"][\"oid\"]\n",
    "                    if pull_request[\"mergeCommit\"]\n",
    "                    else None\n",
    "                ),\n",
    "                \"labels\": [\n",
    "                    label[\"node\"][\"name\"] for label in pull_request[\"labels\"][\"edges\"]\n",
    "                ],\n",
    "                \"commits_count\": pull_request[\"commits\"][\"totalCount\"],\n",
    "                \"changed_files_count\": pull_request[\"changedFiles\"],\n",
    "                \"author_name\": (author[\"login\"] if author else None),\n",
    "                \"author_url\": (author[\"url\"] if author else None),\n",
    "                \"author_account_created_at\": (author[\"createdAt\"] if author else None),\n",
    "                f\"author_repository_count{timestamp_suffix}\": (\n",
    "                    author[\"repositories\"][\"totalCount\"]\n",
    "                    if author and author[\"repositories\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_followers_count{timestamp_suffix}\": (\n",
    "                    author[\"followers\"][\"totalCount\"]\n",
    "                    if author and author[\"followers\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_following_count{timestamp_suffix}\": (\n",
    "                    author[\"following\"][\"totalCount\"]\n",
    "                    if author and author[\"following\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_repository_discussions_count{timestamp_suffix}\": (\n",
    "                    author[\"repositoryDiscussions\"][\"totalCount\"]\n",
    "                    if author and author[\"repositoryDiscussions\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_repository_discussion_comments_count{timestamp_suffix}\": (\n",
    "                    author[\"repositoryDiscussionComments\"][\"totalCount\"]\n",
    "                    if author and author[\"repositoryDiscussionComments\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_organizations{timestamp_suffix}\": author_organizations,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "with open(\"progress.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    spam_prs = data[\"df\"]\n",
    "\n",
    "\n",
    "# template\n",
    "index = start_index\n",
    "se_fm_repository_data = df\n",
    "most_spammed_repos = get_most_spammed_pr_repositories(spam_prs, 10)\n",
    "# Extract top pull requests from the most spammed repositories\n",
    "top_prs = []\n",
    "for repo in most_spammed_repos:\n",
    "    for pr in repo[\"prs\"]:\n",
    "        top_prs.append(pr)\n",
    "\n",
    "repos = get_create_close_max_range_dates(top_prs)\n",
    "\n",
    "for repo in repos:\n",
    "\n",
    "    execute_with_dynamic_date_range(\n",
    "        execute_query=execute_query,\n",
    "        process_results=result_processor,\n",
    "        repo=repo,\n",
    "        index=start_index,\n",
    "        se_fm_repository_data=df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb89147-ab2c-4654-a8d7-637737c90537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"progress.pkl\", \"rb\") as f:\n",
    "#     data = pickle.load(f)\n",
    "#     spam_prs = data[\"df\"]\n",
    "\n",
    "\n",
    "# # template\n",
    "# index = start_index\n",
    "# se_fm_repository_data = df\n",
    "# most_spammed_repos = get_most_spammed_pr_repositories(spam_prs)\n",
    "# # Extract top pull requests from the most spammed repositories\n",
    "# top_prs = []\n",
    "# for repo in most_spammed_repos:\n",
    "#     for pr in repo[\"prs\"]:\n",
    "#         top_prs.append(pr)\n",
    "\n",
    "# repos = get_create_close_max_range_dates(top_prs)\n",
    "\n",
    "# DAYS_OFFSET = 31\n",
    "\n",
    "# for repo in repos:\n",
    "#     repository_name_with_owner = repo.get(\"repository_name_with_owner\")\n",
    "#     earliest_pr_create_date = repo.get(\"start_date\")\n",
    "#     latest_pr_close_date = repo.get(\"end_date\")\n",
    "\n",
    "#     if (\n",
    "#         not repository_name_with_owner\n",
    "#         or not earliest_pr_create_date\n",
    "#         or not latest_pr_close_date\n",
    "#     ):\n",
    "#         continue\n",
    "#     start_date = (earliest_pr_create_date - datetime.timedelta(days=DAYS_OFFSET)).strftime(\n",
    "#         \"%Y-%m-%d\"\n",
    "#     )\n",
    "#     end_date = (latest_pr_close_date + datetime.timedelta(days=DAYS_OFFSET)).strftime(\"%Y-%m-%d\")\n",
    "#     search_keyword = f\"-label:spam repo:{repository_name_with_owner} is:pr is:public archived:false created:{start_date}..{end_date}\"\n",
    "\n",
    "#     try:\n",
    "#         after_cursor = None\n",
    "#         while True:\n",
    "#             response = execute_query(\n",
    "#                 search_keyword, first=10, after_cursor=after_cursor\n",
    "#             )\n",
    "\n",
    "#             if response[\"search\"][\"issueCount\"] == 0:\n",
    "#                 break\n",
    "#             # Extract pr\n",
    "#             for edge in response[\"search\"][\"edges\"]:\n",
    "#                 pull_request = edge[\"node\"]\n",
    "\n",
    "#                 if not pull_request:\n",
    "#                     continue\n",
    "#                 timeline = pull_request[\"timeline\"][\"edges\"]\n",
    "#                 labeled_spam_event = next(\n",
    "#                     filter(\n",
    "#                         lambda x: x[\"node\"]\n",
    "#                         and x[\"node\"][\"__typename\"] == \"LabeledEvent\"\n",
    "#                         and x[\"node\"][\"label\"][\"name\"]\n",
    "#                         and (x[\"node\"][\"label\"][\"name\"]).lower() == \"spam\",\n",
    "#                         timeline,\n",
    "#                     ),\n",
    "#                     None,\n",
    "#                 )\n",
    "#                 labeled_spam_event_node = (\n",
    "#                     labeled_spam_event[\"node\"] if labeled_spam_event else None\n",
    "#                 )\n",
    "#                 closed_event = next(\n",
    "#                     filter(\n",
    "#                         lambda x: x[\"node\"]\n",
    "#                         and x[\"node\"][\"__typename\"] == \"ClosedEvent\",\n",
    "#                         timeline,\n",
    "#                     ),\n",
    "#                     None,\n",
    "#                 )\n",
    "#                 closed_event_node = closed_event[\"node\"] if closed_event else None\n",
    "#                 author = pull_request[\"author\"]\n",
    "#                 comments = [\n",
    "#                     comment[\"node\"] for comment in pull_request[\"comments\"][\"edges\"]\n",
    "#                 ]\n",
    "#                 labeled_spam_at = (\n",
    "#                     labeled_spam_event_node[\"createdAt\"]\n",
    "#                     if labeled_spam_event_node\n",
    "#                     else None\n",
    "#                 )\n",
    "\n",
    "#                 labeled_spam_by = (\n",
    "#                     labeled_spam_event_node[\"actor\"][\"login\"]\n",
    "#                     if labeled_spam_event_node and labeled_spam_event_node[\"actor\"]\n",
    "#                     else None\n",
    "#                 )\n",
    "#                 comments_by_spam_labeler = []\n",
    "\n",
    "#                 closed_by = (\n",
    "#                     closed_event_node[\"actor\"][\"login\"]\n",
    "#                     if closed_event_node and closed_event_node[\"actor\"]\n",
    "#                     else None\n",
    "#                 )\n",
    "#                 end_date = closed_event_node[\"createdAt\"] if closed_event_node else None\n",
    "#                 comments_by_closer = [\n",
    "#                     {\n",
    "#                         **closer_comment,\n",
    "#                         \"commented_before_closing\": (\n",
    "#                             closer_comment[\"createdAt\"] < end_date\n",
    "#                             if end_date\n",
    "#                             else False\n",
    "#                         ),\n",
    "#                     }\n",
    "#                     for closer_comment in comments\n",
    "#                     if (\n",
    "#                         (\n",
    "#                             closer_comment[\"author\"]\n",
    "#                             and closer_comment[\"author\"][\"login\"] == closed_by\n",
    "#                         )\n",
    "#                         or (not closer_comment[\"author\"] and not closed_by)\n",
    "#                     )\n",
    "#                 ]\n",
    "\n",
    "#                 author_organizations = (\n",
    "#                     [\n",
    "#                         organization[\"node\"]\n",
    "#                         for organization in author[\"organizations\"][\"edges\"]\n",
    "#                         if organization[\"node\"]\n",
    "#                     ]\n",
    "#                     if author\n",
    "#                     and author.get(\"organizations\")\n",
    "#                     and author[\"organizations\"].get(\"edges\")\n",
    "#                     else []\n",
    "#                 )\n",
    "\n",
    "#                 timestamp_suffix = (\n",
    "#                     f\"_as_at_{datetime.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "#                 )\n",
    "\n",
    "#                 df.append(\n",
    "#                     {\n",
    "#                         \"id\": pull_request[\"id\"],\n",
    "#                         \"title\": pull_request[\"title\"],\n",
    "#                         \"url\": pull_request[\"url\"],\n",
    "#                         \"state\": pull_request[\"state\"],\n",
    "#                         \"comments_count\": pull_request[\"comments\"][\"totalCount\"],\n",
    "#                         \"comments_by_spam_labeler_count\": len(comments_by_spam_labeler),\n",
    "#                         \"comments_by_spam_labeler\": comments_by_spam_labeler,\n",
    "#                         \"labeled_spam_by\": (\n",
    "#                             labeled_spam_event_node[\"actor\"][\"login\"]\n",
    "#                             if labeled_spam_event_node\n",
    "#                             and labeled_spam_event_node[\"actor\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         \"is_labeled_spam_by_bot\": False,\n",
    "#                         \"labeled_spam_at\": labeled_spam_at,\n",
    "#                         \"comments_by_closer_count\": len(comments_by_closer),\n",
    "#                         \"comments_by_closer\": comments_by_closer,\n",
    "#                         \"closed\": pull_request[\"closed\"],\n",
    "#                         \"is_closed_by_bot\": closed_by is None\n",
    "#                         and pull_request[\"closed\"],\n",
    "#                         \"closed_by\": closed_by,\n",
    "#                         \"closed_at\": end_date,\n",
    "#                         \"merged\": pull_request[\"merged\"],\n",
    "#                         \"body\": pull_request[\"body\"],\n",
    "#                         \"created_at\": pull_request[\"createdAt\"],\n",
    "#                         \"updated_at\": pull_request[\"updatedAt\"],\n",
    "#                         \"repository\": pull_request[\"repository\"],\n",
    "#                         \"repository_name_with_owner\": pull_request[\"repository\"][\n",
    "#                             \"nameWithOwner\"\n",
    "#                         ],\n",
    "#                         \"repository_stargazer_count\": pull_request[\"repository\"][\n",
    "#                             \"stargazerCount\"\n",
    "#                         ],\n",
    "#                         \"repository_watcher_count\": pull_request[\"repository\"][\n",
    "#                             \"watchers\"\n",
    "#                         ][\"totalCount\"],\n",
    "#                         \"repository_is_fork\": pull_request[\"repository\"][\"isFork\"],\n",
    "#                         \"repository_languages\": [\n",
    "#                             language[\"node\"][\"name\"]\n",
    "#                             for language in pull_request[\"repository\"][\"languages\"][\n",
    "#                                 \"edges\"\n",
    "#                             ]\n",
    "#                         ],\n",
    "#                         \"merge_commit\": (\n",
    "#                             pull_request[\"mergeCommit\"][\"oid\"]\n",
    "#                             if pull_request[\"mergeCommit\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         \"labels\": [\n",
    "#                             label[\"node\"][\"name\"]\n",
    "#                             for label in pull_request[\"labels\"][\"edges\"]\n",
    "#                         ],\n",
    "#                         \"commits_count\": pull_request[\"commits\"][\"totalCount\"],\n",
    "#                         \"changed_files_count\": pull_request[\"changedFiles\"],\n",
    "#                         \"author_name\": (author[\"login\"] if author else None),\n",
    "#                         \"author_url\": (author[\"url\"] if author else None),\n",
    "#                         \"author_account_created_at\": (\n",
    "#                             author[\"createdAt\"] if author else None\n",
    "#                         ),\n",
    "#                         f\"author_repository_count{timestamp_suffix}\": (\n",
    "#                             author[\"repositories\"][\"totalCount\"]\n",
    "#                             if author and author[\"repositories\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         f\"author_followers_count{timestamp_suffix}\": (\n",
    "#                             author[\"followers\"][\"totalCount\"]\n",
    "#                             if author and author[\"followers\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         f\"author_following_count{timestamp_suffix}\": (\n",
    "#                             author[\"following\"][\"totalCount\"]\n",
    "#                             if author and author[\"following\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         f\"author_repository_discussions_count{timestamp_suffix}\": (\n",
    "#                             author[\"repositoryDiscussions\"][\"totalCount\"]\n",
    "#                             if author and author[\"repositoryDiscussions\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         f\"author_repository_discussion_comments_count{timestamp_suffix}\": (\n",
    "#                             author[\"repositoryDiscussionComments\"][\"totalCount\"]\n",
    "#                             if author and author[\"repositoryDiscussionComments\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         f\"author_organizations{timestamp_suffix}\": author_organizations,\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#             # Pagination\n",
    "#             page_info = response[\"search\"][\"pageInfo\"]\n",
    "#             if page_info[\"hasNextPage\"]:\n",
    "#                 after_cursor = page_info[\"endCursor\"]\n",
    "#             else:\n",
    "#                 break\n",
    "#         with open(\"non-spam-progress.pkl\", \"wb\") as f:\n",
    "#             pickle.dump({\"df\": se_fm_repository_data, \"start_index\": index + 1}, f)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         log_activity(f\"Failed to retrieve data for search_keyword '{search_keyword}': {e}\")\n",
    "#         # Save progress before terminating\n",
    "#         with open(\"non-spam-progress.pkl\", \"wb\") as f:\n",
    "#             pickle.dump({\"df\": df, \"start_index\": index}, f)\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bca9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_pkl_content(filepath):\n",
    "    \"\"\"\n",
    "    Display the content of a pickle file and save it as CSV and JSON files.\n",
    "\n",
    "    This function reads a pickle file from the given filepath, prints its content,\n",
    "    and saves the data contained in the \"df\" key to both a CSV file and a JSON file.\n",
    "    The CSV file is saved with the name \"spam_data_without_org_join_date.csv\" and the JSON file is saved\n",
    "    with the name \"spam_data_without_org_join_date.json\".\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the pickle file to be read.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error reading the pickle file or writing the CSV/JSON files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        log_activity(f\"Content of {filepath}:\\n\")\n",
    "        filename = \"non-spam_data_without_org_join_date\"\n",
    "        pd.DataFrame(data[\"df\"]).to_csv(f\"{filename}.csv\", index=True)\n",
    "        log_activity(f\"Data written to {filename}.csv successfully.\")\n",
    "        try:\n",
    "            with open(f\"{filename}.json\", \"w\") as f:\n",
    "                json.dump(data[\"df\"], f, indent=4)\n",
    "            log_activity(f\"Data written to {filename}.json successfully.\")\n",
    "        except Exception as e:\n",
    "            log_activity(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        log_activity(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = \"non-spam-progress.pkl\"\n",
    "display_pkl_content(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0deeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate metadata\n",
    "df = []\n",
    "start_index = 0\n",
    "\n",
    "\n",
    "\n",
    "def generate_metadata(filepath):\n",
    "    \"\"\" \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        log_activity(f\"Content of {filepath}:\\n\")\n",
    "        filename = \"non-spam_data.meta\"\n",
    "        pull_requests = data[\"df\"]\n",
    "        unique_repository = {}\n",
    "        unique_pr_author = {}\n",
    "        unique_pr_spam_labeler = {}\n",
    "        unique_pr_closer = {}\n",
    "        merged_pr_count = 0\n",
    "        closed_pr_count = 0\n",
    "        for pull_request in pull_requests:\n",
    "            def update_unique_value_dict(info_dict, key, value):\n",
    "                if not value:\n",
    "                    log_activity(f\"Warning: Pull request missing '{key}' {pull_request}\")\n",
    "                    return False\n",
    "                if value not in info_dict:\n",
    "                    info_dict[value] = value\n",
    "                return True\n",
    "\n",
    "            # Update repository count\n",
    "            update_unique_value_dict(unique_repository, \"repository_name_with_owner\", pull_request[\"repository_name_with_owner\"])\n",
    "\n",
    "            # Update author count\n",
    "            update_unique_value_dict(unique_pr_author, \"author_name\", pull_request[\"author_name\"])\n",
    "\n",
    "            # Update spam labeler count\n",
    "            update_unique_value_dict(unique_pr_spam_labeler, \"labeled_spam_by\", pull_request[\"labeled_spam_by\"])\n",
    "\n",
    "            # Update closer count\n",
    "            update_unique_value_dict(unique_pr_closer, \"closed_by\", pull_request[\"closed_by\"])\n",
    "\n",
    "            merged_pr_count += 1 if pull_request[\"merged\"] else 0\n",
    "            closed_pr_count += 1 if pull_request[\"closed\"] is not None else 0\n",
    "\n",
    "\n",
    "        total_prs= len(pull_requests)\n",
    "        unique_repository_count= len(unique_repository)\n",
    "        unique_pr_author_count= len(unique_pr_author)\n",
    "        unique_pr_spam_labeler_count= len(unique_pr_spam_labeler)\n",
    "        unique_pr_closer_count= len(unique_pr_closer)\n",
    "        \n",
    "        df.append(\n",
    "            {\n",
    "            \"total_prs\": total_prs,\n",
    "            \"unique_repository_count\": unique_repository_count,\n",
    "            \"unique_pr_author_count\": unique_pr_author_count,\n",
    "            \"unique_pr_author_ratio\": round(unique_pr_author_count / total_prs, 3),\n",
    "            \"unique_pr_spam_labeler_count\": unique_pr_spam_labeler_count,\n",
    "            \"unique_pr_spam_labeler_ratio\": round(unique_pr_spam_labeler_count / total_prs, 3),\n",
    "            \"unique_pr_closer_count\": unique_pr_closer_count,\n",
    "            \"unique_pr_closer_ratio\": round(unique_pr_closer_count / total_prs, 3),\n",
    "            \"merged_pr_count\": merged_pr_count,\n",
    "            \"merged_pr_ratio\": round(merged_pr_count / total_prs, 3),\n",
    "            \"closed_pr_count\": closed_pr_count,\n",
    "            \"closed_pr_ratio\": round(closed_pr_count / total_prs, 3),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        pd.DataFrame(df).to_csv(f\"{filename}.csv\", index=True)\n",
    "        log_activity(f\"Data written to {filename}.csv successfully.\")\n",
    "        try:\n",
    "            with open(f\"{filename}.json\", \"w\") as f:\n",
    "                json.dump(df, f, indent=4)\n",
    "            log_activity(f\"Data written to {filename}.json successfully.\")\n",
    "        except Exception as e:\n",
    "            log_activity(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        log_activity(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = \"non-spam-progress.pkl\"\n",
    "generate_metadata(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469812ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
