{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babdda8-11f7-485e-bad8-d8c5b8eebb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Read tokens from a text file\n",
    "tokens_file = \"./env/tokens.txt\"\n",
    "with open(tokens_file, \"r\") as file:\n",
    "    tokens = file.read().splitlines()\n",
    "\n",
    "# Create an iterator to cycle through the tokens\n",
    "token_iterator = itertools.cycle(tokens)\n",
    "current_token = next(token_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c281d1-993b-497b-8984-0e32ac6a61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of User-Agents for randomization\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "]\n",
    "\n",
    "# Define headers to authenticate using the first token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {current_token}\",\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "}\n",
    "\n",
    "# Setup GraphQL endpoint and client\n",
    "graphql_url = \"https://api.github.com/graphql\"\n",
    "transport = RequestsHTTPTransport(url=graphql_url, headers=headers, use_json=True)\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837bc28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_activity(activity: str):\n",
    "    log = f\"{datetime.datetime.now()}: {activity}\\n\"\n",
    "    # print(log)\n",
    "    with open(\"non-spam-prs-output.log\", \"a\") as log_file:\n",
    "        log_file.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062f1b0-a9df-4f94-bb18-56f42774bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all tokens to verify their validity\n",
    "def test_all_tokens():\n",
    "    test_query = gql(\n",
    "        \"\"\"\n",
    "        {\n",
    "          viewer {\n",
    "            login\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    for i, token in enumerate(tokens):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"User-Agent\": random.choice(user_agents),\n",
    "        }\n",
    "        transport = RequestsHTTPTransport(\n",
    "            url=graphql_url, headers=headers, use_json=True\n",
    "        )\n",
    "        client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "        try:\n",
    "            response = client.execute(test_query)\n",
    "            log_activity(\n",
    "                f\"Token {i+1}/{len(tokens)} is valid. Logged in as: {response['viewer']['login']}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log_activity(f\"Token {i+1}/{len(tokens)} failed with error: {e}\")\n",
    "\n",
    "\n",
    "# Run the token validation\n",
    "test_all_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd85df0-71b7-46c8-9ed4-eceb724201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphQL query\n",
    "query_template = gql(\n",
    "    \"\"\"\n",
    "    query searchIssues($keyword: String!, $afterCursor: String, $first: Int) {\n",
    "      search(query: $keyword, type: ISSUE, first: $first, after: $afterCursor) {\n",
    "        issueCount\n",
    "        edges {\n",
    "          cursor\n",
    "          node {\n",
    "            ... on PullRequest {\n",
    "              id\n",
    "              number\n",
    "              title\n",
    "              url\n",
    "              comments(first: 100) {\n",
    "                totalCount # it still gives the total count regardless of the first parameter\n",
    "                edges {\n",
    "                  node {   \n",
    "                    author { ... on User { login } }\n",
    "                    editor { ... on User { login } }\n",
    "                    body\n",
    "                    createdAt\n",
    "                    lastEditedAt\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              state\n",
    "              closed\n",
    "              merged\n",
    "              createdAt\n",
    "              updatedAt\n",
    "              mergeCommit {\n",
    "                oid\n",
    "              }\n",
    "              timeline(last: 100) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    __typename\n",
    "                    ... on LabeledEvent {\n",
    "                      actor { ... on User { login } }\n",
    "                      label { ... on Label { name }}\n",
    "                      createdAt\n",
    "                    }\n",
    "                    ... on ClosedEvent { \n",
    "                      actor { ... on User { login } }\n",
    "                      createdAt\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              commits {\n",
    "                totalCount\n",
    "              }\n",
    "              changedFiles\n",
    "              headRefName\n",
    "              baseRefName\n",
    "              repository {\n",
    "                id\n",
    "                nameWithOwner\n",
    "                stargazerCount\n",
    "                description\n",
    "                codeOfConduct {\n",
    "                  body\n",
    "                  id\n",
    "                  name\n",
    "                  url\n",
    "                }\n",
    "                homepageUrl\n",
    "                assignableUsers(first: 100) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      login\n",
    "                      url\n",
    "                      bio\n",
    "                      company\n",
    "                    }\n",
    "                  }\n",
    "                  totalCount\n",
    "                }\n",
    "                mentionableUsers(first: 100) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      login\n",
    "                      url\n",
    "                      bio\n",
    "                      company\n",
    "                    }\n",
    "                  }\n",
    "                  totalCount\n",
    "                }\n",
    "                forkCount\n",
    "                watchers {\n",
    "                  totalCount\n",
    "                }\n",
    "                isFork\n",
    "                languages(first: 20) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      name\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              author {\n",
    "                ... on User {\n",
    "                  login\n",
    "                  url\n",
    "                  createdAt\n",
    "                  repositories {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  followers {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  following {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  repositoryDiscussions {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  repositoryDiscussionComments {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  organizations (first: 20){\n",
    "                    edges {\n",
    "                      node {\n",
    "                        name\n",
    "                        login\n",
    "                        url\n",
    "                        membersWithRole {\n",
    "                          totalCount\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              labels(first: 10) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    name\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              body\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        pageInfo {\n",
    "          endCursor\n",
    "          hasNextPage\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b32e41-7d3f-458b-ad0f-0c13d3146813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_contributor_count(repo_owner, repo_name):\n",
    "#     global current_token\n",
    "#     max_retries = 3\n",
    "#     retries = 0\n",
    "#     while retries < max_retries:\n",
    "#         try:\n",
    "#             # Randomize User-Agent for each query\n",
    "#             headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "#             headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "#             url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contributors?per_page=1&anon=true\"\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             if response.status_code == 200:\n",
    "#                 return int(response.headers.get(\"Link\", \"\").split(\",\")[-1].split(\"&page=\")[-1].split(\">\")[0]) if \"Link\" in response.headers else len(response.json())\n",
    "#             elif response.status_code == 403:\n",
    "#                 log_activity(f\"Rate limit exceeded, switching token... (Attempt {retries + 1}/{max_retries})\")\n",
    "#                 current_token = next(token_iterator)\n",
    "#                 retries += 1\n",
    "#             else:\n",
    "#                 response.raise_for_status()\n",
    "#         except Exception as e:\n",
    "#             log_activity(f\"Error: {e}, retrying... (Attempt {retries + 1}/{max_retries})\")\n",
    "#             retries += 1\n",
    "#     raise Exception(\"Max retries reached. Unable to complete the request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c2427-7797-4088-a144-6045c87ee349",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.headers = headers\n",
    "# Check rate limit before executing the main query\n",
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "rate_limit_response = client.execute(rate_limit_query)\n",
    "log_activity(f\"Rate limit: {rate_limit_response['rateLimit']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acb9fd-7a25-458d-aec7-8f302a653c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def execute_query(keyword, first=100, after_cursor=None):\n",
    "    global current_token\n",
    "    log_activity(\n",
    "        f\"Executing query with keyword: {keyword}, first: {first}, afterCursor: {after_cursor}\"\n",
    "    )\n",
    "    while True:\n",
    "        try:\n",
    "            # Randomize User-Agent for each query\n",
    "            headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "            transport.headers = headers\n",
    "            # Check rate limit before executing the main query\n",
    "            rate_limit_response = client.execute(rate_limit_query)\n",
    "            remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "            if remaining < 100:\n",
    "                log_activity(\n",
    "                    f\"Rate limit remaining ({remaining}) is below threshold. Switching token...\"\n",
    "                )\n",
    "                # Set up to track whether we have cycled through all tokens\n",
    "                all_tokens_checked = False\n",
    "                initial_token = current_token\n",
    "\n",
    "                while not all_tokens_checked:\n",
    "                    # Switch to the next token\n",
    "                    current_token = next(token_iterator)\n",
    "                    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "                    transport.headers = headers\n",
    "\n",
    "                    # Check the rate limit of the new token\n",
    "                    rate_limit_response = client.execute(rate_limit_query)\n",
    "                    remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "\n",
    "                    if remaining >= 100:\n",
    "                        log_activity(\n",
    "                            f\"Switched to a new token with sufficient rate limit ({remaining} remaining).\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "                    # Check if we have cycled through all tokens\n",
    "                    if current_token == initial_token:\n",
    "                        log_activity(\"All tokens are below threshold. Waiting for 1 hour...\")\n",
    "                        time.sleep(3600)\n",
    "                        all_tokens_checked = True\n",
    "\n",
    "                continue\n",
    "\n",
    "            return client.execute(\n",
    "                query_template,\n",
    "                variable_values={\n",
    "                    \"keyword\": keyword,\n",
    "                    \"first\": first,\n",
    "                    \"afterCursor\": after_cursor,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"API rate limit\" in str(e):\n",
    "                log_activity(\n",
    "                    f\"Rate limit reached: {e}, switching token... (Attempt with first {first})\"\n",
    "                )\n",
    "                current_token = next(token_iterator)\n",
    "                headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "            else:\n",
    "                if first > 1:\n",
    "                    first = max(1, first // 2)\n",
    "                    log_activity(\n",
    "                    f\"Error: {e}, reducing number of results and retrying... (Attempt with first {first})\"\n",
    "                    )\n",
    "                else:\n",
    "                    log_activity(f\"Query failed completely after retries: {e}\")\n",
    "                    break\n",
    "    # Track consecutive failures\n",
    "    abandon_threshold = 3\n",
    "    if 'failure_count' not in globals():\n",
    "        global failure_count\n",
    "        failure_count = 0\n",
    "    failure_count += 1\n",
    "    if failure_count >= abandon_threshold:\n",
    "        log_activity(f\"Query failed {abandon_threshold} times in a row. Abandoning operation.\")\n",
    "        failure_count = 0  # Reset failure count after abandoning\n",
    "        return {\"abandon\": True}\n",
    "    log_activity(\"Max retries reached. Sleeping for 30 minutes and switching token...\")\n",
    "    time.sleep(3)\n",
    "    # time.sleep(1800)\n",
    "    current_token = next(token_iterator)\n",
    "    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "    transport.headers = headers\n",
    "    return execute_query(keyword, first, after_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1685769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_create_close_max_range_dates(pull_requests):\n",
    "    \"\"\"\n",
    "    Get the maximum range of creation and closing dates for pull requests grouped by repository.\n",
    "\n",
    "    This function processes a list of pull requests and calculates the earliest creation date\n",
    "    and the latest closing date for each repository. It returns a list of tuples containing\n",
    "    the repository name, the earliest creation date, and the latest closing date.\n",
    "\n",
    "    Args:\n",
    "        pull_requests (list): A list of dictionaries, where each dictionary represents a pull request.\n",
    "            Each dictionary should have the keys \"repository_name_with_owner\", \"created_at\", and \"closed_at\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "            - str: The repository name.\n",
    "            - datetime.datetime: The earliest creation date of pull requests in the repository.\n",
    "            - datetime.datetime: The latest closing date of pull requests in the repository.\n",
    "\n",
    "    Example:\n",
    "        pull_requests = [\n",
    "            {\"repository_name_with_owner\": \"repo1\", \"created_at\": \"2023-01-01T12:00:00Z\", \"closed_at\": \"2023-01-02T12:00:00Z\"},\n",
    "            {\"repository_name_with_owner\": \"repo1\", \"created_at\": \"2023-01-03T12:00:00Z\", \"closed_at\": \"2023-01-04T12:00:00Z\"},\n",
    "            {\"repository_name_with_owner\": \"repo2\", \"created_at\": \"2023-01-01T12:00:00Z\", \"closed_at\": \"2023-01-05T12:00:00Z\"},\n",
    "        ]\n",
    "        result = get_create_close_max_range_dates(pull_requests)\n",
    "        # result: [('repo1', datetime.datetime(2023, 1, 1, 12, 0), datetime.datetime(2023, 1, 4, 12, 0)),\n",
    "        #          ('repo2', datetime.datetime(2023, 1, 1, 12, 0), datetime.datetime(2023, 1, 5, 12, 0))]\n",
    "    \"\"\"\n",
    "    if not pull_requests:  # Handle empty or None input\n",
    "        return []\n",
    "\n",
    "    date_ranges = {}\n",
    "    for pr in pull_requests:\n",
    "        repo = pr.get(\"repository_name_with_owner\")\n",
    "        created_at = pr.get(\"created_at\")\n",
    "        closed_at = pr.get(\"closed_at\")\n",
    "\n",
    "        if not repo or not created_at or not closed_at:\n",
    "            continue\n",
    "\n",
    "        created_at = datetime.datetime.strptime(created_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        closed_at = datetime.datetime.strptime(closed_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "        if repo not in date_ranges:\n",
    "            date_ranges[repo] = (created_at, closed_at)\n",
    "        else:\n",
    "            current_min, current_max = date_ranges[repo]\n",
    "            date_ranges[repo] = (\n",
    "                min(current_min, created_at),\n",
    "                max(current_max, closed_at),\n",
    "            )\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"repository_name_with_owner\": repo,\n",
    "            \"start_date\": dates[0],\n",
    "            \"end_date\": dates[1],\n",
    "        }\n",
    "        for repo, dates in date_ranges.items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c43708fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if os.path.exists(\"non-spam-progress.pkl\"):\n",
    "    with open(\"non-spam-progress.pkl\", \"rb\") as f:\n",
    "        progress_data = pickle.load(f)\n",
    "        df = progress_data[\"df\"]\n",
    "        start_index = progress_data[\"start_index\"]\n",
    "else:\n",
    "    df = []\n",
    "    start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77be498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = []\n",
    "# start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "517ecc09",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "start_date_param = \"2023-12-17T17:02\"\n",
    "repos_to_skip = [\n",
    "    \"daiyi/blog\",\n",
    "    \"thoemmi/thomasfreudenberg.com\",\n",
    "    \"faithworkcamps/faithworkcamps.github.io\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "def execute_with_dynamic_date_range(\n",
    "    execute_query,\n",
    "    process_results,\n",
    "    index,\n",
    "    se_fm_repository_data,\n",
    "    repo,\n",
    "    days_offset=31,\n",
    "    max_total_allowed_results=950,\n",
    "    default_days_interval=60,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes a GraphQL query within dynamically adjusted date ranges to handle large datasets.\n",
    "\n",
    "    :param keywords: List of keywords for search queries.\n",
    "    :param execute_query: Function to execute the query.\n",
    "    :param process_results: Function to process the query results.\n",
    "    :param max_total_allowed_results: Max allowed results before reducing date range.\n",
    "    :param default_days_interval: Initial days interval for date range.\n",
    "    \"\"\"\n",
    "    repository_name_with_owner = repo.get(\"repository_name_with_owner\")\n",
    "    earliest_pr_create_date = repo.get(\"start_date\")\n",
    "    latest_pr_close_date = repo.get(\"end_date\")\n",
    "\n",
    "    if (\n",
    "        not repository_name_with_owner\n",
    "        or not earliest_pr_create_date\n",
    "        or not latest_pr_close_date\n",
    "    ):\n",
    "        return\n",
    "\n",
    "    start_date = earliest_pr_create_date - datetime.timedelta(days=days_offset)\n",
    "    end_date = latest_pr_close_date + datetime.timedelta(days=days_offset)\n",
    "    days_interval = default_days_interval\n",
    "\n",
    "    while start_date < end_date:\n",
    "        next_date_candidate = start_date + datetime.timedelta(days=days_interval)\n",
    "        next_date = min(next_date_candidate, end_date)\n",
    "\n",
    "        try:\n",
    "            after_cursor = None\n",
    "            while True:\n",
    "                date_range = f\"{start_date.strftime('%Y-%m-%dT%H:%M')}..{next_date.strftime('%Y-%m-%dT%H:%M')}\"\n",
    "                search_keyword = f\"-label:spam repo:{repository_name_with_owner} is:pr is:public archived:false created:{date_range}\"\n",
    "                response = execute_query(\n",
    "                    search_keyword, first=10, after_cursor=after_cursor\n",
    "                )\n",
    "\n",
    "                if \"abandon\" in response and response[\"abandon\"]:\n",
    "                    days_interval = default_days_interval  # Reset interval\n",
    "                    break\n",
    "                log_activity(f'response count: {response[\"search\"][\"issueCount\"]}\\n')\n",
    "\n",
    "                if response[\"search\"][\"issueCount\"] == 0:\n",
    "                    days_interval = default_days_interval  # Reset interval\n",
    "                    break\n",
    "\n",
    "                # Adjust interval if issue count exceeds max allowed\n",
    "                if response[\"search\"][\"issueCount\"] > max_total_allowed_results:\n",
    "                    reduced_interval = (\n",
    "                        max(1, days_interval // 2)\n",
    "                        if days_interval > 1\n",
    "                        else max(0.00069, days_interval / 2)\n",
    "                    )\n",
    "                    log_activity(f\"Reducing interval to {reduced_interval} days...\")\n",
    "                    days_interval = reduced_interval\n",
    "                    next_date = start_date + datetime.timedelta(days=days_interval)\n",
    "                    continue\n",
    "\n",
    "                # Process results\n",
    "                process_results(response)\n",
    "\n",
    "                # Pagination\n",
    "                page_info = response[\"search\"][\"pageInfo\"]\n",
    "                if page_info[\"hasNextPage\"]:\n",
    "                    after_cursor = page_info[\"endCursor\"]\n",
    "                else:\n",
    "                    break\n",
    "            with open(\"non-spam-progress.pkl\", \"wb\") as f:\n",
    "                pickle.dump({\"df\": se_fm_repository_data, \"start_index\": index + 1}, f)\n",
    "\n",
    "            # Reset interval to default after a successful run\n",
    "            days_interval = default_days_interval\n",
    "        except Exception as e:\n",
    "            log_activity(\n",
    "                f\"Error fetching data for '{search_keyword}' in range {date_range}: {e}\"\n",
    "            )\n",
    "            # Save progress before terminating\n",
    "            with open(\"non-spam-progress.pkl\", \"wb\") as f:\n",
    "                pickle.dump({\"df\": df, \"start_index\": index}, f)\n",
    "            raise\n",
    "\n",
    "        start_date = next_date  # Move to the next date interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "101f6383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_spammed_pr_repositories(pull_requests, max=10):\n",
    "    \"\"\"\n",
    "    Get the repositories with the most spam pull requests.\n",
    "\n",
    "    This function processes a list of pull requests and calculates the number of spam pull requests\n",
    "    for each repository. It returns a list of dictionaries containing the repository name and the\n",
    "    count of spam pull requests, sorted in descending order by the count.\n",
    "\n",
    "    Args:\n",
    "        pull_requests (list): A list of dictionaries, where each dictionary represents a pull request.\n",
    "            Each dictionary should have the keys \"repository_name_with_owner\" and \"labeled_spam_by\".\n",
    "        max (int): The maximum number of repositories to return. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains:\n",
    "            - str: The repository name.\n",
    "            - int: The count of spam pull requests in the repository.\n",
    "\n",
    "    Example:\n",
    "        pull_requests = [\n",
    "            {\"repository_name_with_owner\": \"repo1\", \"labeled_spam_by\": \"user1\"},\n",
    "            {\"repository_name_with_owner\": \"repo1\", \"labeled_spam_by\": \"user2\"},\n",
    "            {\"repository_name_with_owner\": \"repo2\", \"labeled_spam_by\": \"user1\"},\n",
    "        ]\n",
    "        result = get_most_spammed_pr_repositories(pull_requests)\n",
    "        # result: [{'repository_name_with_owner': 'repo1', 'spam_pr_count': 2},\n",
    "        #          {'repository_name_with_owner': 'repo2', 'spam_pr_count': 1}]\n",
    "    \"\"\"\n",
    "    if not pull_requests:  # Handle empty or None input\n",
    "        return []\n",
    "\n",
    "    spam_tracker = {}\n",
    "    for pr in pull_requests:\n",
    "        repo = pr.get(\"repository_name_with_owner\")\n",
    "\n",
    "        if not repo:\n",
    "            continue\n",
    "\n",
    "        if repo not in spam_tracker:\n",
    "            spam_tracker[repo] = {\"count\": 0, \"details\": []}\n",
    "        spam_tracker[repo][\"count\"] += 1\n",
    "        spam_tracker[repo][\"details\"].append(pr)\n",
    "\n",
    "    sorted_spam_counts = sorted(\n",
    "        spam_tracker.items(), key=lambda x: x[1][\"count\"], reverse=1\n",
    "    )\n",
    "    return [\n",
    "        {\"prs\": repo_data[\"details\"], \"spam_pr_count\": repo_data[\"count\"]}\n",
    "        for _, repo_data in sorted_spam_counts[:max]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21dcc155",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 188\u001b[0m\n\u001b[1;32m    184\u001b[0m     repo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mstrptime(\n\u001b[1;32m    185\u001b[0m         start_date_param, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m     )\n\u001b[1;32m    187\u001b[0m     has_used_start_date_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m \u001b[43mexecute_with_dynamic_date_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocess_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mse_fm_repository_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 48\u001b[0m, in \u001b[0;36mexecute_with_dynamic_date_range\u001b[0;34m(execute_query, process_results, index, se_fm_repository_data, repo, days_offset, max_total_allowed_results, default_days_interval)\u001b[0m\n\u001b[1;32m     46\u001b[0m date_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m..\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m search_keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-label:spam repo:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepository_name_with_owner\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is:pr is:public archived:false created:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_range\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_keyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter_cursor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mafter_cursor\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m log_activity(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabandon\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response \u001b[38;5;129;01mand\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabandon\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[55], line 64\u001b[0m, in \u001b[0;36mexecute_query\u001b[0;34m(keyword, first, after_cursor)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# raise Exception(\"Simulated error for testing failure count logic\")\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariable_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeyword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mafterCursor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter_cursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI rate limit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/gql/client.py:484\u001b[0m, in \u001b[0;36mClient.execute\u001b[0;34m(self, document, variable_values, operation_name, serialize_variables, parse_result, get_execution_result, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Sync transports\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariable_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserialize_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserialize_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_execution_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_execution_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/gql/client.py:248\u001b[0m, in \u001b[0;36mClient.execute_sync\u001b[0;34m(self, document, variable_values, operation_name, serialize_variables, parse_result, get_execution_result, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\":meta private:\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariable_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserialize_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserialize_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_execution_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_execution_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/gql/client.py:1017\u001b[0m, in \u001b[0;36mSyncClientSession.execute\u001b[0;34m(self, document, variable_values, operation_name, serialize_variables, parse_result, get_execution_result, **kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the provided document AST synchronously using\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;124;03mthe sync transport.\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \n\u001b[1;32m   1014\u001b[0m \u001b[38;5;124;03mThe extra arguments are passed to the transport execute method.\"\"\"\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# Validate and execute on the transport\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialize_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserialize_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# Raise an error if an error is returned in the ExecutionResult object\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/gql/client.py:926\u001b[0m, in \u001b[0;36mSyncClientSession._execute\u001b[0;34m(self, document, variable_values, operation_name, serialize_variables, parse_result, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m     result \u001b[38;5;241m=\u001b[39m future_result\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 926\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariable_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# Unserialize the result if requested\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mschema:\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/gql/transport/requests.py:237\u001b[0m, in \u001b[0;36mRequestsHTTPTransport.execute\u001b[0;34m(self, document, variable_values, operation_name, timeout, extra_args, upload_files)\u001b[0m\n\u001b[1;32m    234\u001b[0m     post_args\u001b[38;5;241m.\u001b[39mupdate(extra_args)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Using the created session to perform requests\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpost_args\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_headers \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mheaders\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_response_error\u001b[39m(resp: requests\u001b[38;5;241m.\u001b[39mResponse, reason: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# We raise a TransportServerError if the status code is 400 or higher\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# We raise a TransportProtocolError in the other cases\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Documents/ayo-workspace/iwadi/spam/spam/lib/python3.13/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1430\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1430\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1431\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1432\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot read from timed out object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1303\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def result_processor(\n",
    "    response,\n",
    "):\n",
    "    for edge in response[\"search\"][\"edges\"]:\n",
    "        pull_request = edge[\"node\"]\n",
    "\n",
    "        if not pull_request:\n",
    "            continue\n",
    "        timeline = pull_request[\"timeline\"][\"edges\"]\n",
    "        labeled_spam_event = next(\n",
    "            filter(\n",
    "                lambda x: x[\"node\"]\n",
    "                and x[\"node\"][\"__typename\"] == \"LabeledEvent\"\n",
    "                and x[\"node\"][\"label\"][\"name\"]\n",
    "                and (x[\"node\"][\"label\"][\"name\"]).lower()\n",
    "                in [\"ai spam\", \"spam\", \"ai-spam\"],\n",
    "                timeline,\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        labeled_spam_event_node = (\n",
    "            labeled_spam_event[\"node\"] if labeled_spam_event else None\n",
    "        )\n",
    "        closed_event = next(\n",
    "            filter(\n",
    "                lambda x: x[\"node\"] and x[\"node\"][\"__typename\"] == \"ClosedEvent\",\n",
    "                timeline,\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        closed_event_node = closed_event[\"node\"] if closed_event else None\n",
    "        author = pull_request[\"author\"]\n",
    "        comments = [comment[\"node\"] for comment in pull_request[\"comments\"][\"edges\"]]\n",
    "        labeled_spam_at = (\n",
    "            labeled_spam_event_node[\"createdAt\"] if labeled_spam_event_node else None\n",
    "        )\n",
    "\n",
    "        comments_by_spam_labeler = []\n",
    "\n",
    "        closed_by = (\n",
    "            closed_event_node[\"actor\"][\"login\"]\n",
    "            if closed_event_node and closed_event_node[\"actor\"]\n",
    "            else None\n",
    "        )\n",
    "        end_date = closed_event_node[\"createdAt\"] if closed_event_node else None\n",
    "        comments_by_closer = [\n",
    "            {\n",
    "                **closer_comment,\n",
    "                \"commented_before_closing\": (\n",
    "                    closer_comment[\"createdAt\"] < end_date if end_date else False\n",
    "                ),\n",
    "            }\n",
    "            for closer_comment in comments\n",
    "            if (\n",
    "                (\n",
    "                    closer_comment[\"author\"]\n",
    "                    and closer_comment[\"author\"][\"login\"] == closed_by\n",
    "                )\n",
    "                or (not closer_comment[\"author\"] and not closed_by)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        author_organizations = (\n",
    "            [\n",
    "                organization[\"node\"]\n",
    "                for organization in author[\"organizations\"][\"edges\"]\n",
    "                if organization[\"node\"]\n",
    "            ]\n",
    "            if author\n",
    "            and author.get(\"organizations\")\n",
    "            and author[\"organizations\"].get(\"edges\")\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        timestamp_suffix = f\"_as_at_{datetime.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "        df.append(\n",
    "            {\n",
    "                \"id\": pull_request[\"id\"],\n",
    "                \"title\": pull_request[\"title\"],\n",
    "                \"url\": pull_request[\"url\"],\n",
    "                \"state\": pull_request[\"state\"],\n",
    "                \"comments_count\": pull_request[\"comments\"][\"totalCount\"],\n",
    "                \"comments_by_spam_labeler_count\": len(comments_by_spam_labeler),\n",
    "                \"comments_by_spam_labeler\": comments_by_spam_labeler,\n",
    "                \"labeled_spam_by\": (\n",
    "                    labeled_spam_event_node[\"actor\"][\"login\"]\n",
    "                    if labeled_spam_event_node and labeled_spam_event_node[\"actor\"]\n",
    "                    else None\n",
    "                ),\n",
    "                \"is_labeled_spam_by_bot\": False,\n",
    "                \"labeled_spam_at\": labeled_spam_at,\n",
    "                \"comments_by_closer_count\": len(comments_by_closer),\n",
    "                \"comments_by_closer\": comments_by_closer,\n",
    "                \"closed\": pull_request[\"closed\"],\n",
    "                \"is_closed_by_bot\": closed_by is None and pull_request[\"closed\"],\n",
    "                \"closed_by\": closed_by,\n",
    "                \"closed_at\": end_date,\n",
    "                \"merged\": pull_request[\"merged\"],\n",
    "                \"body\": pull_request[\"body\"],\n",
    "                \"created_at\": pull_request[\"createdAt\"],\n",
    "                \"updated_at\": pull_request[\"updatedAt\"],\n",
    "                \"repository\": pull_request[\"repository\"],\n",
    "                \"repository_name_with_owner\": pull_request[\"repository\"][\n",
    "                    \"nameWithOwner\"\n",
    "                ],\n",
    "                \"repository_stargazer_count\": pull_request[\"repository\"][\n",
    "                    \"stargazerCount\"\n",
    "                ],\n",
    "                \"repository_watcher_count\": pull_request[\"repository\"][\"watchers\"][\n",
    "                    \"totalCount\"\n",
    "                ],\n",
    "                \"repository_is_fork\": pull_request[\"repository\"][\"isFork\"],\n",
    "                \"repository_languages\": [\n",
    "                    language[\"node\"][\"name\"]\n",
    "                    for language in pull_request[\"repository\"][\"languages\"][\"edges\"]\n",
    "                ],\n",
    "                \"merge_commit\": (\n",
    "                    pull_request[\"mergeCommit\"][\"oid\"]\n",
    "                    if pull_request[\"mergeCommit\"]\n",
    "                    else None\n",
    "                ),\n",
    "                \"labels\": [\n",
    "                    label[\"node\"][\"name\"] for label in pull_request[\"labels\"][\"edges\"]\n",
    "                ],\n",
    "                \"commits_count\": pull_request[\"commits\"][\"totalCount\"],\n",
    "                \"changed_files_count\": pull_request[\"changedFiles\"],\n",
    "                \"author_name\": (author[\"login\"] if author else None),\n",
    "                \"author_url\": (author[\"url\"] if author else None),\n",
    "                \"author_account_created_at\": (author[\"createdAt\"] if author else None),\n",
    "                f\"author_repository_count{timestamp_suffix}\": (\n",
    "                    author[\"repositories\"][\"totalCount\"]\n",
    "                    if author and author[\"repositories\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_followers_count{timestamp_suffix}\": (\n",
    "                    author[\"followers\"][\"totalCount\"]\n",
    "                    if author and author[\"followers\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_following_count{timestamp_suffix}\": (\n",
    "                    author[\"following\"][\"totalCount\"]\n",
    "                    if author and author[\"following\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_repository_discussions_count{timestamp_suffix}\": (\n",
    "                    author[\"repositoryDiscussions\"][\"totalCount\"]\n",
    "                    if author and author[\"repositoryDiscussions\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_repository_discussion_comments_count{timestamp_suffix}\": (\n",
    "                    author[\"repositoryDiscussionComments\"][\"totalCount\"]\n",
    "                    if author and author[\"repositoryDiscussionComments\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_organizations{timestamp_suffix}\": author_organizations,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "with open(\"progress.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    spam_prs = data[\"df\"]\n",
    "\n",
    "\n",
    "# template\n",
    "index = start_index\n",
    "se_fm_repository_data = df\n",
    "most_spammed_repos = get_most_spammed_pr_repositories(spam_prs, 10)\n",
    "# Extract top pull requests from the most spammed repositories\n",
    "top_prs = []\n",
    "for repo in most_spammed_repos:\n",
    "    for pr in repo[\"prs\"]:\n",
    "        top_prs.append(pr)\n",
    "\n",
    "repos = get_create_close_max_range_dates(top_prs)\n",
    "\n",
    "has_used_start_date_param = False\n",
    "for repo in repos:\n",
    "\n",
    "    if repo.get(\"repository_name_with_owner\") in repos_to_skip:\n",
    "        continue\n",
    "    if not has_used_start_date_param:\n",
    "        repo[\"start_date\"] = datetime.datetime.strptime(\n",
    "            start_date_param, \"%Y-%m-%dT%H:%M\"\n",
    "        )\n",
    "        has_used_start_date_param = True\n",
    "    execute_with_dynamic_date_range(\n",
    "        execute_query=execute_query,\n",
    "        process_results=result_processor,\n",
    "        repo=repo,\n",
    "        index=start_index,\n",
    "        se_fm_repository_data=df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb89147-ab2c-4654-a8d7-637737c90537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"progress.pkl\", \"rb\") as f:\n",
    "#     data = pickle.load(f)\n",
    "#     spam_prs = data[\"df\"]\n",
    "\n",
    "\n",
    "# # template\n",
    "# index = start_index\n",
    "# se_fm_repository_data = df\n",
    "# most_spammed_repos = get_most_spammed_pr_repositories(spam_prs)\n",
    "# # Extract top pull requests from the most spammed repositories\n",
    "# top_prs = []\n",
    "# for repo in most_spammed_repos:\n",
    "#     for pr in repo[\"prs\"]:\n",
    "#         top_prs.append(pr)\n",
    "\n",
    "# repos = get_create_close_max_range_dates(top_prs)\n",
    "\n",
    "# DAYS_OFFSET = 31\n",
    "\n",
    "# for repo in repos:\n",
    "#     repository_name_with_owner = repo.get(\"repository_name_with_owner\")\n",
    "#     earliest_pr_create_date = repo.get(\"start_date\")\n",
    "#     latest_pr_close_date = repo.get(\"end_date\")\n",
    "\n",
    "#     if (\n",
    "#         not repository_name_with_owner\n",
    "#         or not earliest_pr_create_date\n",
    "#         or not latest_pr_close_date\n",
    "#     ):\n",
    "#         continue\n",
    "#     start_date = (earliest_pr_create_date - datetime.timedelta(days=DAYS_OFFSET)).strftime(\n",
    "#         \"%Y-%m-%d\"\n",
    "#     )\n",
    "#     end_date = (latest_pr_close_date + datetime.timedelta(days=DAYS_OFFSET)).strftime(\"%Y-%m-%d\")\n",
    "#     search_keyword = f\"-label:spam repo:{repository_name_with_owner} is:pr is:public archived:false created:{start_date}..{end_date}\"\n",
    "\n",
    "#     try:\n",
    "#         after_cursor = None\n",
    "#         while True:\n",
    "#             response = execute_query(\n",
    "#                 search_keyword, first=10, after_cursor=after_cursor\n",
    "#             )\n",
    "\n",
    "#             if response[\"search\"][\"issueCount\"] == 0:\n",
    "#                 break\n",
    "#             # Extract pr\n",
    "#             for edge in response[\"search\"][\"edges\"]:\n",
    "#                 pull_request = edge[\"node\"]\n",
    "\n",
    "#                 if not pull_request:\n",
    "#                     continue\n",
    "#                 timeline = pull_request[\"timeline\"][\"edges\"]\n",
    "#                 labeled_spam_event = next(\n",
    "#                     filter(\n",
    "#                         lambda x: x[\"node\"]\n",
    "#                         and x[\"node\"][\"__typename\"] == \"LabeledEvent\"\n",
    "#                         and x[\"node\"][\"label\"][\"name\"]\n",
    "#                         and (x[\"node\"][\"label\"][\"name\"]).lower() == \"spam\",\n",
    "#                         timeline,\n",
    "#                     ),\n",
    "#                     None,\n",
    "#                 )\n",
    "#                 labeled_spam_event_node = (\n",
    "#                     labeled_spam_event[\"node\"] if labeled_spam_event else None\n",
    "#                 )\n",
    "#                 closed_event = next(\n",
    "#                     filter(\n",
    "#                         lambda x: x[\"node\"]\n",
    "#                         and x[\"node\"][\"__typename\"] == \"ClosedEvent\",\n",
    "#                         timeline,\n",
    "#                     ),\n",
    "#                     None,\n",
    "#                 )\n",
    "#                 closed_event_node = closed_event[\"node\"] if closed_event else None\n",
    "#                 author = pull_request[\"author\"]\n",
    "#                 comments = [\n",
    "#                     comment[\"node\"] for comment in pull_request[\"comments\"][\"edges\"]\n",
    "#                 ]\n",
    "#                 labeled_spam_at = (\n",
    "#                     labeled_spam_event_node[\"createdAt\"]\n",
    "#                     if labeled_spam_event_node\n",
    "#                     else None\n",
    "#                 )\n",
    "\n",
    "#                 labeled_spam_by = (\n",
    "#                     labeled_spam_event_node[\"actor\"][\"login\"]\n",
    "#                     if labeled_spam_event_node and labeled_spam_event_node[\"actor\"]\n",
    "#                     else None\n",
    "#                 )\n",
    "#                 comments_by_spam_labeler = []\n",
    "\n",
    "#                 closed_by = (\n",
    "#                     closed_event_node[\"actor\"][\"login\"]\n",
    "#                     if closed_event_node and closed_event_node[\"actor\"]\n",
    "#                     else None\n",
    "#                 )\n",
    "#                 end_date = closed_event_node[\"createdAt\"] if closed_event_node else None\n",
    "#                 comments_by_closer = [\n",
    "#                     {\n",
    "#                         **closer_comment,\n",
    "#                         \"commented_before_closing\": (\n",
    "#                             closer_comment[\"createdAt\"] < end_date\n",
    "#                             if end_date\n",
    "#                             else False\n",
    "#                         ),\n",
    "#                     }\n",
    "#                     for closer_comment in comments\n",
    "#                     if (\n",
    "#                         (\n",
    "#                             closer_comment[\"author\"]\n",
    "#                             and closer_comment[\"author\"][\"login\"] == closed_by\n",
    "#                         )\n",
    "#                         or (not closer_comment[\"author\"] and not closed_by)\n",
    "#                     )\n",
    "#                 ]\n",
    "\n",
    "#                 author_organizations = (\n",
    "#                     [\n",
    "#                         organization[\"node\"]\n",
    "#                         for organization in author[\"organizations\"][\"edges\"]\n",
    "#                         if organization[\"node\"]\n",
    "#                     ]\n",
    "#                     if author\n",
    "#                     and author.get(\"organizations\")\n",
    "#                     and author[\"organizations\"].get(\"edges\")\n",
    "#                     else []\n",
    "#                 )\n",
    "\n",
    "#                 timestamp_suffix = (\n",
    "#                     f\"_as_at_{datetime.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "#                 )\n",
    "\n",
    "#                 df.append(\n",
    "#                     {\n",
    "#                         \"id\": pull_request[\"id\"],\n",
    "#                         \"title\": pull_request[\"title\"],\n",
    "#                         \"url\": pull_request[\"url\"],\n",
    "#                         \"state\": pull_request[\"state\"],\n",
    "#                         \"comments_count\": pull_request[\"comments\"][\"totalCount\"],\n",
    "#                         \"comments_by_spam_labeler_count\": len(comments_by_spam_labeler),\n",
    "#                         \"comments_by_spam_labeler\": comments_by_spam_labeler,\n",
    "#                         \"labeled_spam_by\": (\n",
    "#                             labeled_spam_event_node[\"actor\"][\"login\"]\n",
    "#                             if labeled_spam_event_node\n",
    "#                             and labeled_spam_event_node[\"actor\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         \"is_labeled_spam_by_bot\": False,\n",
    "#                         \"labeled_spam_at\": labeled_spam_at,\n",
    "#                         \"comments_by_closer_count\": len(comments_by_closer),\n",
    "#                         \"comments_by_closer\": comments_by_closer,\n",
    "#                         \"closed\": pull_request[\"closed\"],\n",
    "#                         \"is_closed_by_bot\": closed_by is None\n",
    "#                         and pull_request[\"closed\"],\n",
    "#                         \"closed_by\": closed_by,\n",
    "#                         \"closed_at\": end_date,\n",
    "#                         \"merged\": pull_request[\"merged\"],\n",
    "#                         \"body\": pull_request[\"body\"],\n",
    "#                         \"created_at\": pull_request[\"createdAt\"],\n",
    "#                         \"updated_at\": pull_request[\"updatedAt\"],\n",
    "#                         \"repository\": pull_request[\"repository\"],\n",
    "#                         \"repository_name_with_owner\": pull_request[\"repository\"][\n",
    "#                             \"nameWithOwner\"\n",
    "#                         ],\n",
    "#                         \"repository_stargazer_count\": pull_request[\"repository\"][\n",
    "#                             \"stargazerCount\"\n",
    "#                         ],\n",
    "#                         \"repository_watcher_count\": pull_request[\"repository\"][\n",
    "#                             \"watchers\"\n",
    "#                         ][\"totalCount\"],\n",
    "#                         \"repository_is_fork\": pull_request[\"repository\"][\"isFork\"],\n",
    "#                         \"repository_languages\": [\n",
    "#                             language[\"node\"][\"name\"]\n",
    "#                             for language in pull_request[\"repository\"][\"languages\"][\n",
    "#                                 \"edges\"\n",
    "#                             ]\n",
    "#                         ],\n",
    "#                         \"merge_commit\": (\n",
    "#                             pull_request[\"mergeCommit\"][\"oid\"]\n",
    "#                             if pull_request[\"mergeCommit\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         \"labels\": [\n",
    "#                             label[\"node\"][\"name\"]\n",
    "#                             for label in pull_request[\"labels\"][\"edges\"]\n",
    "#                         ],\n",
    "#                         \"commits_count\": pull_request[\"commits\"][\"totalCount\"],\n",
    "#                         \"changed_files_count\": pull_request[\"changedFiles\"],\n",
    "#                         \"author_name\": (author[\"login\"] if author else None),\n",
    "#                         \"author_url\": (author[\"url\"] if author else None),\n",
    "#                         \"author_account_created_at\": (\n",
    "#                             author[\"createdAt\"] if author else None\n",
    "#                         ),\n",
    "#                         f\"author_repository_count{timestamp_suffix}\": (\n",
    "#                             author[\"repositories\"][\"totalCount\"]\n",
    "#                             if author and author[\"repositories\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         f\"author_followers_count{timestamp_suffix}\": (\n",
    "#                             author[\"followers\"][\"totalCount\"]\n",
    "#                             if author and author[\"followers\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         f\"author_following_count{timestamp_suffix}\": (\n",
    "#                             author[\"following\"][\"totalCount\"]\n",
    "#                             if author and author[\"following\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         f\"author_repository_discussions_count{timestamp_suffix}\": (\n",
    "#                             author[\"repositoryDiscussions\"][\"totalCount\"]\n",
    "#                             if author and author[\"repositoryDiscussions\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         f\"author_repository_discussion_comments_count{timestamp_suffix}\": (\n",
    "#                             author[\"repositoryDiscussionComments\"][\"totalCount\"]\n",
    "#                             if author and author[\"repositoryDiscussionComments\"]\n",
    "#                             else None\n",
    "#                         ),\n",
    "#                         f\"author_organizations{timestamp_suffix}\": author_organizations,\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#             # Pagination\n",
    "#             page_info = response[\"search\"][\"pageInfo\"]\n",
    "#             if page_info[\"hasNextPage\"]:\n",
    "#                 after_cursor = page_info[\"endCursor\"]\n",
    "#             else:\n",
    "#                 break\n",
    "#         with open(\"non-spam-progress.pkl\", \"wb\") as f:\n",
    "#             pickle.dump({\"df\": se_fm_repository_data, \"start_index\": index + 1}, f)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         log_activity(f\"Failed to retrieve data for search_keyword '{search_keyword}': {e}\")\n",
    "#         # Save progress before terminating\n",
    "#         with open(\"non-spam-progress.pkl\", \"wb\") as f:\n",
    "#             pickle.dump({\"df\": df, \"start_index\": index}, f)\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the .pkl file\n",
    "with open(\"non-spam-progress.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Check if it's a list of dictionaries\n",
    "if isinstance(data[\"df\"], list) and all(isinstance(d, dict) for d in data[\"df\"]):\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data[\"df\"])\n",
    "    \n",
    "    # Remove duplicates by 'id'\n",
    "    df = df.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "\n",
    "    # Convert back to a list of dictionaries\n",
    "    cleaned_data = {**data, \"df\": df.to_dict(orient=\"records\")}\n",
    "\n",
    "# Check if it's already a DataFrame\n",
    "elif isinstance(data[\"df\"], pd.DataFrame):\n",
    "    # Remove duplicates by 'id'\n",
    "    cleaned_df = data[\"df\"].drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "    cleaned_data = {**data, \"df\": cleaned_df}\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(\"non-spam-progress.pkl\", \"wb\") as file:\n",
    "    pickle.dump(cleaned_data, file)\n",
    "\n",
    "log_activity(\"Duplicates removed and data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bca9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_pkl_content_as_csv_and_json(filepath):\n",
    "    \"\"\"\n",
    "    Display the content of a pickle file and save it as CSV and JSON files.\n",
    "\n",
    "    This function reads a pickle file from the given filepath, prints its content,\n",
    "    and saves the data contained in the \"df\" key to both a CSV file and a JSON file.\n",
    "    The CSV file is saved with the name \"spam_data_without_org_join_date.csv\" and the JSON file is saved\n",
    "    with the name \"spam_data_without_org_join_date.json\".\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the pickle file to be read.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error reading the pickle file or writing the CSV/JSON files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        log_activity(f\"Content of {filepath}:\\n\")\n",
    "        filename = \"non-spam_data_without_org_join_date\"\n",
    "        pd.DataFrame(data[\"df\"]).to_csv(f\"{filename}.csv\", index=True)\n",
    "        log_activity(f\"Data written to {filename}.csv successfully.\")\n",
    "        try:\n",
    "            with open(f\"{filename}.json\", \"w\") as f:\n",
    "                json.dump(data[\"df\"], f, indent=4)\n",
    "            log_activity(f\"Data written to {filename}.json successfully.\")\n",
    "        except Exception as e:\n",
    "            log_activity(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        log_activity(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = \"non-spam-progress.pkl\"\n",
    "save_pkl_content_as_csv_and_json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0deeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate metadata\n",
    "df = []\n",
    "start_index = 0\n",
    "\n",
    "\n",
    "\n",
    "def generate_metadata(filepath):\n",
    "    \"\"\" \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        log_activity(f\"Content of {filepath}:\\n\")\n",
    "        filename = \"non-spam_data.meta\"\n",
    "        pull_requests = data[\"df\"]\n",
    "        unique_repository = {}\n",
    "        unique_pr_author = {}\n",
    "        unique_pr_spam_labeler = {}\n",
    "        unique_pr_closer = {}\n",
    "        merged_pr_count = 0\n",
    "        closed_pr_count = 0\n",
    "        for pull_request in pull_requests:\n",
    "            def update_unique_value_dict(info_dict, key, value):\n",
    "                if not value:\n",
    "                    log_activity(f\"Warning: Pull request missing '{key}' {pull_request}\")\n",
    "                    return False\n",
    "                if value not in info_dict:\n",
    "                    info_dict[value] = value\n",
    "                return True\n",
    "\n",
    "            # Update repository count\n",
    "            update_unique_value_dict(unique_repository, \"repository_name_with_owner\", pull_request[\"repository_name_with_owner\"])\n",
    "\n",
    "            # Update author count\n",
    "            update_unique_value_dict(unique_pr_author, \"author_name\", pull_request[\"author_name\"])\n",
    "\n",
    "            # Update spam labeler count\n",
    "            update_unique_value_dict(unique_pr_spam_labeler, \"labeled_spam_by\", pull_request[\"labeled_spam_by\"])\n",
    "\n",
    "            # Update closer count\n",
    "            update_unique_value_dict(unique_pr_closer, \"closed_by\", pull_request[\"closed_by\"])\n",
    "\n",
    "            merged_pr_count += 1 if pull_request[\"merged\"] else 0\n",
    "            closed_pr_count += 1 if pull_request[\"closed\"] is not None else 0\n",
    "\n",
    "\n",
    "        total_prs= len(pull_requests)\n",
    "        unique_repository_count= len(unique_repository)\n",
    "        unique_pr_author_count= len(unique_pr_author)\n",
    "        unique_pr_spam_labeler_count= len(unique_pr_spam_labeler)\n",
    "        unique_pr_closer_count= len(unique_pr_closer)\n",
    "        \n",
    "        df.append(\n",
    "            {\n",
    "            \"total_prs\": total_prs,\n",
    "            \"unique_repository_count\": unique_repository_count,\n",
    "            \"unique_pr_author_count\": unique_pr_author_count,\n",
    "            \"unique_pr_author_ratio\": round(unique_pr_author_count / total_prs, 3),\n",
    "            \"unique_pr_spam_labeler_count\": unique_pr_spam_labeler_count,\n",
    "            \"unique_pr_spam_labeler_ratio\": round(unique_pr_spam_labeler_count / total_prs, 3),\n",
    "            \"unique_pr_closer_count\": unique_pr_closer_count,\n",
    "            \"unique_pr_closer_ratio\": round(unique_pr_closer_count / total_prs, 3),\n",
    "            \"merged_pr_count\": merged_pr_count,\n",
    "            \"merged_pr_ratio\": round(merged_pr_count / total_prs, 3),\n",
    "            \"closed_pr_count\": closed_pr_count,\n",
    "            \"closed_pr_ratio\": round(closed_pr_count / total_prs, 3),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        pd.DataFrame(df).to_csv(f\"{filename}.csv\", index=True)\n",
    "        log_activity(f\"Data written to {filename}.csv successfully.\")\n",
    "        try:\n",
    "            with open(f\"{filename}.json\", \"w\") as f:\n",
    "                json.dump(df, f, indent=4)\n",
    "            log_activity(f\"Data written to {filename}.json successfully.\")\n",
    "        except Exception as e:\n",
    "            log_activity(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        log_activity(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = \"non-spam-progress.pkl\"\n",
    "generate_metadata(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469812ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
