{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4ece8b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing progress file: jabref-prs-progress.pkl\n",
      "Removed existing log file: jabref-prs-output.log\n"
     ]
    }
   ],
   "source": [
    "# Clear existing progress for fresh start\n",
    "import os\n",
    "progress_file = \"jabref-prs-progress.pkl\"\n",
    "if os.path.exists(progress_file):\n",
    "    os.remove(progress_file)\n",
    "    print(f\"Removed existing progress file: {progress_file}\")\n",
    "else:\n",
    "    print(\"No existing progress file found\")\n",
    "\n",
    "# Also clear the log file for clean debugging\n",
    "log_file = \"jabref-prs-output.log\"\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "    print(f\"Removed existing log file: {log_file}\")\n",
    "else:\n",
    "    print(\"No existing log file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4123d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Read tokens from a text file\n",
    "tokens_file = \"./env/tokens.txt\"\n",
    "with open(tokens_file, \"r\") as file:\n",
    "    tokens = file.read().splitlines()\n",
    "\n",
    "# Create an iterator to cycle through the tokens\n",
    "token_iterator = itertools.cycle(tokens)\n",
    "current_token = next(token_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9b28fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of User-Agents for randomization\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "]\n",
    "\n",
    "# Define headers to authenticate using the first token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {current_token}\",\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "}\n",
    "\n",
    "# Setup GraphQL endpoint and client\n",
    "graphql_url = \"https://api.github.com/graphql\"\n",
    "transport = RequestsHTTPTransport(url=graphql_url, headers=headers, use_json=True)\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0ea0a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'jabref'\n",
    "def log_activity(activity: str):\n",
    "    log = f\"{datetime.datetime.now()}: {activity}\\n\"\n",
    "    # print(log)\n",
    "    with open(f\"{tag}-prs-output.log\", \"a\") as log_file:\n",
    "        log_file.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7703fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all tokens to verify their validity\n",
    "def test_all_tokens():\n",
    "    test_query = gql(\n",
    "        \"\"\"\n",
    "        {\n",
    "          viewer {\n",
    "            login\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    for i, token in enumerate(tokens):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"User-Agent\": random.choice(user_agents),\n",
    "        }\n",
    "        transport = RequestsHTTPTransport(\n",
    "            url=graphql_url, headers=headers, use_json=True\n",
    "        )\n",
    "        client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "        try:\n",
    "            response = client.execute(test_query)\n",
    "            log_activity(\n",
    "                f\"Token {i+1}/{len(tokens)} is valid. Logged in as: {response['viewer']['login']}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log_activity(f\"Token {i+1}/{len(tokens)} failed with error: {e}\")\n",
    "\n",
    "\n",
    "# Run the token validation\n",
    "test_all_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3abbf51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphQL query\n",
    "query_template = gql(\n",
    "    \"\"\"\n",
    "    query searchIssues($keyword: String!, $afterCursor: String, $first: Int) {\n",
    "      search(query: $keyword, type: ISSUE, first: $first, after: $afterCursor) {\n",
    "        issueCount\n",
    "        edges {\n",
    "          cursor\n",
    "          node {\n",
    "            ... on PullRequest {\n",
    "              id\n",
    "              number\n",
    "              title\n",
    "              url\n",
    "              comments {\n",
    "                totalCount\n",
    "              }\n",
    "              state\n",
    "              closed\n",
    "              merged\n",
    "              createdAt\n",
    "              updatedAt\n",
    "              mergeCommit {\n",
    "                oid\n",
    "              }\n",
    "              timeline(last: 100) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    __typename\n",
    "                    ... on ClosedEvent { \n",
    "                      actor { ... on User { login } }\n",
    "                      createdAt\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              commits {\n",
    "                totalCount\n",
    "              }\n",
    "              changedFiles\n",
    "              additions\n",
    "              deletions\n",
    "              headRefName\n",
    "              baseRefName\n",
    "              repository {\n",
    "                id\n",
    "                nameWithOwner\n",
    "                stargazerCount\n",
    "                description\n",
    "                codeOfConduct {\n",
    "                  body\n",
    "                  id\n",
    "                  name\n",
    "                  url\n",
    "                }\n",
    "                homepageUrl\n",
    "                forkCount\n",
    "                watchers {\n",
    "                  totalCount\n",
    "                }\n",
    "                isFork\n",
    "                languages(first: 20) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      name\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              author {\n",
    "                 ... on User {\n",
    "                  login\n",
    "                  url\n",
    "                }\n",
    "                ... on Mannequin {\n",
    "                  login\n",
    "                  url\n",
    "                }\n",
    "                ... on Bot {\n",
    "                  login\n",
    "                  url\n",
    "                }\n",
    "              }\n",
    "              labels(first: 20) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    name\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              body\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        pageInfo {\n",
    "          endCursor\n",
    "          hasNextPage\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a998256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_contributor_count(repo_owner, repo_name):\n",
    "#     global current_token\n",
    "#     max_retries = 3\n",
    "#     retries = 0\n",
    "#     while retries < max_retries:\n",
    "#         try:\n",
    "#             # Randomize User-Agent for each query\n",
    "#             headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "#             headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "#             url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contributors?per_page=1&anon=true\"\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             if response.status_code == 200:\n",
    "#                 return int(response.headers.get(\"Link\", \"\").split(\",\")[-1].split(\"&page=\")[-1].split(\">\")[0]) if \"Link\" in response.headers else len(response.json())\n",
    "#             elif response.status_code == 403:\n",
    "#                 print(f\"Rate limit exceeded, switching token... (Attempt {retries + 1}/{max_retries})\")\n",
    "#                 current_token = next(token_iterator)\n",
    "#                 retries += 1\n",
    "#             else:\n",
    "#                 response.raise_for_status()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error: {e}, retrying... (Attempt {retries + 1}/{max_retries})\")\n",
    "#             retries += 1\n",
    "#     raise Exception(\"Max retries reached. Unable to complete the request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0754bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.headers = headers\n",
    "# Check rate limit before executing the main query\n",
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "rate_limit_response = client.execute(rate_limit_query)\n",
    "log_activity(f\"Rate limit: {rate_limit_response['rateLimit']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cf460a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def execute_query(keyword, first=100, after_cursor=None):\n",
    "    global current_token\n",
    "    log_activity(\n",
    "        f\"Executing query with keyword: {keyword}, first: {first}, afterCursor: {after_cursor}\"\n",
    "    )\n",
    "    while True:\n",
    "        try:\n",
    "            # Randomize User-Agent for each query\n",
    "            headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "            transport.headers = headers\n",
    "            # Check rate limit before executing the main query\n",
    "            rate_limit_response = client.execute(rate_limit_query)\n",
    "            remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "            if remaining < 100:\n",
    "                log_activity(\n",
    "                    f\"Rate limit remaining ({remaining}) is below threshold. Switching token...\"\n",
    "                )\n",
    "                # Set up to track whether we have cycled through all tokens\n",
    "                all_tokens_checked = False\n",
    "                initial_token = current_token\n",
    "\n",
    "                while not all_tokens_checked:\n",
    "                    # Switch to the next token\n",
    "                    current_token = next(token_iterator)\n",
    "                    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "                    transport.headers = headers\n",
    "\n",
    "                    # Check the rate limit of the new token\n",
    "                    rate_limit_response = client.execute(rate_limit_query)\n",
    "                    remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "\n",
    "                    if remaining >= 100:\n",
    "                        log_activity(\n",
    "                            f\"Switched to a new token with sufficient rate limit ({remaining} remaining).\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "                    # Check if we have cycled through all tokens\n",
    "                    if current_token == initial_token:\n",
    "                        log_activity(\"All tokens are below threshold. Waiting for 1 hour...\")\n",
    "                        time.sleep(3600)\n",
    "                        all_tokens_checked = True\n",
    "\n",
    "                continue\n",
    "            return client.execute(\n",
    "                query_template,\n",
    "                variable_values={\n",
    "                    \"keyword\": keyword,\n",
    "                    \"first\": first,\n",
    "                    \"afterCursor\": after_cursor,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"API rate limit\" in str(e):\n",
    "                log_activity(\n",
    "                    f\"Rate limit reached: {e}, switching token... (Attempt with first {first})\"\n",
    "                )\n",
    "                current_token = next(token_iterator)\n",
    "                headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "            else:\n",
    "                if first > 1:\n",
    "                    first = max(1, first // 2)\n",
    "                    log_activity(\n",
    "                    f\"Error: {e}, reducing number of results and retrying... (Attempt with first {first})\"\n",
    "                    )\n",
    "                else:\n",
    "                    log_activity(f\"Query failed completely after retries: {e}\")\n",
    "                    break\n",
    "    log_activity(\"Max retries reached. Sleeping for 30 minutes and switching token...\")\n",
    "    time.sleep(1800)\n",
    "    current_token = next(token_iterator)\n",
    "    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "    transport.headers = headers\n",
    "    return execute_query(keyword, first, after_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "202e0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "progress_pkl = f\"{tag}-prs-progress.pkl\"\n",
    "if os.path.exists(progress_pkl):\n",
    "    with open(progress_pkl, \"rb\") as f:\n",
    "        progress_data = pickle.load(f)\n",
    "        df = progress_data[\"df\"]\n",
    "        start_index = progress_data[\"start_index\"]\n",
    "else:\n",
    "    df = []\n",
    "    start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "47cabaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = []\n",
    "# start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "930ec50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_param = \"2014-02-09T00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def execute_with_dynamic_date_range(\n",
    "    repository,\n",
    "    execute_query,\n",
    "    process_results,\n",
    "    start_date_param,\n",
    "    index,\n",
    "    se_fm_repository_data,\n",
    "    max_total_allowed_results=950,\n",
    "    default_days_interval=60,\n",
    "    max_prs_limit=30,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes a GraphQL query within dynamically adjusted date ranges to handle large datasets.\n",
    "\n",
    "    :param repository: Repository name in format \"owner/repo\" to search PRs for.\n",
    "    :param execute_query: Function to execute the query.\n",
    "    :param process_results: Function to process the query results.\n",
    "    :param start_date_param: Start date in ISO format (\"%Y-%m-%dT%H:%M\").\n",
    "    :param max_total_allowed_results: Max allowed results before reducing date range.\n",
    "    :param default_days_interval: Initial days interval for date range.\n",
    "    :param max_prs_limit: Maximum number of PRs to collect (for testing purposes).\n",
    "    \"\"\"\n",
    "    start_date = datetime.datetime.strptime(start_date_param, \"%Y-%m-%dT%H:%M\")\n",
    "    current_date = datetime.datetime.now()\n",
    "    end_date = current_date\n",
    "    days_interval = default_days_interval\n",
    "\n",
    "    while start_date < end_date:\n",
    "        # Check if we've reached the PR limit\n",
    "        current_count = len(se_fm_repository_data)\n",
    "        log_activity(f\"Main loop: current PR count = {current_count}, limit = {max_prs_limit}\")\n",
    "            \n",
    "        next_date_candidate = start_date + datetime.timedelta(days=days_interval)\n",
    "        next_date = min(next_date_candidate, end_date)\n",
    "        date_range = f\"{start_date.strftime('%Y-%m-%dT%H:%M')}..{next_date.strftime('%Y-%m-%dT%H:%M')}\"\n",
    "\n",
    "        try:\n",
    "            after_cursor = None\n",
    "            while True:\n",
    "                search_keyword = f\"repo:{repository} is:pr is:public archived:false created:{date_range}\"\n",
    "                response = execute_query(\n",
    "                    search_keyword, first=10, after_cursor=after_cursor\n",
    "                )\n",
    "                log_activity(\n",
    "                    f'response count: {response[\"search\"][\"issueCount\"]}\\n'\n",
    "                )\n",
    "\n",
    "                if response[\"search\"][\"issueCount\"] == 0:\n",
    "                    days_interval = default_days_interval  # Reset interval\n",
    "                    break\n",
    "\n",
    "                # Adjust interval if issue count exceeds max allowed\n",
    "                if response[\"search\"][\"issueCount\"] > max_total_allowed_results:\n",
    "                    reduced_interval = (\n",
    "                        max(1, days_interval // 2)\n",
    "                        if days_interval > 1\n",
    "                        else max(0.00069, days_interval / 2)\n",
    "                    )\n",
    "                    log_activity(f\"Reducing interval to {reduced_interval} days...\")\n",
    "                    days_interval = reduced_interval\n",
    "                    next_date = start_date + datetime.timedelta(days=days_interval)\n",
    "                    continue\n",
    "\n",
    "                # Process results\n",
    "                process_results(response, repository)\n",
    "                current_count_after = len(se_fm_repository_data)\n",
    "                log_activity(f\"After processing: current PR count = {current_count_after}, limit = {max_prs_limit}\")\n",
    "                if current_count_after >= max_prs_limit:\n",
    "                    log_activity(f\"Reached PR limit of {max_prs_limit} after processing. Stopping collection.\")\n",
    "                    # Save progress before returning\n",
    "                    with open(progress_pkl, \"wb\") as f:\n",
    "                        pickle.dump(\n",
    "                            {\"df\": se_fm_repository_data, \"start_index\": index + 1}, f\n",
    "                        )\n",
    "                    return\n",
    "\n",
    "                # Pagination\n",
    "                page_info = response[\"search\"][\"pageInfo\"]\n",
    "                if page_info[\"hasNextPage\"]:\n",
    "                    after_cursor = page_info[\"endCursor\"]\n",
    "                else:\n",
    "                    break\n",
    "            with open(progress_pkl, \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    {\"df\": se_fm_repository_data, \"start_index\": index + 1}, f\n",
    "                )\n",
    "            # Reset interval to default after a successful run\n",
    "            days_interval = default_days_interval\n",
    "        except Exception as e:\n",
    "            log_activity(\n",
    "                f\"Error fetching data for '{repository}' in range {date_range}: {e}\"\n",
    "            )\n",
    "            # Save progress before terminating\n",
    "            with open(progress_pkl, \"wb\") as f:\n",
    "                pickle.dump({\"df\": df, \"start_index\": index}, f)\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "        start_date = next_date  # Move to the next date interval        start_date = next_date  # Move to the next date interval\n",
    "        start_date = next_date  # Move to the next date interval        start_date = next_date  # Move to the next date interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3262c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_processor(response, repository):\n",
    "    initial_count = len(df)\n",
    "    processed_count = 0\n",
    "    \n",
    "    for edge in response[\"search\"][\"edges\"]:\n",
    "        pull_request = edge[\"node\"]\n",
    "\n",
    "        if not pull_request:\n",
    "            continue\n",
    "        timeline = pull_request[\"timeline\"][\"edges\"]\n",
    "        closed_event = next(\n",
    "            filter(\n",
    "                lambda x: x[\"node\"] and x[\"node\"][\"__typename\"] == \"ClosedEvent\",\n",
    "                timeline,\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        closed_event_node = closed_event[\"node\"] if closed_event else None\n",
    "        author = pull_request[\"author\"]\n",
    "        \n",
    "        closed_by = (\n",
    "            closed_event_node[\"actor\"][\"login\"]\n",
    "            if closed_event_node and closed_event_node[\"actor\"]\n",
    "            else None\n",
    "        )\n",
    "        closed_at = closed_event_node[\"createdAt\"] if closed_event_node else None\n",
    "      \n",
    "\n",
    "        df.append(\n",
    "            {\n",
    "                \"id\": pull_request[\"id\"],\n",
    "                \"title\": pull_request[\"title\"],\n",
    "                \"url\": pull_request[\"url\"],\n",
    "                \"state\": pull_request[\"state\"],\n",
    "                \"comments_count\": pull_request[\"comments\"][\"totalCount\"],\n",
    "                \"closed\": pull_request[\"closed\"],\n",
    "                \"is_closed_by_bot\": closed_by is None,\n",
    "                \"closed_by\": closed_by,\n",
    "                \"closed_at\": closed_at,\n",
    "                \"merged\": pull_request[\"merged\"],\n",
    "                \"body\": pull_request[\"body\"],\n",
    "                \"created_at\": pull_request[\"createdAt\"],\n",
    "                \"updated_at\": pull_request[\"updatedAt\"],\n",
    "                \"repository\": pull_request[\"repository\"],\n",
    "                \"repository_name_with_owner\": pull_request[\"repository\"][\n",
    "                    \"nameWithOwner\"\n",
    "                ],\n",
    "                \"repository_stargazer_count\": pull_request[\"repository\"][\n",
    "                    \"stargazerCount\"\n",
    "                ],\n",
    "                \"repository_watcher_count\": pull_request[\"repository\"][\"watchers\"][\n",
    "                    \"totalCount\"\n",
    "                ],\n",
    "                \"repository_is_fork\": pull_request[\"repository\"][\"isFork\"],\n",
    "                \"repository_languages\": [\n",
    "                    language[\"node\"][\"name\"]\n",
    "                    for language in pull_request[\"repository\"][\"languages\"][\"edges\"]\n",
    "                ],\n",
    "                \"merge_commit\": (\n",
    "                    pull_request[\"mergeCommit\"][\"oid\"]\n",
    "                    if pull_request[\"mergeCommit\"]\n",
    "                    else None\n",
    "                ),\n",
    "                \"labels\": [\n",
    "                    label[\"node\"][\"name\"] for label in pull_request[\"labels\"][\"edges\"]\n",
    "                ],\n",
    "                \"commits_count\": pull_request[\"commits\"][\"totalCount\"],\n",
    "                \"changed_files_count\": pull_request[\"changedFiles\"],\n",
    "                \"additions_count\": pull_request[\"additions\"],\n",
    "                \"deletions_count\": pull_request[\"deletions\"],\n",
    "                \"author_name\": (author[\"login\"] if author else None),\n",
    "                \"author_url\": (author[\"url\"] if author else None),\n",
    "                \"search_repository\": repository,\n",
    "            }\n",
    "        )\n",
    "        processed_count += 1\n",
    "    \n",
    "    final_count = len(df)\n",
    "    log_activity(f\"result_processor: started with {initial_count}, processed {processed_count} PRs, final count: {final_count}\")\n",
    "\n",
    "\n",
    "execute_with_dynamic_date_range(\n",
    "    repository=\"JabRef/jabref\",\n",
    "    execute_query=execute_query,\n",
    "    process_results=result_processor,\n",
    "    start_date_param=start_date_param,\n",
    "    index=start_index,\n",
    "    se_fm_repository_data = df,\n",
    "    max_prs_limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e63a323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the .pkl file\n",
    "with open(progress_pkl, \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Initialize cleaned_data\n",
    "cleaned_data = None\n",
    "\n",
    "# Check if it's a list of dictionaries\n",
    "if isinstance(data[\"df\"], list) and all(isinstance(d, dict) for d in data[\"df\"]):\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data[\"df\"])\n",
    "    \n",
    "    log_activity(f\"len => {len(df)}\")\n",
    "    # Remove duplicates by 'id'\n",
    "    df = df.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "\n",
    "    # Convert back to a list of dictionaries\n",
    "    cleaned_data = {**data, \"df\": df.to_dict(orient=\"records\")}\n",
    "\n",
    "# Check if it's already a DataFrame\n",
    "elif isinstance(data[\"df\"], pd.DataFrame):\n",
    "    log_activity(f\"len2 => {len(df)}\")\n",
    "    # Remove duplicates by 'id'\n",
    "    cleaned_df = data[\"df\"].drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "    cleaned_data = {**data, \"df\": cleaned_df}\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(progress_pkl, \"wb\") as file:\n",
    "    pickle.dump(cleaned_data, file)\n",
    "\n",
    "log_activity(\"Duplicates removed and data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3f4c9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_pkl_content_as_csv_and_json(filepath):\n",
    "    \"\"\"\n",
    "    This function reads a pickle file from the given filepath\n",
    "    and saves the data contained in the \"df\" key to both a CSV file and a JSON file.\n",
    "    The CSV file is saved with the name \"{filepath}.csv\" and the JSON file is saved\n",
    "    with the name \"{filepath}.json\".\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the pickle file to be read.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error reading the pickle file or writing the CSV/JSON files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        log_activity(f\"Content of {filepath}:\\n\")\n",
    "        filename = f\"{tag}-prs\"\n",
    "        pd.DataFrame(data[\"df\"]).to_csv(f\"{filename}.csv\", index=True)\n",
    "        log_activity(f\"Data written to {filename}.csv successfully.\")\n",
    "        try:\n",
    "            with open(f\"{filename}.json\", \"w\") as f:\n",
    "                json.dump(data[\"df\"], f, indent=4)\n",
    "            log_activity(f\"Data written to {filename}.json successfully.\")\n",
    "        except Exception as e:\n",
    "            log_activity(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        log_activity(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = progress_pkl\n",
    "save_pkl_content_as_csv_and_json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bfbdb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate metadata\n",
    "df = []\n",
    "start_index = 0\n",
    "\n",
    "\n",
    "\n",
    "def generate_metadata(filepath):\n",
    "    \"\"\" \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        log_activity(f\"Content of {filepath}:\\n\")\n",
    "        filename = f\"{tag}-prs.meta\"\n",
    "        pull_requests = data[\"df\"]\n",
    "        unique_repository = {}\n",
    "        unique_pr_author = {}\n",
    "        unique_pr_closer = {}\n",
    "        merged_pr_count = 0\n",
    "        closed_pr_count = 0\n",
    "        for pull_request in pull_requests:\n",
    "            def update_unique_value_dict(info_dict, key, value):\n",
    "                if not value:\n",
    "                    log_activity(f\"Warning: Pull request missing '{key}' {pull_request[\"id\"]}\")\n",
    "                    return False\n",
    "                if value not in info_dict:\n",
    "                    info_dict[value] = value\n",
    "                return True\n",
    "\n",
    "            # Update repository count\n",
    "            update_unique_value_dict(unique_repository, \"repository_name_with_owner\", pull_request[\"repository_name_with_owner\"])\n",
    "\n",
    "            # Update author count\n",
    "            update_unique_value_dict(unique_pr_author, \"author_name\", pull_request[\"author_name\"])\n",
    "\n",
    "\n",
    "            # Update closer count\n",
    "            update_unique_value_dict(unique_pr_closer, \"closed_by\", pull_request[\"closed_by\"])\n",
    "\n",
    "            merged_pr_count += 1 if pull_request[\"merged\"] else 0\n",
    "            closed_pr_count += 1 if pull_request[\"closed\"] is not None else 0\n",
    "\n",
    "\n",
    "        total_prs= len(pull_requests)\n",
    "        unique_repository_count= len(unique_repository)\n",
    "        unique_pr_author_count= len(unique_pr_author)\n",
    "        unique_pr_closer_count= len(unique_pr_closer)\n",
    "        \n",
    "        df.append(\n",
    "            {\n",
    "            \"total_prs\": total_prs,\n",
    "            \"unique_repository_count\": unique_repository_count,\n",
    "            \"unique_repository_ratio\": round(unique_repository_count / total_prs, 3),\n",
    "            \"unique_pr_author_count\": unique_pr_author_count,\n",
    "            \"unique_pr_author_ratio\": round(unique_pr_author_count / total_prs, 3),\n",
    "            \"unique_pr_closer_count\": unique_pr_closer_count,\n",
    "            \"unique_pr_closer_ratio\": round(unique_pr_closer_count / total_prs, 3),\n",
    "            \"merged_pr_count\": merged_pr_count,\n",
    "            \"merged_pr_ratio\": round(merged_pr_count / total_prs, 3),\n",
    "            \"closed_pr_count\": closed_pr_count,\n",
    "            \"closed_pr_ratio\": round(closed_pr_count / total_prs, 3),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        pd.DataFrame(df).to_csv(f\"{filename}.csv\", index=True)\n",
    "        log_activity(f\"Data written to {filename}.csv successfully.\")\n",
    "        try:\n",
    "            with open(f\"{filename}.json\", \"w\") as f:\n",
    "                json.dump(df, f, indent=4)\n",
    "            log_activity(f\"Data written to {filename}.json successfully.\")\n",
    "        except Exception as e:\n",
    "            log_activity(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        log_activity(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = progress_pkl\n",
    "generate_metadata(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f2e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
