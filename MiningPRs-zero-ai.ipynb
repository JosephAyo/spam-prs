{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e824625",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_name = 'tensorflow/tensorflow'\n",
    "# repository_name = 'zero-ai-prs'\n",
    "repository_created_at = \"2024-11-08T05:07:28\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "parsed_repo_name = repository_name.replace(\"/\", \"__\")\n",
    "os.makedirs(f\"../datasets/{parsed_repo_name}\", exist_ok=True)\n",
    "pkl_filename = f'../datasets/{parsed_repo_name}/{parsed_repo_name}-progress.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babdda8-11f7-485e-bad8-d8c5b8eebb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import itertools\n",
    "import datetime\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Read tokens from a text file\n",
    "tokens_file = \"./env/tokens.txt\"\n",
    "with open(tokens_file, \"r\") as file:\n",
    "    tokens = file.read().splitlines()\n",
    "\n",
    "# Choose a random start index\n",
    "start_index = random.randint(0, len(tokens) - 1)\n",
    "\n",
    "# Rotate the tokens list starting at a random position\n",
    "rotated_tokens = tokens[start_index:] + tokens[:start_index]\n",
    "\n",
    "# Create an infinite cycle iterator from the rotated list\n",
    "token_iterator = itertools.cycle(rotated_tokens)\n",
    "\n",
    "# Get the first token\n",
    "current_token = next(token_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c281d1-993b-497b-8984-0e32ac6a61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of User-Agents for randomization\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "]\n",
    "\n",
    "# Define headers to authenticate using the first token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {current_token}\",\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "}\n",
    "\n",
    "# Setup GraphQL endpoint and client\n",
    "graphql_url = \"https://api.github.com/graphql\"\n",
    "transport = RequestsHTTPTransport(url=graphql_url, headers=headers, use_json=True)\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_activity(activity: str):\n",
    "    log = f\"{datetime.datetime.now()}: {activity}\\n\"\n",
    "    # print(log)\n",
    "    with open(f\"../datasets/{parsed_repo_name}/{parsed_repo_name}-output.log\", \"a\") as log_file:\n",
    "        log_file.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062f1b0-a9df-4f94-bb18-56f42774bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all tokens to verify their validity\n",
    "def test_all_tokens():\n",
    "    test_query = gql(\n",
    "        \"\"\"\n",
    "        {\n",
    "          viewer {\n",
    "            login\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    for i, token in enumerate(rotated_tokens):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"User-Agent\": random.choice(user_agents),\n",
    "        }\n",
    "        transport = RequestsHTTPTransport(\n",
    "            url=graphql_url, headers=headers, use_json=True\n",
    "        )\n",
    "        client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "        try:\n",
    "            response = client.execute(test_query)\n",
    "            log_activity(\n",
    "                f\"Token {i+1}/{len(rotated_tokens)} is valid. Logged in as: {response['viewer']['login']}\"\n",
    "    )\n",
    "        except Exception as e:\n",
    "            log_activity(f\"Token {i+1}/{len(rotated_tokens)} failed with error: {e}\")\n",
    "\n",
    "\n",
    "# Run the token validation\n",
    "test_all_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd85df0-71b7-46c8-9ed4-eceb724201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphQL query\n",
    "query_template = gql(\n",
    "    \"\"\"\n",
    "    query searchIssues($keyword: String!, $afterCursor: String, $first: Int) {\n",
    "      search(query: $keyword, type: ISSUE, first: $first, after: $afterCursor) {\n",
    "        issueCount\n",
    "        edges {\n",
    "          cursor\n",
    "          node {\n",
    "            ... on PullRequest {\n",
    "              id\n",
    "              number\n",
    "              title\n",
    "              url\n",
    "              comments {\n",
    "                totalCount\n",
    "              }\n",
    "              state\n",
    "              closed\n",
    "              merged\n",
    "              createdAt\n",
    "              updatedAt\n",
    "              closedAt\n",
    "              deletions\n",
    "              mergeCommit {\n",
    "                oid\n",
    "              }\n",
    "              timeline {\n",
    "                totalCount\n",
    "              }\n",
    "              commits {\n",
    "                totalCount\n",
    "              }\n",
    "              changedFiles\n",
    "              headRefName\n",
    "              baseRefName\n",
    "              repository {\n",
    "                id\n",
    "                nameWithOwner\n",
    "                stargazerCount\n",
    "                description\n",
    "                codeOfConduct {\n",
    "                  body\n",
    "                  id\n",
    "                  name\n",
    "                  url\n",
    "                }\n",
    "                homepageUrl\n",
    "                assignableUsers {\n",
    "                  totalCount\n",
    "                }\n",
    "                mentionableUsers {\n",
    "                  totalCount\n",
    "                }\n",
    "                forkCount\n",
    "                watchers {\n",
    "                  totalCount\n",
    "                }\n",
    "                isFork\n",
    "                languages(first: 20) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      name\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              author {\n",
    "                ... on User {\n",
    "                  login\n",
    "                  url\n",
    "                  createdAt\n",
    "                  repositories {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  followers {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  following {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  repositoryDiscussions {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  repositoryDiscussionComments {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  organizations (first: 20){\n",
    "                    edges {\n",
    "                      node {\n",
    "                        name\n",
    "                        login\n",
    "                        url\n",
    "                        membersWithRole {\n",
    "                          totalCount\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              labels(first: 10) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    name\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              body\n",
    "              bodyHTML\n",
    "              bodyText\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        pageInfo {\n",
    "          endCursor\n",
    "          hasNextPage\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c2427-7797-4088-a144-6045c87ee349",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.headers = headers\n",
    "# Check rate limit before executing the main query\n",
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "rate_limit_response = client.execute(rate_limit_query)\n",
    "log_activity(f\"Rate limit: {rate_limit_response['rateLimit']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acb9fd-7a25-458d-aec7-8f302a653c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def execute_query(keyword, first=100, after_cursor=None):\n",
    "    global current_token\n",
    "    log_activity(\n",
    "        f\"Executing query with keyword: {keyword}, first: {first}, afterCursor: {after_cursor}\"\n",
    "    )\n",
    "    while True:\n",
    "        try:\n",
    "            # Randomize User-Agent for each query\n",
    "            headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "            transport.headers = headers\n",
    "            # Check rate limit before executing the main query\n",
    "            rate_limit_response = client.execute(rate_limit_query)\n",
    "            remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "            if remaining < 100:\n",
    "                log_activity(\n",
    "                    f\"Rate limit remaining ({remaining}) is below threshold. Switching token...\"\n",
    "                )\n",
    "                # Set up to track whether we have cycled through all tokens\n",
    "                all_tokens_checked = False\n",
    "                initial_token = current_token\n",
    "\n",
    "                while not all_tokens_checked:\n",
    "                    # Switch to the next token\n",
    "                    current_token = next(token_iterator)\n",
    "                    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "                    transport.headers = headers\n",
    "\n",
    "                    # Check the rate limit of the new token\n",
    "                    rate_limit_response = client.execute(rate_limit_query)\n",
    "                    remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "\n",
    "                    if remaining >= 100:\n",
    "                        log_activity(\n",
    "                            f\"Switched to a new token with sufficient rate limit ({remaining} remaining).\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "                    # Check if we have cycled through all tokens\n",
    "                    if current_token == initial_token:\n",
    "                        log_activity(\"All tokens are below threshold. Waiting for 1 hour...\")\n",
    "                        time.sleep(3600)\n",
    "                        all_tokens_checked = True\n",
    "\n",
    "                continue\n",
    "            return client.execute(\n",
    "                query_template,\n",
    "                variable_values={\n",
    "                    \"keyword\": keyword,\n",
    "                    \"first\": first,\n",
    "                    \"afterCursor\": after_cursor,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"API rate limit\" in str(e):\n",
    "                log_activity(\n",
    "                    f\"Rate limit reached: {e}, switching token... (Attempt with first {first})\"\n",
    "                )\n",
    "                current_token = next(token_iterator)\n",
    "                headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "            else:\n",
    "                if first > 1:\n",
    "                    first = max(1, first // 2)\n",
    "                    log_activity(\n",
    "                    f\"Error: {e}, reducing number of results and retrying... (Attempt with first {first})\"\n",
    "                    )\n",
    "                else:\n",
    "                    log_activity(f\"Query failed completely after retries: {e}\")\n",
    "                    break\n",
    "    log_activity(\"Max retries reached. Sleeping for 30 minutes and switching token...\")\n",
    "    time.sleep(1800)\n",
    "    current_token = next(token_iterator)\n",
    "    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "    transport.headers = headers\n",
    "    return execute_query(keyword, first, after_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2f9a0-95df-4b00-8544-efeb1ba4d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if os.path.exists(pkl_filename):\n",
    "    with open(pkl_filename, \"rb\") as f:\n",
    "        progress_data = pickle.load(f)\n",
    "        df = progress_data[\"df\"]\n",
    "        start_index = progress_data[\"start_index\"]\n",
    "else:\n",
    "    df = []\n",
    "    start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = []\n",
    "# start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9074db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "def execute_with_dynamic_date_range(\n",
    "    repo_name,\n",
    "    execute_query,\n",
    "    process_results,\n",
    "    start_date_arg,\n",
    "    se_fm_repository_data,\n",
    "    max_total_allowed_results=950,\n",
    "    default_days_interval=60,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes a GraphQL query within dynamically adjusted date ranges to handle large datasets.\n",
    "\n",
    "    :param keywords: List of keywords for search queries.\n",
    "    :param execute_query: Function to execute the query.\n",
    "    :param process_results: Function to process the query results.\n",
    "    :param start_date_arg: Start date in ISO format (\"%Y-%m-%dT%H:%M:%S\").\n",
    "    :param max_total_allowed_results: Max allowed results before reducing date range.\n",
    "    :param default_days_interval: Initial days interval for date range.\n",
    "    \"\"\"\n",
    "    start_date = datetime.datetime.strptime(start_date_arg, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    current_date = datetime.datetime.now()\n",
    "    end_date: datetime.datetime = current_date\n",
    "    days_interval = default_days_interval\n",
    "\n",
    "    while start_date < end_date:\n",
    "        next_date_candidate = start_date + datetime.timedelta(days=days_interval)\n",
    "        next_date = min(next_date_candidate, end_date)\n",
    "\n",
    "        try:\n",
    "            after_cursor = None\n",
    "            while True:\n",
    "                date_range = f\"{start_date.strftime('%Y-%m-%dT%H:%M')}..{next_date.strftime('%Y-%m-%dT%H:%M')}\"\n",
    "                search_keyword = f\"is:pr is:public archived:false created:{date_range} repo:{repo_name}\"\n",
    "                response = execute_query(\n",
    "                    search_keyword, first=10, after_cursor=after_cursor\n",
    "                )\n",
    "                log_activity(f'response count: {response[\"search\"][\"issueCount\"]}\\n')\n",
    "\n",
    "                if response[\"search\"][\"issueCount\"] == 0:\n",
    "                    days_interval = default_days_interval  # Reset interval\n",
    "                    break\n",
    "\n",
    "                # Adjust interval if issue count exceeds max allowed\n",
    "                if response[\"search\"][\"issueCount\"] > max_total_allowed_results:\n",
    "                    reduced_interval = (\n",
    "                        max(1, days_interval // 2)\n",
    "                        if days_interval > 1\n",
    "                        else max(0.00069, days_interval / 2)\n",
    "                    )\n",
    "                    log_activity(f\"Reducing interval to {reduced_interval} days...\")\n",
    "                    days_interval = reduced_interval\n",
    "                    next_date = start_date + datetime.timedelta(days=days_interval)\n",
    "                    continue\n",
    "\n",
    "                # Process results\n",
    "                process_results(\n",
    "                    response,\n",
    "                )\n",
    "\n",
    "                # Pagination\n",
    "                page_info = response[\"search\"][\"pageInfo\"]\n",
    "                if page_info[\"hasNextPage\"]:\n",
    "                    after_cursor = page_info[\"endCursor\"]\n",
    "                else:\n",
    "                    break\n",
    "            with open(pkl_filename, \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    {\"df\": se_fm_repository_data, \"start_index\": start_index + 1}, f\n",
    "                )\n",
    "\n",
    "            # Reset interval to default after a successful run\n",
    "            days_interval = default_days_interval\n",
    "        except Exception as e:\n",
    "            log_activity(\n",
    "                f\"Error fetching data for '{repo_name}' in range {date_range}: {e}\"\n",
    "            )\n",
    "            # Save progress before terminating\n",
    "            with open(pkl_filename, \"wb\") as f:\n",
    "                pickle.dump({\"df\": df, \"start_index\": start_index}, f)\n",
    "            raise\n",
    "\n",
    "        start_date = next_date  # Move to the next date interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def result_processor(\n",
    "    response,\n",
    "):\n",
    "    for edge in response[\"search\"][\"edges\"]:\n",
    "        pull_request = edge[\"node\"]\n",
    "\n",
    "        if not pull_request:\n",
    "            continue\n",
    "\n",
    "        author = pull_request[\"author\"]\n",
    "\n",
    "        author_organizations = (\n",
    "            [\n",
    "                organization[\"node\"]\n",
    "                for organization in author[\"organizations\"][\"edges\"]\n",
    "                if organization[\"node\"]\n",
    "            ]\n",
    "            if author\n",
    "            and author.get(\"organizations\")\n",
    "            and author[\"organizations\"].get(\"edges\")\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        timestamp_suffix = f\"_as_at_{datetime.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "        df.append(\n",
    "            {\n",
    "                \"id\": pull_request[\"id\"],\n",
    "                \"title\": pull_request[\"title\"],\n",
    "                \"url\": pull_request[\"url\"],\n",
    "                \"state\": pull_request[\"state\"],\n",
    "                \"comments_count\": pull_request[\"comments\"][\"totalCount\"],\n",
    "                \"deletions\": pull_request[\"deletions\"],\n",
    "                \"closed\": pull_request[\"closed\"],\n",
    "                \"closed_at\": pull_request[\"closedAt\"],\n",
    "                \"merged\": pull_request[\"merged\"],\n",
    "                \"body\": pull_request[\"body\"],\n",
    "                \"bodyHTML\": pull_request[\"bodyHTML\"],\n",
    "                \"bodyText\": pull_request[\"bodyText\"],\n",
    "                \"created_at\": pull_request[\"createdAt\"],\n",
    "                \"updated_at\": pull_request[\"updatedAt\"],\n",
    "                \"repository\": pull_request[\"repository\"],\n",
    "                \"repository_name_with_owner\": pull_request[\"repository\"][\n",
    "                    \"nameWithOwner\"\n",
    "                ],\n",
    "                \"repository_stargazer_count\": pull_request[\"repository\"][\n",
    "                    \"stargazerCount\"\n",
    "                ],\n",
    "                \"repository_watcher_count\": pull_request[\"repository\"][\"watchers\"][\n",
    "                    \"totalCount\"\n",
    "                ],\n",
    "                \"repository_is_fork\": pull_request[\"repository\"][\"isFork\"],\n",
    "                \"repository_languages\": [\n",
    "                    language[\"node\"][\"name\"]\n",
    "                    for language in pull_request[\"repository\"][\"languages\"][\"edges\"]\n",
    "                ],\n",
    "                \"merge_commit\": (\n",
    "                    pull_request[\"mergeCommit\"][\"oid\"]\n",
    "                    if pull_request[\"mergeCommit\"]\n",
    "                    else None\n",
    "                ),\n",
    "                \"labels\": [\n",
    "                    label[\"node\"][\"name\"] for label in pull_request[\"labels\"][\"edges\"]\n",
    "                ],\n",
    "                \"commits_count\": pull_request[\"commits\"][\"totalCount\"],\n",
    "                \"changed_files_count\": pull_request[\"changedFiles\"],\n",
    "                \"author_name\": (author[\"login\"] if author else None),\n",
    "                \"author_url\": (author[\"url\"] if author else None),\n",
    "                \"author_account_created_at\": (author[\"createdAt\"] if author else None),\n",
    "                f\"author_repository_count{timestamp_suffix}\": (\n",
    "                    author[\"repositories\"][\"totalCount\"]\n",
    "                    if author and author[\"repositories\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_followers_count{timestamp_suffix}\": (\n",
    "                    author[\"followers\"][\"totalCount\"]\n",
    "                    if author and author[\"followers\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_following_count{timestamp_suffix}\": (\n",
    "                    author[\"following\"][\"totalCount\"]\n",
    "                    if author and author[\"following\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_repository_discussions_count{timestamp_suffix}\": (\n",
    "                    author[\"repositoryDiscussions\"][\"totalCount\"]\n",
    "                    if author and author[\"repositoryDiscussions\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_repository_discussion_comments_count{timestamp_suffix}\": (\n",
    "                    author[\"repositoryDiscussionComments\"][\"totalCount\"]\n",
    "                    if author and author[\"repositoryDiscussionComments\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_organizations{timestamp_suffix}\": author_organizations,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "execute_with_dynamic_date_range(\n",
    "    repo_name=repository_name,\n",
    "    execute_query=execute_query,\n",
    "    process_results=result_processor,\n",
    "    start_date_arg=repository_created_at,\n",
    "     se_fm_repository_data=df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the .pkl file\n",
    "with open(pkl_filename, \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Check if it's a list of dictionaries\n",
    "if isinstance(data[\"df\"], list) and all(isinstance(d, dict) for d in data[\"df\"]):\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data[\"df\"])\n",
    "    \n",
    "    # Remove duplicates by 'id'\n",
    "    df = df.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "\n",
    "    # Convert back to a list of dictionaries\n",
    "    cleaned_data = {**data, \"df\": df.to_dict(orient=\"records\")}\n",
    "\n",
    "# Check if it's already a DataFrame\n",
    "elif isinstance(data[\"df\"], pd.DataFrame):\n",
    "    # Remove duplicates by 'id'\n",
    "    cleaned_df = data[\"df\"].drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "    cleaned_data = {**data, \"df\": cleaned_df}\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(pkl_filename, \"wb\") as file:\n",
    "    pickle.dump(cleaned_data, file)\n",
    "\n",
    "log_activity(\"Duplicates removed and data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bca9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_pkl_content_as_csv_and_json(filepath):\n",
    "    \"\"\"\n",
    "    This function reads a pickle file from the given filepath\n",
    "    and saves the data contained in the \"df\" key to both a CSV file and a JSON file.\n",
    "    The CSV and json files are saved.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the pickle file to be read.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error reading the pickle file or writing the CSV/JSON files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        filename = filepath.replace('.pkl', '')\n",
    "        pd.DataFrame(data[\"df\"]).to_csv(f\"{filename}.csv\", index=True)\n",
    "        log_activity(f\"Data written to {filename}.csv successfully.\")\n",
    "    except Exception as e:\n",
    "        log_activity(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = pkl_filename\n",
    "save_pkl_content_as_csv_and_json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0deeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate metadata\n",
    "df = []\n",
    "start_index = 0\n",
    "\n",
    "\n",
    "def generate_metadata(filepath):\n",
    "    \"\"\" \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        filename = filepath.replace(\".pkl\", \"\") + \".meta\"\n",
    "        pull_requests = data[\"df\"]\n",
    "        unique_pr_author = {}\n",
    "        merged_pr_count = 0\n",
    "        closed_pr_count = 0\n",
    "        for pull_request in pull_requests:\n",
    "\n",
    "            def update_unique_value_dict(info_dict, key, value):\n",
    "                if not value:\n",
    "                    log_activity(\n",
    "                        f\"Warning: Pull request missing '{key}' {pull_request}\"\n",
    "                    )\n",
    "                    return False\n",
    "                if value not in info_dict:\n",
    "                    info_dict[value] = value\n",
    "                return True\n",
    "\n",
    "            # Update author count\n",
    "            update_unique_value_dict(\n",
    "                unique_pr_author, \"author_name\", pull_request[\"author_name\"]\n",
    "            )\n",
    "\n",
    "            merged_pr_count += 1 if pull_request[\"merged\"] else 0\n",
    "            closed_pr_count += 1 if pull_request[\"closed\"] is not None else 0\n",
    "\n",
    "        total_prs = len(pull_requests)\n",
    "        unique_pr_author_count = len(unique_pr_author)\n",
    "\n",
    "        df.append(\n",
    "            {\n",
    "                \"total_prs\": total_prs,\n",
    "                \"unique_pr_author_count\": unique_pr_author_count,\n",
    "                \"unique_pr_author_ratio\": (\n",
    "                    round(unique_pr_author_count / total_prs, 3) if total_prs > 0 else 0\n",
    "                ),\n",
    "                \"merged_pr_count\": merged_pr_count,\n",
    "                \"merged_pr_ratio\": (\n",
    "                    round(merged_pr_count / total_prs, 3) if total_prs > 0 else 0\n",
    "                ),\n",
    "                \"closed_pr_count\": closed_pr_count,\n",
    "                \"closed_pr_ratio\": (\n",
    "                    round(closed_pr_count / total_prs, 3) if total_prs > 0 else 0\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        pd.DataFrame(df).to_csv(f\"{filename}.csv\", index=True)\n",
    "        log_activity(f\"Data written to {filename}.csv successfully.\")\n",
    "        try:\n",
    "            with open(f\"{filename}.json\", \"w\") as f:\n",
    "                json.dump(df, f, indent=4)\n",
    "            log_activity(f\"Data written to {filename}.json successfully.\")\n",
    "        except Exception as e:\n",
    "            log_activity(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        log_activity(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = pkl_filename\n",
    "generate_metadata(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469812ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
