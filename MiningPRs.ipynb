{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babdda8-11f7-485e-bad8-d8c5b8eebb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Read tokens from a text file\n",
    "tokens_file = \"./env/tokens.txt\"\n",
    "with open(tokens_file, \"r\") as file:\n",
    "    tokens = file.read().splitlines()\n",
    "\n",
    "# Create an iterator to cycle through the tokens\n",
    "token_iterator = itertools.cycle(tokens)\n",
    "current_token = next(token_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c281d1-993b-497b-8984-0e32ac6a61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of User-Agents for randomization\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "]\n",
    "\n",
    "# Define headers to authenticate using the first token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {current_token}\",\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "}\n",
    "\n",
    "# Setup GraphQL endpoint and client\n",
    "graphql_url = \"https://api.github.com/graphql\"\n",
    "transport = RequestsHTTPTransport(url=graphql_url, headers=headers, use_json=True)\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_activity(activity: str):\n",
    "    log = f\"{datetime.datetime.now()}: {activity}\\n\"\n",
    "    # print(log)\n",
    "    with open(\"spam-prs-output.log\", \"a\") as log_file:\n",
    "        log_file.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062f1b0-a9df-4f94-bb18-56f42774bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all tokens to verify their validity\n",
    "def test_all_tokens():\n",
    "    test_query = gql(\n",
    "        \"\"\"\n",
    "        {\n",
    "          viewer {\n",
    "            login\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    for i, token in enumerate(tokens):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"User-Agent\": random.choice(user_agents),\n",
    "        }\n",
    "        transport = RequestsHTTPTransport(\n",
    "            url=graphql_url, headers=headers, use_json=True\n",
    "        )\n",
    "        client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "        try:\n",
    "            response = client.execute(test_query)\n",
    "            log_activity(\n",
    "                f\"Token {i+1}/{len(tokens)} is valid. Logged in as: {response['viewer']['login']}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log_activity(f\"Token {i+1}/{len(tokens)} failed with error: {e}\")\n",
    "\n",
    "\n",
    "# Run the token validation\n",
    "test_all_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd85df0-71b7-46c8-9ed4-eceb724201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphQL query\n",
    "query_template = gql(\n",
    "    \"\"\"\n",
    "    query searchIssues($keyword: String!, $afterCursor: String, $first: Int) {\n",
    "      search(query: $keyword, type: ISSUE, first: $first, after: $afterCursor) {\n",
    "        issueCount\n",
    "        edges {\n",
    "          cursor\n",
    "          node {\n",
    "            ... on PullRequest {\n",
    "              id\n",
    "              number\n",
    "              title\n",
    "              url\n",
    "              comments(first: 100) {\n",
    "                totalCount # it still gives the total count regardless of the first parameter\n",
    "                edges {\n",
    "                  node {   \n",
    "                    author { ... on User { login } }\n",
    "                    editor { ... on User { login } }\n",
    "                    body\n",
    "                    createdAt\n",
    "                    lastEditedAt\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              state\n",
    "              closed\n",
    "              merged\n",
    "              createdAt\n",
    "              updatedAt\n",
    "              mergeCommit {\n",
    "                oid\n",
    "              }\n",
    "              timeline(last: 100) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    __typename\n",
    "                    ... on LabeledEvent {\n",
    "                      actor { ... on User { login } }\n",
    "                      label { ... on Label { name }}\n",
    "                      createdAt\n",
    "                    }\n",
    "                    ... on ClosedEvent { \n",
    "                      actor { ... on User { login } }\n",
    "                      createdAt\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              commits {\n",
    "                totalCount\n",
    "              }\n",
    "              changedFiles\n",
    "              headRefName\n",
    "              baseRefName\n",
    "              repository {\n",
    "                id\n",
    "                nameWithOwner\n",
    "                stargazerCount\n",
    "                description\n",
    "                codeOfConduct {\n",
    "                  body\n",
    "                  id\n",
    "                  name\n",
    "                  url\n",
    "                }\n",
    "                homepageUrl\n",
    "                assignableUsers(first: 100) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      login\n",
    "                      url\n",
    "                      bio\n",
    "                      company\n",
    "                    }\n",
    "                  }\n",
    "                  totalCount\n",
    "                }\n",
    "                mentionableUsers(first: 100) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      login\n",
    "                      url\n",
    "                      bio\n",
    "                      company\n",
    "                    }\n",
    "                  }\n",
    "                  totalCount\n",
    "                }\n",
    "                forkCount\n",
    "                watchers {\n",
    "                  totalCount\n",
    "                }\n",
    "                isFork\n",
    "                languages(first: 20) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      name\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              author {\n",
    "                ... on User {\n",
    "                  login\n",
    "                  url\n",
    "                  createdAt\n",
    "                  repositories {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  followers {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  following {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  repositoryDiscussions {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  repositoryDiscussionComments {\n",
    "                    totalCount\n",
    "                  }\n",
    "                  organizations (first: 20){\n",
    "                    edges {\n",
    "                      node {\n",
    "                        name\n",
    "                        login\n",
    "                        url\n",
    "                        membersWithRole {\n",
    "                          totalCount\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              labels(first: 10) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    name\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              body\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        pageInfo {\n",
    "          endCursor\n",
    "          hasNextPage\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b32e41-7d3f-458b-ad0f-0c13d3146813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_contributor_count(repo_owner, repo_name):\n",
    "#     global current_token\n",
    "#     max_retries = 3\n",
    "#     retries = 0\n",
    "#     while retries < max_retries:\n",
    "#         try:\n",
    "#             # Randomize User-Agent for each query\n",
    "#             headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "#             headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "#             url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contributors?per_page=1&anon=true\"\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             if response.status_code == 200:\n",
    "#                 return int(response.headers.get(\"Link\", \"\").split(\",\")[-1].split(\"&page=\")[-1].split(\">\")[0]) if \"Link\" in response.headers else len(response.json())\n",
    "#             elif response.status_code == 403:\n",
    "#                 print(f\"Rate limit exceeded, switching token... (Attempt {retries + 1}/{max_retries})\")\n",
    "#                 current_token = next(token_iterator)\n",
    "#                 retries += 1\n",
    "#             else:\n",
    "#                 response.raise_for_status()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error: {e}, retrying... (Attempt {retries + 1}/{max_retries})\")\n",
    "#             retries += 1\n",
    "#     raise Exception(\"Max retries reached. Unable to complete the request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c2427-7797-4088-a144-6045c87ee349",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.headers = headers\n",
    "# Check rate limit before executing the main query\n",
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "rate_limit_response = client.execute(rate_limit_query)\n",
    "log_activity(f\"Rate limit: {rate_limit_response['rateLimit']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acb9fd-7a25-458d-aec7-8f302a653c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def execute_query(keyword, first=100, after_cursor=None):\n",
    "    global current_token\n",
    "    log_activity(\n",
    "        f\"Executing query with keyword: {keyword}, first: {first}, afterCursor: {after_cursor}\"\n",
    "    )\n",
    "    while True:\n",
    "        try:\n",
    "            # Randomize User-Agent for each query\n",
    "            headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "            transport.headers = headers\n",
    "            # Check rate limit before executing the main query\n",
    "            rate_limit_response = client.execute(rate_limit_query)\n",
    "            remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "            if remaining < 100:\n",
    "                log_activity(\n",
    "                    f\"Rate limit remaining ({remaining}) is below threshold. Switching token...\"\n",
    "                )\n",
    "                # Set up to track whether we have cycled through all tokens\n",
    "                all_tokens_checked = False\n",
    "                initial_token = current_token\n",
    "\n",
    "                while not all_tokens_checked:\n",
    "                    # Switch to the next token\n",
    "                    current_token = next(token_iterator)\n",
    "                    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "                    transport.headers = headers\n",
    "\n",
    "                    # Check the rate limit of the new token\n",
    "                    rate_limit_response = client.execute(rate_limit_query)\n",
    "                    remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "\n",
    "                    if remaining >= 100:\n",
    "                        log_activity(\n",
    "                            f\"Switched to a new token with sufficient rate limit ({remaining} remaining).\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "                    # Check if we have cycled through all tokens\n",
    "                    if current_token == initial_token:\n",
    "                        log_activity(\"All tokens are below threshold. Waiting for 1 hour...\")\n",
    "                        time.sleep(3600)\n",
    "                        all_tokens_checked = True\n",
    "\n",
    "                continue\n",
    "            return client.execute(\n",
    "                query_template,\n",
    "                variable_values={\n",
    "                    \"keyword\": keyword,\n",
    "                    \"first\": first,\n",
    "                    \"afterCursor\": after_cursor,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"API rate limit\" in str(e):\n",
    "                log_activity(\n",
    "                    f\"Rate limit reached: {e}, switching token... (Attempt with first {first})\"\n",
    "                )\n",
    "                current_token = next(token_iterator)\n",
    "                headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "            else:\n",
    "                if first > 1:\n",
    "                    first = max(1, first // 2)\n",
    "                    log_activity(\n",
    "                    f\"Error: {e}, reducing number of results and retrying... (Attempt with first {first})\"\n",
    "                    )\n",
    "                else:\n",
    "                    log_activity(f\"Query failed completely after retries: {e}\")\n",
    "                    break\n",
    "    log_activity(\"Max retries reached. Sleeping for 30 minutes and switching token...\")\n",
    "    time.sleep(1800)\n",
    "    current_token = next(token_iterator)\n",
    "    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "    transport.headers = headers\n",
    "    return execute_query(keyword, first, after_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2f9a0-95df-4b00-8544-efeb1ba4d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if os.path.exists(\"progress.pkl\"):\n",
    "    with open(\"progress.pkl\", \"rb\") as f:\n",
    "        progress_data = pickle.load(f)\n",
    "        df = progress_data[\"df\"]\n",
    "        start_index = progress_data[\"start_index\"]\n",
    "else:\n",
    "    df = []\n",
    "    start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = []\n",
    "# start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786db526",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "start_date_param = \"2010-1-1T00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9074db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def execute_with_dynamic_date_range(\n",
    "    keywords,\n",
    "    execute_query,\n",
    "    process_results,\n",
    "    start_date_param,\n",
    "    max_total_allowed_results=250,\n",
    "    default_days_interval=60,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes a GraphQL query within dynamically adjusted date ranges to handle large datasets.\n",
    "\n",
    "    :param keywords: List of keywords for search queries.\n",
    "    :param execute_query: Function to execute the query.\n",
    "    :param process_results: Function to process the query results.\n",
    "    :param start_date_param: Start date in ISO format (\"%Y-%m-%dT%H:%M\").\n",
    "    :param max_total_allowed_results: Max allowed results before reducing date range.\n",
    "    :param default_days_interval: Initial days interval for date range.\n",
    "    \"\"\"\n",
    "    start_date = datetime.datetime.strptime(start_date_param, \"%Y-%m-%dT%H:%M\")\n",
    "    current_date = datetime.datetime(2023, 11, 1)\n",
    "    end_date = current_date\n",
    "    days_interval = default_days_interval\n",
    "\n",
    "    while start_date < end_date:\n",
    "        next_date_candidate = start_date + datetime.timedelta(days=days_interval)\n",
    "        next_date = min(next_date_candidate, end_date)\n",
    "\n",
    "        for keyword in tqdm(keywords):\n",
    "            try:\n",
    "                after_cursor = None\n",
    "                while True:\n",
    "                    date_range = f\"{start_date.strftime('%Y-%m-%dT%H:%M')}..{next_date.strftime('%Y-%m-%dT%H:%M')}\"\n",
    "                    search_keyword = f\"label:{keyword} is:pr is:public archived:false created:{date_range}\"\n",
    "                    response = execute_query(\n",
    "                        search_keyword, first=10, after_cursor=after_cursor\n",
    "                    )\n",
    "                    log_activity(\n",
    "                        f'response count: {response[\"search\"][\"issueCount\"]}\\n'\n",
    "                    )\n",
    "\n",
    "                    if response[\"search\"][\"issueCount\"] == 0:\n",
    "                        days_interval = default_days_interval  # Reset interval\n",
    "                        break\n",
    "\n",
    "                    # Adjust interval if issue count exceeds max allowed\n",
    "                    if response[\"search\"][\"issueCount\"] > max_total_allowed_results:\n",
    "                        reduced_interval = (\n",
    "                            max(1, days_interval // 2)\n",
    "                            if days_interval > 1\n",
    "                            else max(0.00069, days_interval / 2)\n",
    "                        )\n",
    "                        log_activity(f\"Reducing interval to {reduced_interval} days...\")\n",
    "                        days_interval = reduced_interval\n",
    "                        next_date = start_date + datetime.timedelta(days=days_interval)\n",
    "                        continue\n",
    "\n",
    "                    # Process results\n",
    "                    process_results(response, keyword)\n",
    "\n",
    "                    # Pagination\n",
    "                    page_info = response[\"search\"][\"pageInfo\"]\n",
    "                    if page_info[\"hasNextPage\"]:\n",
    "                        after_cursor = page_info[\"endCursor\"]\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # Reset interval to default after a successful run\n",
    "                days_interval = default_days_interval\n",
    "            except Exception as e:\n",
    "                log_activity(\n",
    "                    f\"Error fetching data for '{keyword}' in range {date_range}: {e}\"\n",
    "                )\n",
    "                raise\n",
    "\n",
    "        start_date = next_date  # Move to the next date interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_processor(response, keyword):\n",
    "    for edge in response[\"search\"][\"edges\"]:\n",
    "        pull_request = edge[\"node\"]\n",
    "\n",
    "        if not pull_request:\n",
    "            continue\n",
    "        timeline = pull_request[\"timeline\"][\"edges\"]\n",
    "        labeled_spam_event = next(\n",
    "            filter(\n",
    "                lambda x: x[\"node\"]\n",
    "                and x[\"node\"][\"__typename\"] == \"LabeledEvent\"\n",
    "                and x[\"node\"][\"label\"][\"name\"]\n",
    "                and (x[\"node\"][\"label\"][\"name\"]).lower() == keyword.strip('\"').lower(),\n",
    "                timeline,\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        labeled_spam_event_node = (\n",
    "            labeled_spam_event[\"node\"] if labeled_spam_event else None\n",
    "        )\n",
    "        closed_event = next(\n",
    "            filter(\n",
    "                lambda x: x[\"node\"] and x[\"node\"][\"__typename\"] == \"ClosedEvent\",\n",
    "                timeline,\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        closed_event_node = closed_event[\"node\"] if closed_event else None\n",
    "        author = pull_request[\"author\"]\n",
    "        comments = [comment[\"node\"] for comment in pull_request[\"comments\"][\"edges\"]]\n",
    "        labeled_spam_at = (\n",
    "            labeled_spam_event_node[\"createdAt\"] if labeled_spam_event_node else None\n",
    "        )\n",
    "\n",
    "        labeled_spam_by = (\n",
    "            labeled_spam_event_node[\"actor\"][\"login\"]\n",
    "            if labeled_spam_event_node and labeled_spam_event_node[\"actor\"]\n",
    "            else None\n",
    "        )\n",
    "        comments_by_spam_labeler = [\n",
    "            {\n",
    "                **labeler_comment,\n",
    "                \"commented_before_labeling_spam\": (\n",
    "                    labeler_comment[\"createdAt\"] < labeled_spam_at\n",
    "                    if (labeler_comment[\"createdAt\"] and labeled_spam_at) is not None\n",
    "                    else None\n",
    "                ),\n",
    "            }\n",
    "            for labeler_comment in comments\n",
    "            if (\n",
    "                (\n",
    "                    labeler_comment[\"author\"]\n",
    "                    and labeler_comment[\"author\"][\"login\"] == labeled_spam_by\n",
    "                )\n",
    "                or (not labeler_comment[\"author\"] and not labeled_spam_by)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        closed_by = (\n",
    "            closed_event_node[\"actor\"][\"login\"]\n",
    "            if closed_event_node and closed_event_node[\"actor\"]\n",
    "            else None\n",
    "        )\n",
    "        closed_at = closed_event_node[\"createdAt\"] if closed_event_node else None\n",
    "        comments_by_closer = [\n",
    "            {\n",
    "                **closer_comment,\n",
    "                \"commented_before_closing\": (\n",
    "                    closer_comment[\"createdAt\"] < closed_at if closed_at else False\n",
    "                ),\n",
    "            }\n",
    "            for closer_comment in comments\n",
    "            if (\n",
    "                (\n",
    "                    closer_comment[\"author\"]\n",
    "                    and closer_comment[\"author\"][\"login\"] == closed_by\n",
    "                )\n",
    "                or (not closer_comment[\"author\"] and not closed_by)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        author_organizations = (\n",
    "            [\n",
    "                organization[\"node\"]\n",
    "                for organization in author[\"organizations\"][\"edges\"]\n",
    "                if organization[\"node\"]\n",
    "            ]\n",
    "            if author\n",
    "            and author.get(\"organizations\")\n",
    "            and author[\"organizations\"].get(\"edges\")\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        timestamp_suffix = f\"_as_at_{datetime.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "        df.append(\n",
    "            {\n",
    "                \"id\": pull_request[\"id\"],\n",
    "                \"title\": pull_request[\"title\"],\n",
    "                \"url\": pull_request[\"url\"],\n",
    "                \"state\": pull_request[\"state\"],\n",
    "                \"comments_count\": pull_request[\"comments\"][\"totalCount\"],\n",
    "                \"comments_by_spam_labeler_count\": len(comments_by_spam_labeler),\n",
    "                \"comments_by_spam_labeler\": comments_by_spam_labeler,\n",
    "                \"labeled_spam_by\": (\n",
    "                    labeled_spam_event_node[\"actor\"][\"login\"]\n",
    "                    if labeled_spam_event_node and labeled_spam_event_node[\"actor\"]\n",
    "                    else None\n",
    "                ),\n",
    "                \"is_labeled_spam_by_bot\": labeled_spam_by is None,\n",
    "                \"labeled_spam_at\": labeled_spam_at,\n",
    "                \"comments_by_closer_count\": len(comments_by_closer),\n",
    "                \"comments_by_closer\": comments_by_closer,\n",
    "                \"closed\": pull_request[\"closed\"],\n",
    "                \"is_closed_by_bot\": closed_by is None,\n",
    "                \"closed_by\": closed_by,\n",
    "                \"closed_at\": closed_at,\n",
    "                \"merged\": pull_request[\"merged\"],\n",
    "                \"body\": pull_request[\"body\"],\n",
    "                \"created_at\": pull_request[\"createdAt\"],\n",
    "                \"updated_at\": pull_request[\"updatedAt\"],\n",
    "                \"repository\": pull_request[\"repository\"],\n",
    "                \"repository_name_with_owner\": pull_request[\"repository\"][\n",
    "                    \"nameWithOwner\"\n",
    "                ],\n",
    "                \"repository_stargazer_count\": pull_request[\"repository\"][\n",
    "                    \"stargazerCount\"\n",
    "                ],\n",
    "                \"repository_watcher_count\": pull_request[\"repository\"][\"watchers\"][\n",
    "                    \"totalCount\"\n",
    "                ],\n",
    "                \"repository_is_fork\": pull_request[\"repository\"][\"isFork\"],\n",
    "                \"repository_languages\": [\n",
    "                    language[\"node\"][\"name\"]\n",
    "                    for language in pull_request[\"repository\"][\"languages\"][\"edges\"]\n",
    "                ],\n",
    "                \"merge_commit\": (\n",
    "                    pull_request[\"mergeCommit\"][\"oid\"]\n",
    "                    if pull_request[\"mergeCommit\"]\n",
    "                    else None\n",
    "                ),\n",
    "                \"labels\": [\n",
    "                    label[\"node\"][\"name\"] for label in pull_request[\"labels\"][\"edges\"]\n",
    "                ],\n",
    "                \"commits_count\": pull_request[\"commits\"][\"totalCount\"],\n",
    "                \"changed_files_count\": pull_request[\"changedFiles\"],\n",
    "                \"author_name\": (author[\"login\"] if author else None),\n",
    "                \"author_url\": (author[\"url\"] if author else None),\n",
    "                \"author_account_created_at\": (author[\"createdAt\"] if author else None),\n",
    "                f\"author_repository_count{timestamp_suffix}\": (\n",
    "                    author[\"repositories\"][\"totalCount\"]\n",
    "                    if author and author[\"repositories\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_followers_count{timestamp_suffix}\": (\n",
    "                    author[\"followers\"][\"totalCount\"]\n",
    "                    if author and author[\"followers\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_following_count{timestamp_suffix}\": (\n",
    "                    author[\"following\"][\"totalCount\"]\n",
    "                    if author and author[\"following\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_repository_discussions_count{timestamp_suffix}\": (\n",
    "                    author[\"repositoryDiscussions\"][\"totalCount\"]\n",
    "                    if author and author[\"repositoryDiscussions\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_repository_discussion_comments_count{timestamp_suffix}\": (\n",
    "                    author[\"repositoryDiscussionComments\"][\"totalCount\"]\n",
    "                    if author and author[\"repositoryDiscussionComments\"]\n",
    "                    else None\n",
    "                ),\n",
    "                f\"author_organizations{timestamp_suffix}\": author_organizations,\n",
    "                \"spam_search_keyword\": keyword,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "execute_with_dynamic_date_range(\n",
    "    keywords=[\"ai spam\", \"spam\", \"ai-spam\"],\n",
    "    execute_query=execute_query,\n",
    "    process_results=result_processor,\n",
    "    start_date_param=start_date_param,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb89147-ab2c-4654-a8d7-637737c90537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import close\n",
    "\n",
    "# keywords = ['\"ai spam\"', \"spam\", \"ai-spam\"]\n",
    "# index = start_index\n",
    "# se_fm_repository_data = df\n",
    "# current_date = datetime.datetime.now()\n",
    "# start_date = datetime.datetime.strptime(start_date_param, \"%Y-%m-%dT%H:%M\")\n",
    "# end_date = current_date\n",
    "# max_total_allowed_results = 950\n",
    "# default_days_interval = 60\n",
    "# days_interval = default_days_interval\n",
    "\n",
    "# while start_date < end_date:\n",
    "#     next_date_candidate = start_date + datetime.timedelta(days=days_interval)\n",
    "#     next_date = next_date_candidate if next_date_candidate < end_date else end_date\n",
    "#     keyword_search_start_date = start_date\n",
    "#     keyword_search_next_date = next_date\n",
    "\n",
    "#     for keyword in tqdm(keywords):\n",
    "#         try:\n",
    "#             after_cursor = None\n",
    "#             while True:\n",
    "#                 date_range = f\"{keyword_search_start_date.strftime('%Y-%m-%dT%H:%M')}..{keyword_search_next_date.strftime('%Y-%m-%dT%H:%M')}\"\n",
    "#                 search_keyword: str = (\n",
    "#                     f\"label:{keyword} is:pr is:public archived:false created:{date_range}\"\n",
    "#                 )\n",
    "#                 response = execute_query(\n",
    "#                     search_keyword, first=10, after_cursor=after_cursor\n",
    "#                 )\n",
    "#                 log_activity(\n",
    "#                     f'response[\"search\"][\"issueCount\"]: {response[\"search\"][\"issueCount\"]}\\n'\n",
    "#                 )\n",
    "#                 if response[\"search\"][\"issueCount\"] == 0:\n",
    "#                     days_interval = default_days_interval  # Reset interval to normal\n",
    "#                     break\n",
    "#                 # Adjust interval based on issue count\n",
    "#                 if response[\"search\"][\"issueCount\"] > max_total_allowed_results:\n",
    "#                     reduced_interval = days_interval\n",
    "#                     if days_interval <= 1:\n",
    "#                         reduced_interval = max(0.00069, days_interval / 2) # allow up to 1 min interval\n",
    "#                     else:    \n",
    "#                         reduced_interval = max(1, days_interval // 2)\n",
    "#                     log_activity(\n",
    "#                         f\"count exceeds: {max_total_allowed_results}, reducing interval to {reduced_interval} days...\\n\"\n",
    "#                     )\n",
    "#                     days_interval = reduced_interval  # Reduce interval\n",
    "#                     keyword_search_next_date = (\n",
    "#                         keyword_search_start_date\n",
    "#                         + datetime.timedelta(days=days_interval)\n",
    "#                     )\n",
    "#                     continue\n",
    "\n",
    "#                 # Extract pr\n",
    "#                 for edge in response[\"search\"][\"edges\"]:\n",
    "#                     pull_request = edge[\"node\"]\n",
    "\n",
    "#                     if not pull_request:\n",
    "#                         continue\n",
    "#                     timeline = pull_request[\"timeline\"][\"edges\"]\n",
    "#                     labeled_spam_event = next(\n",
    "#                         filter(\n",
    "#                             lambda x: x[\"node\"]\n",
    "#                             and x[\"node\"][\"__typename\"] == \"LabeledEvent\"\n",
    "#                             and x[\"node\"][\"label\"][\"name\"]\n",
    "#                             and (x[\"node\"][\"label\"][\"name\"]).lower()\n",
    "#                             == keyword.strip('\"').lower(),\n",
    "#                             timeline,\n",
    "#                         ),\n",
    "#                         None,\n",
    "#                     )\n",
    "\n",
    "#                     labeled_spam_event_node = (\n",
    "#                         labeled_spam_event[\"node\"] if labeled_spam_event else None\n",
    "#                     )\n",
    "#                     closed_event = next(\n",
    "#                         filter(\n",
    "#                             lambda x: x[\"node\"]\n",
    "#                             and x[\"node\"][\"__typename\"] == \"ClosedEvent\",\n",
    "#                             timeline,\n",
    "#                         ),\n",
    "#                         None,\n",
    "#                     )\n",
    "#                     closed_event_node = closed_event[\"node\"] if closed_event else None\n",
    "#                     author = pull_request[\"author\"]\n",
    "#                     comments = [\n",
    "#                         comment[\"node\"] for comment in pull_request[\"comments\"][\"edges\"]\n",
    "#                     ]\n",
    "#                     labeled_spam_at = (\n",
    "#                         labeled_spam_event_node[\"createdAt\"]\n",
    "#                         if labeled_spam_event_node\n",
    "#                         else None\n",
    "#                     )\n",
    "\n",
    "#                     labeled_spam_by = (\n",
    "#                         labeled_spam_event_node[\"actor\"][\"login\"]\n",
    "#                         if labeled_spam_event_node and labeled_spam_event_node[\"actor\"]\n",
    "#                         else None\n",
    "#                     )\n",
    "#                     comments_by_spam_labeler = [\n",
    "#                         {\n",
    "#                             **labeler_comment,\n",
    "#                             \"commented_before_labeling_spam\": (\n",
    "#                                 labeler_comment[\"createdAt\"] < labeled_spam_at\n",
    "#                                 if (labeler_comment[\"createdAt\"] and labeled_spam_at)\n",
    "#                                 is not None\n",
    "#                                 else None\n",
    "#                             ),\n",
    "#                         }\n",
    "#                         for labeler_comment in comments\n",
    "#                         if (\n",
    "#                             (\n",
    "#                                 labeler_comment[\"author\"]\n",
    "#                                 and labeler_comment[\"author\"][\"login\"]\n",
    "#                                 == labeled_spam_by\n",
    "#                             )\n",
    "#                             or (not labeler_comment[\"author\"] and not labeled_spam_by)\n",
    "#                         )\n",
    "#                     ]\n",
    "\n",
    "#                     closed_by = (\n",
    "#                         closed_event_node[\"actor\"][\"login\"]\n",
    "#                         if closed_event_node and closed_event_node[\"actor\"]\n",
    "#                         else None\n",
    "#                     )\n",
    "#                     closed_at = (\n",
    "#                         closed_event_node[\"createdAt\"] if closed_event_node else None\n",
    "#                     )\n",
    "#                     comments_by_closer = [\n",
    "#                         {\n",
    "#                             **closer_comment,\n",
    "#                             \"commented_before_closing\": (\n",
    "#                                 closer_comment[\"createdAt\"] < closed_at\n",
    "#                                 if closed_at\n",
    "#                                 else False\n",
    "#                             ),\n",
    "#                         }\n",
    "#                         for closer_comment in comments\n",
    "#                         if (\n",
    "#                             (\n",
    "#                                 closer_comment[\"author\"]\n",
    "#                                 and closer_comment[\"author\"][\"login\"] == closed_by\n",
    "#                             )\n",
    "#                             or (not closer_comment[\"author\"] and not closed_by)\n",
    "#                         )\n",
    "#                     ]\n",
    "\n",
    "#                     author_organizations = (\n",
    "#                         [\n",
    "#                             organization[\"node\"]\n",
    "#                             for organization in author[\"organizations\"][\"edges\"]\n",
    "#                             if organization[\"node\"]\n",
    "#                         ]\n",
    "#                         if author\n",
    "#                         and author.get(\"organizations\")\n",
    "#                         and author[\"organizations\"].get(\"edges\")\n",
    "#                         else []\n",
    "#                     )\n",
    "\n",
    "#                     timestamp_suffix = (\n",
    "#                         f\"_as_at_{datetime.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "#                     )\n",
    "\n",
    "#                     df.append(\n",
    "#                         {\n",
    "#                             \"id\": pull_request[\"id\"],\n",
    "#                             \"title\": pull_request[\"title\"],\n",
    "#                             \"url\": pull_request[\"url\"],\n",
    "#                             \"state\": pull_request[\"state\"],\n",
    "#                             \"comments_count\": pull_request[\"comments\"][\"totalCount\"],\n",
    "#                             \"comments_by_spam_labeler_count\": len(\n",
    "#                                 comments_by_spam_labeler\n",
    "#                             ),\n",
    "#                             \"comments_by_spam_labeler\": comments_by_spam_labeler,\n",
    "#                             \"labeled_spam_by\": (\n",
    "#                                 labeled_spam_event_node[\"actor\"][\"login\"]\n",
    "#                                 if labeled_spam_event_node\n",
    "#                                 and labeled_spam_event_node[\"actor\"]\n",
    "#                                 else None\n",
    "#                             ),\n",
    "#                             \"is_labeled_spam_by_bot\": labeled_spam_by is None,\n",
    "#                             \"labeled_spam_at\": labeled_spam_at,\n",
    "#                             \"comments_by_closer_count\": len(comments_by_closer),\n",
    "#                             \"comments_by_closer\": comments_by_closer,\n",
    "#                             \"closed\": pull_request[\"closed\"],\n",
    "#                             \"is_closed_by_bot\": closed_by is None,\n",
    "#                             \"closed_by\": closed_by,\n",
    "#                             \"closed_at\": closed_at,\n",
    "#                             \"merged\": pull_request[\"merged\"],\n",
    "#                             \"body\": pull_request[\"body\"],\n",
    "#                             \"created_at\": pull_request[\"createdAt\"],\n",
    "#                             \"updated_at\": pull_request[\"updatedAt\"],\n",
    "#                             \"repository\": pull_request[\"repository\"],\n",
    "#                             \"repository_name_with_owner\": pull_request[\"repository\"][\n",
    "#                                 \"nameWithOwner\"\n",
    "#                             ],\n",
    "#                             \"repository_stargazer_count\": pull_request[\"repository\"][\n",
    "#                                 \"stargazerCount\"\n",
    "#                             ],\n",
    "#                             \"repository_watcher_count\": pull_request[\"repository\"][\n",
    "#                                 \"watchers\"\n",
    "#                             ][\"totalCount\"],\n",
    "#                             \"repository_is_fork\": pull_request[\"repository\"][\"isFork\"],\n",
    "#                             \"repository_languages\": [\n",
    "#                                 language[\"node\"][\"name\"]\n",
    "#                                 for language in pull_request[\"repository\"][\"languages\"][\n",
    "#                                     \"edges\"\n",
    "#                                 ]\n",
    "#                             ],\n",
    "#                             \"merge_commit\": (\n",
    "#                                 pull_request[\"mergeCommit\"][\"oid\"]\n",
    "#                                 if pull_request[\"mergeCommit\"]\n",
    "#                                 else None\n",
    "#                             ),\n",
    "#                             \"labels\": [\n",
    "#                                 label[\"node\"][\"name\"]\n",
    "#                                 for label in pull_request[\"labels\"][\"edges\"]\n",
    "#                             ],\n",
    "#                             \"commits_count\": pull_request[\"commits\"][\"totalCount\"],\n",
    "#                             \"changed_files_count\": pull_request[\"changedFiles\"],\n",
    "#                             \"author_name\": (author[\"login\"] if author else None),\n",
    "#                             \"author_url\": (author[\"url\"] if author else None),\n",
    "#                             \"author_account_created_at\": (\n",
    "#                                 author[\"createdAt\"] if author else None\n",
    "#                             ),\n",
    "#                             f\"author_repository_count{timestamp_suffix}\": (\n",
    "#                                 author[\"repositories\"][\"totalCount\"]\n",
    "#                                 if author and author[\"repositories\"]\n",
    "#                                 else None\n",
    "#                             ),\n",
    "#                             f\"author_followers_count{timestamp_suffix}\": (\n",
    "#                                 author[\"followers\"][\"totalCount\"]\n",
    "#                                 if author and author[\"followers\"]\n",
    "#                                 else None\n",
    "#                             ),\n",
    "#                             f\"author_following_count{timestamp_suffix}\": (\n",
    "#                                 author[\"following\"][\"totalCount\"]\n",
    "#                                 if author and author[\"following\"]\n",
    "#                                 else None\n",
    "#                             ),\n",
    "#                             f\"author_repository_discussions_count{timestamp_suffix}\": (\n",
    "#                                 author[\"repositoryDiscussions\"][\"totalCount\"]\n",
    "#                                 if author and author[\"repositoryDiscussions\"]\n",
    "#                                 else None\n",
    "#                             ),\n",
    "#                             f\"author_repository_discussion_comments_count{timestamp_suffix}\": (\n",
    "#                                 author[\"repositoryDiscussionComments\"][\"totalCount\"]\n",
    "#                                 if author and author[\"repositoryDiscussionComments\"]\n",
    "#                                 else None\n",
    "#                             ),\n",
    "#                             f\"author_organizations{timestamp_suffix}\": author_organizations,\n",
    "#                             \"spam_search_keyword\": keyword,\n",
    "#                         }\n",
    "#                     )\n",
    "\n",
    "#                 # Pagination\n",
    "#                 page_info = response[\"search\"][\"pageInfo\"]\n",
    "#                 if page_info[\"hasNextPage\"]:\n",
    "#                     after_cursor = page_info[\"endCursor\"]\n",
    "#                 else:\n",
    "#                     break\n",
    "#             with open(\"progress.pkl\", \"wb\") as f:\n",
    "#                 pickle.dump({\"df\": se_fm_repository_data, \"start_index\": index + 1}, f)\n",
    "#             #  inner execution\n",
    "\n",
    "#             days_interval = default_days_interval # Reset interval to normal\n",
    "#         except Exception as e:\n",
    "#             log_activity(\n",
    "#                 f\"Failed to retrieve data for keywords '{keyword}' in date range: {date_range} {e}\"\n",
    "#             )\n",
    "#             # Save progress before terminating\n",
    "#             with open(\"progress.pkl\", \"wb\") as f:\n",
    "#                 pickle.dump({\"df\": df, \"start_index\": index}, f)\n",
    "#             raise\n",
    "    \n",
    "#     start_date = keyword_search_next_date # start next date iteration from the last used keyword search date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the .pkl file\n",
    "with open(\"progress.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Check if it's a list of dictionaries\n",
    "if isinstance(data[\"df\"], list) and all(isinstance(d, dict) for d in data[\"df\"]):\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data[\"df\"])\n",
    "    \n",
    "    # Remove duplicates by 'id'\n",
    "    df = df.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "\n",
    "    # Convert back to a list of dictionaries\n",
    "    cleaned_data = {**data, \"df\": df.to_dict(orient=\"records\")}\n",
    "\n",
    "# Check if it's already a DataFrame\n",
    "elif isinstance(data[\"df\"], pd.DataFrame):\n",
    "    # Remove duplicates by 'id'\n",
    "    cleaned_df = data[\"df\"].drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "    cleaned_data = {**data, \"df\": cleaned_df}\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(\"progress.pkl\", \"wb\") as file:\n",
    "    pickle.dump(cleaned_data, file)\n",
    "\n",
    "log_activity(\"Duplicates removed and data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bca9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_pkl_content_as_csv_and_json(filepath):\n",
    "    \"\"\"\n",
    "    This function reads a pickle file from the given filepath\n",
    "    and saves the data contained in the \"df\" key to both a CSV file and a JSON file.\n",
    "    The CSV file is saved with the name \"spam_data_without_org_join_date.csv\" and the JSON file is saved\n",
    "    with the name \"spam_data_without_org_join_date.json\".\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the pickle file to be read.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error reading the pickle file or writing the CSV/JSON files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        log_activity(f\"Content of {filepath}:\\n\")\n",
    "        filename = \"spam_data_without_org_join_date\"\n",
    "        pd.DataFrame(data[\"df\"]).to_csv(f\"{filename}.csv\", index=True)\n",
    "        log_activity(f\"Data written to {filename}.csv successfully.\")\n",
    "        try:\n",
    "            with open(f\"{filename}.json\", \"w\") as f:\n",
    "                json.dump(data[\"df\"], f, indent=4)\n",
    "            log_activity(f\"Data written to {filename}.json successfully.\")\n",
    "        except Exception as e:\n",
    "            log_activity(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        log_activity(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = \"progress.pkl\"\n",
    "save_pkl_content_as_csv_and_json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0deeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate metadata\n",
    "df = []\n",
    "start_index = 0\n",
    "\n",
    "\n",
    "\n",
    "def generate_metadata(filepath):\n",
    "    \"\"\" \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        log_activity(f\"Content of {filepath}:\\n\")\n",
    "        filename = \"spam_data.meta\"\n",
    "        pull_requests = data[\"df\"]\n",
    "        unique_repository = {}\n",
    "        unique_pr_author = {}\n",
    "        unique_pr_spam_labeler = {}\n",
    "        unique_pr_closer = {}\n",
    "        merged_pr_count = 0\n",
    "        closed_pr_count = 0\n",
    "        for pull_request in pull_requests:\n",
    "            def update_unique_value_dict(info_dict, key, value):\n",
    "                if not value:\n",
    "                    log_activity(f\"Warning: Pull request missing '{key}' {pull_request}\")\n",
    "                    return False\n",
    "                if value not in info_dict:\n",
    "                    info_dict[value] = value\n",
    "                return True\n",
    "\n",
    "            # Update repository count\n",
    "            update_unique_value_dict(unique_repository, \"repository_name_with_owner\", pull_request[\"repository_name_with_owner\"])\n",
    "\n",
    "            # Update author count\n",
    "            update_unique_value_dict(unique_pr_author, \"author_name\", pull_request[\"author_name\"])\n",
    "\n",
    "            # Update spam labeler count\n",
    "            update_unique_value_dict(unique_pr_spam_labeler, \"labeled_spam_by\", pull_request[\"labeled_spam_by\"])\n",
    "\n",
    "            # Update closer count\n",
    "            update_unique_value_dict(unique_pr_closer, \"closed_by\", pull_request[\"closed_by\"])\n",
    "\n",
    "            merged_pr_count += 1 if pull_request[\"merged\"] else 0\n",
    "            closed_pr_count += 1 if pull_request[\"closed\"] is not None else 0\n",
    "\n",
    "\n",
    "        total_prs= len(pull_requests)\n",
    "        unique_repository_count= len(unique_repository)\n",
    "        unique_pr_author_count= len(unique_pr_author)\n",
    "        unique_pr_spam_labeler_count= len(unique_pr_spam_labeler)\n",
    "        unique_pr_closer_count= len(unique_pr_closer)\n",
    "        \n",
    "        df.append(\n",
    "            {\n",
    "            \"total_prs\": total_prs,\n",
    "            \"unique_repository_count\": unique_repository_count,\n",
    "            \"unique_repository_ratio\": round(unique_repository_count / total_prs, 3),\n",
    "            \"unique_pr_author_count\": unique_pr_author_count,\n",
    "            \"unique_pr_author_ratio\": round(unique_pr_author_count / total_prs, 3),\n",
    "            \"unique_pr_spam_labeler_count\": unique_pr_spam_labeler_count,\n",
    "            \"unique_pr_spam_labeler_ratio\": round(unique_pr_spam_labeler_count / total_prs, 3),\n",
    "            \"unique_pr_closer_count\": unique_pr_closer_count,\n",
    "            \"unique_pr_closer_ratio\": round(unique_pr_closer_count / total_prs, 3),\n",
    "            \"merged_pr_count\": merged_pr_count,\n",
    "            \"merged_pr_ratio\": round(merged_pr_count / total_prs, 3),\n",
    "            \"closed_pr_count\": closed_pr_count,\n",
    "            \"closed_pr_ratio\": round(closed_pr_count / total_prs, 3),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        pd.DataFrame(df).to_csv(f\"{filename}.csv\", index=True)\n",
    "        log_activity(f\"Data written to {filename}.csv successfully.\")\n",
    "        try:\n",
    "            with open(f\"{filename}.json\", \"w\") as f:\n",
    "                json.dump(df, f, indent=4)\n",
    "            log_activity(f\"Data written to {filename}.json successfully.\")\n",
    "        except Exception as e:\n",
    "            log_activity(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        log_activity(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = \"progress.pkl\"\n",
    "generate_metadata(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469812ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
