{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4babdda8-11f7-485e-bad8-d8c5b8eebb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import itertools\n",
    "import os\n",
    "from datetime import datetime\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Read tokens from a text file\n",
    "tokens_file = \"./env/tokens.txt\"\n",
    "with open(tokens_file, \"r\") as file:\n",
    "    tokens = file.read().splitlines()\n",
    "\n",
    "# Create an iterator to cycle through the tokens\n",
    "token_iterator = itertools.cycle(tokens)\n",
    "current_token = next(token_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0c281d1-993b-497b-8984-0e32ac6a61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of User-Agents for randomization\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "]\n",
    "\n",
    "# Define headers to authenticate using the first token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {current_token}\",\n",
    "    \"User-Agent\": random.choice(user_agents)\n",
    "}\n",
    "\n",
    "# Setup GraphQL endpoint and client\n",
    "graphql_url = \"https://api.github.com/graphql\"\n",
    "transport = RequestsHTTPTransport(url=graphql_url, headers=headers, use_json=True)\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062f1b0-a9df-4f94-bb18-56f42774bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all tokens to verify their validity\n",
    "def test_all_tokens():\n",
    "    test_query = gql(\n",
    "        \"\"\"\n",
    "        {\n",
    "          viewer {\n",
    "            login\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    for i, token in enumerate(tokens):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"User-Agent\": random.choice(user_agents)\n",
    "        }\n",
    "        transport = RequestsHTTPTransport(url=graphql_url, headers=headers, use_json=True)\n",
    "        client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "        \n",
    "        try:\n",
    "            response = client.execute(test_query)\n",
    "            print(f\"Token {i+1}/{len(tokens)} is valid. Logged in as: {response['viewer']['login']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Token {i+1}/{len(tokens)} failed with error: {e}\")\n",
    "\n",
    "# Run the token validation\n",
    "test_all_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdd85df0-71b7-46c8-9ed4-eceb724201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphQL query\n",
    "query_template = gql(\n",
    "    \"\"\"\n",
    "    query searchRepositories($keyword: String!, $afterCursor: String, $first: Int) {\n",
    "      search(query: $keyword, type: REPOSITORY, first: $first, after: $afterCursor) {\n",
    "        repositoryCount\n",
    "        edges {\n",
    "          cursor\n",
    "          node {\n",
    "            ... on Repository {\n",
    "              name\n",
    "              description\n",
    "              repositoryTopics(first: 100) {\n",
    "                            edges {\n",
    "                                node {\n",
    "                                    topic {\n",
    "                                        name\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "              url\n",
    "              stargazers {\n",
    "                totalCount\n",
    "              }\n",
    "              forks {\n",
    "                totalCount\n",
    "              }\n",
    "              watchers {\n",
    "                totalCount\n",
    "              }\n",
    "              issues(states: OPEN) {\n",
    "                totalCount\n",
    "              }\n",
    "              issuesClosed: issues(states: CLOSED) {\n",
    "                totalCount\n",
    "              }\n",
    "              pullRequests(states: OPEN) {\n",
    "                totalCount\n",
    "              }\n",
    "              pullRequestsClosed: pullRequests(states: CLOSED) {\n",
    "                totalCount\n",
    "              }\n",
    "              defaultBranchRef {\n",
    "                target {\n",
    "                  ... on Commit {\n",
    "                    history {\n",
    "                      totalCount\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              createdAt\n",
    "              pushedAt\n",
    "              releases {\n",
    "                totalCount\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        pageInfo {\n",
    "          endCursor\n",
    "          hasNextPage\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7b32e41-7d3f-458b-ad0f-0c13d3146813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_contributor_count(repo_owner, repo_name):\n",
    "#     global current_token\n",
    "#     max_retries = 3\n",
    "#     retries = 0\n",
    "#     while retries < max_retries:\n",
    "#         try:\n",
    "#             # Randomize User-Agent for each query\n",
    "#             headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "#             headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "#             url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contributors?per_page=1&anon=true\"\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             if response.status_code == 200:\n",
    "#                 return int(response.headers.get(\"Link\", \"\").split(\",\")[-1].split(\"&page=\")[-1].split(\">\")[0]) if \"Link\" in response.headers else len(response.json())\n",
    "#             elif response.status_code == 403:\n",
    "#                 print(f\"Rate limit exceeded, switching token... (Attempt {retries + 1}/{max_retries})\")\n",
    "#                 current_token = next(token_iterator)\n",
    "#                 retries += 1\n",
    "#             else:\n",
    "#                 response.raise_for_status()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error: {e}, retrying... (Attempt {retries + 1}/{max_retries})\")\n",
    "#             retries += 1\n",
    "#     raise Exception(\"Max retries reached. Unable to complete the request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c2427-7797-4088-a144-6045c87ee349",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.headers = headers\n",
    "# Check rate limit before executing the main query\n",
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "rate_limit_response = client.execute(rate_limit_query)\n",
    "print(f\"Rate limit: {rate_limit_response['rateLimit']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49acb9fd-7a25-458d-aec7-8f302a653c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "def execute_query(keyword, first=100, after_cursor=None):\n",
    "    global current_token\n",
    "    while True:\n",
    "        try:\n",
    "            # Randomize User-Agent for each query\n",
    "            headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "            transport.headers = headers\n",
    "            # Check rate limit before executing the main query\n",
    "            rate_limit_response = client.execute(rate_limit_query)\n",
    "            remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "            if remaining < 100:\n",
    "                print(f\"Rate limit remaining ({remaining}) is below threshold. Switching token...\")\n",
    "                # Set up to track whether we have cycled through all tokens\n",
    "                all_tokens_checked = False\n",
    "                initial_token = current_token\n",
    "            \n",
    "                while not all_tokens_checked:\n",
    "                    # Switch to the next token\n",
    "                    current_token = next(token_iterator)\n",
    "                    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "                    transport.headers = headers\n",
    "                    \n",
    "                    # Check the rate limit of the new token\n",
    "                    rate_limit_response = client.execute(rate_limit_query)\n",
    "                    remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "            \n",
    "                    if remaining >= 100:\n",
    "                        print(f\"Switched to a new token with sufficient rate limit ({remaining} remaining).\")\n",
    "                        break\n",
    "            \n",
    "                    # Check if we have cycled through all tokens\n",
    "                    if current_token == initial_token:\n",
    "                        print(\"All tokens are below threshold. Waiting for 1 hour...\")\n",
    "                        time.sleep(3600)\n",
    "                        all_tokens_checked = True\n",
    "            \n",
    "                continue\n",
    "            return client.execute(query_template, variable_values={\"keyword\": keyword, \"first\": first, \"afterCursor\": after_cursor})\n",
    "        except Exception as e:\n",
    "            if \"API rate limit\" in str(e):\n",
    "                print(f\"Rate limit reached: {e}, switching token... (Attempt with first {first})\")\n",
    "                current_token = next(token_iterator)\n",
    "                headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "            else:\n",
    "                if first > 1:\n",
    "                    first = max(1, first // 2)\n",
    "                    print(f\"Error: {e}, reducing number of results and retrying... (Attempt with first {first})\")\n",
    "                else:\n",
    "                    break\n",
    "    print(\"Max retries reached. Sleeping for 60 minutes and switching token...\")\n",
    "    time.sleep(3600)\n",
    "    current_token = next(token_iterator)\n",
    "    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "    transport.headers = headers\n",
    "    return execute_query(keyword, first, after_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10e2f9a0-95df-4b00-8544-efeb1ba4d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if os.path.exists('progress.pkl'):\n",
    "    with open('progress.pkl', 'rb') as f:\n",
    "        progress_data = pickle.load(f)\n",
    "        df = progress_data['df']\n",
    "        start_index = progress_data['start_index']\n",
    "else:\n",
    "    df = []\n",
    "    start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abb89147-ab2c-4654-a8d7-637737c90537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.05s/it]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# template\n",
    "keywords = ['spam']\n",
    "index = start_index\n",
    "se_fm_repository_data = df\n",
    "for keyword in tqdm(keywords):\n",
    "    search_keyword = f'\"{keyword}\" in:name,readme,description,topics created:>=2024-11-30 fork:false is:public archived:false size:>0 stars:>=5 sort:stars-desc'\n",
    "    try:\n",
    "        after_cursor = None\n",
    "        while True:\n",
    "            response = execute_query(search_keyword, first=100, after_cursor=after_cursor)\n",
    "            if response['search']['repositoryCount'] == 0:\n",
    "                break\n",
    "            # Extract repositories\n",
    "            for edge in response['search']['edges']:\n",
    "                repo = edge['node']\n",
    "                df.append({\n",
    "                    \"name\": repo[\"name\"],\n",
    "                    \"description\": repo[\"description\"],\n",
    "                    \"url\": repo[\"url\"],\n",
    "                    \"topics\": [topic[\"node\"][\"topic\"][\"name\"] for topic in repo[\"repositoryTopics\"][\"edges\"]],\n",
    "                    \"stars\": repo[\"stargazers\"][\"totalCount\"],\n",
    "                    \"forks\": repo[\"forks\"][\"totalCount\"],\n",
    "                    \"watchers\": repo[\"watchers\"][\"totalCount\"],\n",
    "                    \"open_issues\": repo[\"issues\"][\"totalCount\"],\n",
    "                    \"closed_issues\": repo[\"issuesClosed\"][\"totalCount\"],\n",
    "                    \"total_issues\": repo[\"issues\"][\"totalCount\"] + repo[\"issuesClosed\"][\"totalCount\"],\n",
    "                    \"open_pull_requests\": repo[\"pullRequests\"][\"totalCount\"],\n",
    "                    \"closed_pull_requests\": repo[\"pullRequestsClosed\"][\"totalCount\"],\n",
    "                    \"total_pull_requests\": repo[\"pullRequests\"][\"totalCount\"] + repo[\"pullRequestsClosed\"][\"totalCount\"],\n",
    "                    \"commits\": repo[\"defaultBranchRef\"][\"target\"][\"history\"][\"totalCount\"],\n",
    "                    \"created_at\": repo[\"createdAt\"],\n",
    "                    \"last_commit\": repo[\"pushedAt\"],\n",
    "                    \"releases\": repo[\"releases\"][\"totalCount\"]\n",
    "                })\n",
    "\n",
    "            # Pagination\n",
    "            page_info = response['search']['pageInfo']\n",
    "            if page_info['hasNextPage']:\n",
    "                after_cursor = page_info['endCursor']\n",
    "            else:\n",
    "                break\n",
    "        with open('progress.pkl', 'wb') as f:\n",
    "            pickle.dump({'df': se_fm_repository_data, 'start_index': index + 1}, f)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve data for keywords '{keyword}': {e}\")\n",
    "        # Save progress before terminating\n",
    "        with open('progress.pkl', 'wb') as f:\n",
    "            pickle.dump({'df': df, 'start_index': index}, f)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24bca9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of progress.pkl:\n",
      "\n",
      "Data is a Dictionary:\n",
      "\n",
      "Key: df\n",
      "Value is a List:\n",
      "[{'name': 'Solana-Raydium-Bundler', 'description': 'Raydium bundler, raydium bundler with jito, Raydium bundler, Raydium bundler', 'url': 'https://github.com/g0drlc/Solana-Raydium-Bundler', 'topics': ['raydium', 'raydium-bundler-bot', 'spl-token'], 'stars': 131, 'forks': 113, 'watchers': 1, 'open_issues': 0, 'closed_issues': 0, 'total_issues': 0, 'open_pull_requests': 0, 'closed_pull_requests': 0, 'total_pull_requests': 0, 'commits': 5, 'created_at': '2024-12-05T08:48:15Z', 'last_commit': '2024-12-23T14:42:11Z', 'releases': 0}, {'name': 'tirreno', 'description': 'Open source security user analytics platform. Get started - free.', 'url': 'https://github.com/TirrenoTechnologies/tirreno', 'topics': ['analytics', 'fraud-detection', 'fraud-management', 'fraud-prevention', 'intelligence', 'intranet', 'php', 'privacy', 'uba', 'web-analytics', 'self-hosted', 'behavior-analytics', 'php-project', 'rules-engine', 'audit-trail', 'audit-log', 'osint', 'osint-tool', 'ciso', 'self-hosting'], 'stars': 122, 'forks': 7, 'watchers': 1, 'open_issues': 0, 'closed_issues': 0, 'total_issues': 0, 'open_pull_requests': 0, 'closed_pull_requests': 0, 'total_pull_requests': 0, 'commits': 8, 'created_at': '2024-12-08T21:24:42Z', 'last_commit': '2025-01-02T18:33:32Z', 'releases': 1}, {'name': 'pumpfun-comment-spam-bot', 'description': 'shill & comment bot for pump fun', 'url': 'https://github.com/noirge/pumpfun-comment-spam-bot', 'topics': ['pumpfun', 'pumpfun-comment', 'pumpfun-spam', 'solana-bot'], 'stars': 112, 'forks': 4, 'watchers': 4, 'open_issues': 0, 'closed_issues': 0, 'total_issues': 0, 'open_pull_requests': 0, 'closed_pull_requests': 1, 'total_pull_requests': 1, 'commits': 7, 'created_at': '2024-12-21T12:30:57Z', 'last_commit': '2024-12-29T13:21:06Z', 'releases': 0}, {'name': 'solana-arbitrage-bot', 'description': 'Automated Arbitrage Bot Using Jupiter on Solana', 'url': 'https://github.com/ARBProtocol-Rabby/solana-arbitrage-bot', 'topics': ['amm', 'arbitrage', 'arbitragebot', 'blockchain', 'bot', 'crypto', 'crypto-arbitrage', 'crypto-bot', 'jupiter', 'jupiter-aggregator', 'solana', 'solana-arbitrage', 'solana-jupiter', 'solana-token', 'solanabot', 'tradingbot', 'web3'], 'stars': 87, 'forks': 65, 'watchers': 37, 'open_issues': 4, 'closed_issues': 0, 'total_issues': 4, 'open_pull_requests': 0, 'closed_pull_requests': 0, 'total_pull_requests': 0, 'commits': 1, 'created_at': '2025-01-04T08:06:22Z', 'last_commit': '2025-01-04T08:14:08Z', 'releases': 0}, {'name': 'arbigent', 'description': \"Zero to AI agent testing in minutes for Android, iOS, and Web apps. Arbigent's intuitive UI and powerful code interface make it accessible to everyone, while its scenario breakdown feature ensures scalability for even the most complex tasks.\", 'url': 'https://github.com/takahirom/arbigent', 'topics': ['ai', 'android', 'ios', 'testing', 'agentic', 'aiagent'], 'stars': 83, 'forks': 1, 'watchers': 5, 'open_issues': 3, 'closed_issues': 0, 'total_issues': 3, 'open_pull_requests': 0, 'closed_pull_requests': 0, 'total_pull_requests': 0, 'commits': 294, 'created_at': '2024-12-31T08:38:58Z', 'last_commit': '2025-01-13T07:42:55Z', 'releases': 11}, {'name': 'data-engineering-projects', 'description': None, 'url': 'https://github.com/garage-education/data-engineering-projects', 'topics': [], 'stars': 74, 'forks': 8, 'watchers': 4, 'open_issues': 0, 'closed_issues': 0, 'total_issues': 0, 'open_pull_requests': 2, 'closed_pull_requests': 0, 'total_pull_requests': 2, 'commits': 6, 'created_at': '2024-12-27T00:25:45Z', 'last_commit': '2024-12-27T00:31:33Z', 'releases': 0}, {'name': 'Protect_Loader', 'description': 'A fucking real shellcode loader with a GUI. Work-in-Progress.', 'url': 'https://github.com/furax124/Protect_Loader', 'topics': [], 'stars': 64, 'forks': 11, 'watchers': 2, 'open_issues': 0, 'closed_issues': 0, 'total_issues': 0, 'open_pull_requests': 0, 'closed_pull_requests': 0, 'total_pull_requests': 0, 'commits': 71, 'created_at': '2024-12-27T21:33:42Z', 'last_commit': '2025-01-10T20:38:05Z', 'releases': 0}, {'name': 'flyphish', 'description': 'Deploy a phishing infrastructure on the fly.', 'url': 'https://github.com/VirtualSamuraii/flyphish', 'topics': [], 'stars': 63, 'forks': 7, 'watchers': 2, 'open_issues': 0, 'closed_issues': 0, 'total_issues': 0, 'open_pull_requests': 0, 'closed_pull_requests': 0, 'total_pull_requests': 0, 'commits': 3, 'created_at': '2024-12-20T23:28:50Z', 'last_commit': '2024-12-21T14:57:28Z', 'releases': 0}, {'name': 'YouTubeBooster', 'description': 'YouTube Booster is a tool designed to promote your YouTube channel, helping you increase views, engagement, and audience growth.', 'url': 'https://github.com/mirshapovalovak/YouTubeBooster', 'topics': [], 'stars': 59, 'forks': 0, 'watchers': 1, 'open_issues': 0, 'closed_issues': 0, 'total_issues': 0, 'open_pull_requests': 0, 'closed_pull_requests': 0, 'total_pull_requests': 0, 'commits': 20, 'created_at': '2024-12-11T10:19:32Z', 'last_commit': '2024-12-15T17:53:10Z', 'releases': 0}, {'name': 'LLMs_from_scratch', 'description': 'Learning records for building a large language model from scratch', 'url': 'https://github.com/yhlleo/LLMs_from_scratch', 'topics': [], 'stars': 39, 'forks': 0, 'watchers': 2, 'open_issues': 0, 'closed_issues': 1, 'total_issues': 1, 'open_pull_requests': 0, 'closed_pull_requests': 0, 'total_pull_requests': 0, 'commits': 32, 'created_at': '2024-12-23T06:39:56Z', 'last_commit': '2025-01-01T07:51:29Z', 'releases': 0}]\n",
      "\n",
      "List Length: 198\n",
      "\n",
      "Key: start_index\n",
      "Value: 1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def display_pkl_content(filepath):\n",
    "    \"\"\"\n",
    "    Loads and displays the content of a .pkl (pickle) file in a Jupyter Notebook.\n",
    "    Handles different data structures within the pickle file and provides informative output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        print(f\"Content of {filepath}:\\n\")\n",
    "\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            print(\"Data is a Pandas DataFrame:\")\n",
    "            print(data.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\")) # Display first few rows as a Markdown table\n",
    "            print(\"\\nDataFrame Info:\")\n",
    "            data.info() # Display DataFrame information\n",
    "        elif isinstance(data, list):\n",
    "            print(\"Data is a List:\")\n",
    "            if all(isinstance(item, dict) for item in data): # Check if list contains dictionaries\n",
    "              df = pd.DataFrame(data)\n",
    "              print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\")) # Display first few rows as a Markdown table\n",
    "              print(\"\\nDataFrame Info:\")\n",
    "              df.info() # Display DataFrame information\n",
    "            else:\n",
    "              print(data[:10]) # Print first 10 items if not dictionaries\n",
    "              print(f\"\\nList Length: {len(data)}\")\n",
    "        elif isinstance(data, dict):\n",
    "            print(\"Data is a Dictionary:\")\n",
    "            for key, value in data.items():\n",
    "                print(f\"\\nKey: {key}\")\n",
    "                if isinstance(value, pd.DataFrame):\n",
    "                    print(\"Value is a Pandas DataFrame:\")\n",
    "                    print(value.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "                    print(\"\\nDataFrame Info:\")\n",
    "                    value.info()\n",
    "                elif isinstance(value, list):\n",
    "                    print(\"Value is a List:\")\n",
    "                    print(value[:10])\n",
    "                    print(f\"\\nList Length: {len(value)}\")\n",
    "                else:\n",
    "                    print(f\"Value: {value}\")\n",
    "        elif isinstance(data, set):\n",
    "          print(\"Data is a Set:\")\n",
    "          print(list(data)[:10])\n",
    "          print(f\"\\nSet Length: {len(data)}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Data is of type: {type(data)}\")\n",
    "            print(data)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "    except pickle.UnpicklingError:\n",
    "        print(f\"Error: Could not unpickle data from {filepath}. The file might be corrupted or use a different pickle protocol.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "filepath = 'progress.pkl'  # Replace with the actual path to your .pkl file\n",
    "display_pkl_content(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469812ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
