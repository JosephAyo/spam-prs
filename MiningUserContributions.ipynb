{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4babdda8-11f7-485e-bad8-d8c5b8eebb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import itertools\n",
    "import os\n",
    "from datetime import datetime\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Read tokens from a text file\n",
    "tokens_file = \"./env/tokens.txt\"\n",
    "with open(tokens_file, \"r\") as file:\n",
    "    tokens = file.read().splitlines()\n",
    "\n",
    "# Create an iterator to cycle through the tokens\n",
    "token_iterator = itertools.cycle(tokens)\n",
    "current_token = next(token_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0c281d1-993b-497b-8984-0e32ac6a61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of User-Agents for randomization\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "]\n",
    "\n",
    "# Define headers to authenticate using the first token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {current_token}\",\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "}\n",
    "\n",
    "# Setup GraphQL endpoint and client\n",
    "graphql_url = \"https://api.github.com/graphql\"\n",
    "transport = RequestsHTTPTransport(url=graphql_url, headers=headers, use_json=True)\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062f1b0-a9df-4f94-bb18-56f42774bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all tokens to verify their validity\n",
    "def test_all_tokens():\n",
    "    test_query = gql(\n",
    "        \"\"\"\n",
    "        {\n",
    "          viewer {\n",
    "            login\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    for i, token in enumerate(tokens):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"User-Agent\": random.choice(user_agents),\n",
    "        }\n",
    "        transport = RequestsHTTPTransport(\n",
    "            url=graphql_url, headers=headers, use_json=True\n",
    "        )\n",
    "        client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "        try:\n",
    "            response = client.execute(test_query)\n",
    "            print(\n",
    "                f\"Token {i+1}/{len(tokens)} is valid. Logged in as: {response['viewer']['login']}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Token {i+1}/{len(tokens)} failed with error: {e}\")\n",
    "\n",
    "\n",
    "# Run the token validation\n",
    "test_all_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cdd85df0-71b7-46c8-9ed4-eceb724201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphQL query\n",
    "query_template = gql(\n",
    "    \"\"\"\n",
    "      query($username: String!, $start: DateTime!, $end: DateTime!) {\n",
    "        user(login: $username) {\n",
    "          url\n",
    "          contributionsCollection(from: $start, to: $end) {\n",
    "            contributionCalendar {\n",
    "              totalContributions\n",
    "            }\n",
    "            totalCommitContributions\n",
    "            totalIssueContributions\n",
    "            totalPullRequestContributions\n",
    "            totalPullRequestReviewContributions\n",
    "            totalRepositoriesWithContributedCommits\n",
    "            totalRepositoriesWithContributedIssues\n",
    "            totalRepositoriesWithContributedPullRequestReviews\n",
    "            totalRepositoriesWithContributedPullRequests\n",
    "            totalRepositoryContributions\n",
    "            restrictedContributionsCount\n",
    "            # move this to a separate query and limit start and end dates to the 1 before and after when the 'spam' PR was submitted\n",
    "            # commitContributionsByRepository {\n",
    "            #   repository {\n",
    "            #     name\n",
    "            #     owner {\n",
    "            #       login\n",
    "            #     }\n",
    "            #   }\n",
    "            #   contributions {\n",
    "            #     totalCount\n",
    "            #   }\n",
    "            # }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7b32e41-7d3f-458b-ad0f-0c13d3146813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_contributor_count(repo_owner, repo_name):\n",
    "#     global current_token\n",
    "#     max_retries = 3\n",
    "#     retries = 0\n",
    "#     while retries < max_retries:\n",
    "#         try:\n",
    "#             # Randomize User-Agent for each query\n",
    "#             headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "#             headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "#             url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contributors?per_page=1&anon=true\"\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             if response.status_code == 200:\n",
    "#                 return int(response.headers.get(\"Link\", \"\").split(\",\")[-1].split(\"&page=\")[-1].split(\">\")[0]) if \"Link\" in response.headers else len(response.json())\n",
    "#             elif response.status_code == 403:\n",
    "#                 print(f\"Rate limit exceeded, switching token... (Attempt {retries + 1}/{max_retries})\")\n",
    "#                 current_token = next(token_iterator)\n",
    "#                 retries += 1\n",
    "#             else:\n",
    "#                 response.raise_for_status()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error: {e}, retrying... (Attempt {retries + 1}/{max_retries})\")\n",
    "#             retries += 1\n",
    "#     raise Exception(\"Max retries reached. Unable to complete the request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c2427-7797-4088-a144-6045c87ee349",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.headers = headers\n",
    "# Check rate limit before executing the main query\n",
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "rate_limit_response = client.execute(rate_limit_query)\n",
    "print(f\"Rate limit: {rate_limit_response['rateLimit']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49acb9fd-7a25-458d-aec7-8f302a653c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def execute_query(username, start=100, end=None):\n",
    "    global current_token\n",
    "    print(\n",
    "        f\"Executing query with username: {username}, start: {start}, afterCursor: {end}\"\n",
    "    )\n",
    "    while True:\n",
    "        try:\n",
    "            # Randomize User-Agent for each query\n",
    "            headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "            transport.headers = headers\n",
    "            # Check rate limit before executing the main query\n",
    "            rate_limit_response = client.execute(rate_limit_query)\n",
    "            remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "            if remaining < 100:\n",
    "                print(\n",
    "                    f\"Rate limit remaining ({remaining}) is below threshold. Switching token...\"\n",
    "                )\n",
    "                # Set up to track whether we have cycled through all tokens\n",
    "                all_tokens_checked = False\n",
    "                initial_token = current_token\n",
    "\n",
    "                while not all_tokens_checked:\n",
    "                    # Switch to the next token\n",
    "                    current_token = next(token_iterator)\n",
    "                    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "                    transport.headers = headers\n",
    "\n",
    "                    # Check the rate limit of the new token\n",
    "                    rate_limit_response = client.execute(rate_limit_query)\n",
    "                    remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "\n",
    "                    if remaining >= 100:\n",
    "                        print(\n",
    "                            f\"Switched to a new token with sufficient rate limit ({remaining} remaining).\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "                    # Check if we have cycled through all tokens\n",
    "                    if current_token == initial_token:\n",
    "                        print(\"All tokens are below threshold. Waiting for 1 hour...\")\n",
    "                        time.sleep(3600)\n",
    "                        all_tokens_checked = True\n",
    "\n",
    "                continue\n",
    "            return client.execute(\n",
    "                query_template,\n",
    "                variable_values={\n",
    "                    \"username\": username,\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"API rate limit\" in str(e):\n",
    "                print(\n",
    "                    f\"Rate limit reached: {e}, switching token... (Attempt with start {start} and end {end})\"\n",
    "                )\n",
    "                current_token = next(token_iterator)\n",
    "                headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "            else:\n",
    "                print(f\"Error: {e}, stopped during (Attempt with start {start} and end {end})\")\n",
    "                break\n",
    "                # if first > 1:\n",
    "                #     first = max(1, first // 2)\n",
    "                #     print(\n",
    "                #         f\"Error: {e}, reducing number of results and retrying... (Attempt with first {first})\"\n",
    "                #     )\n",
    "                # else:\n",
    "                #     break\n",
    "    print(\"Max retries reached. Sleeping for 60 minutes and switching token...\")\n",
    "    time.sleep(3600)\n",
    "    current_token = next(token_iterator)\n",
    "    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "    transport.headers = headers\n",
    "    return execute_query(username, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10e2f9a0-95df-4b00-8544-efeb1ba4d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if os.path.exists(\"user_contribution_count_progress.pkl\"):\n",
    "    with open(\"user_contribution_count_progress.pkl\", \"rb\") as f:\n",
    "        progress_data = pickle.load(f)\n",
    "        df = progress_data[\"df\"]\n",
    "        start_index = progress_data[\"start_index\"]\n",
    "else:\n",
    "    df = []\n",
    "    start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a9a4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def get_unique_authors_with_creation_time(pull_requests):\n",
    "    \"\"\"\n",
    "    Extracts and returns a list of unique author information (name and earliest creation time)\n",
    "    from a list of pull requests.\n",
    "\n",
    "    Args:\n",
    "        pull_requests: A list of dictionaries representing pull requests,\n",
    "            each with 'author_name' and 'created_at' keys.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary contains:\n",
    "            - \"author_name\": The unique author name.\n",
    "            - \"first_created_at\": The earliest creation time (datetime object) for that author.\n",
    "        Returns an empty list if pull_requests is empty or None.\n",
    "    \"\"\"\n",
    "\n",
    "    if not pull_requests:  # Handle empty or None input\n",
    "        return []\n",
    "\n",
    "    author_info = {}\n",
    "\n",
    "    for pr in pull_requests:\n",
    "        author_name = pr.get(\"author_name\")\n",
    "        created_at_str = pr.get(\"author_account_created_at\")\n",
    "\n",
    "        if not author_name or not created_at_str:\n",
    "            print(f\"Warning: Pull request missing 'author_name' or 'created_at': {pr}\")\n",
    "            continue  # Skip this PR if it's missing required data\n",
    "\n",
    "        try:\n",
    "            created_at = datetime.datetime.strptime(\n",
    "                created_at_str, \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "            ).replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Invalid date format: {created_at_str} for PR: {pr}\")\n",
    "            continue\n",
    "\n",
    "        if author_name not in author_info:\n",
    "            author_info[author_name] = {\n",
    "                \"author_name\": author_name,\n",
    "                \"first_created_at\": created_at,\n",
    "            }\n",
    "        else:\n",
    "            if created_at < author_info[author_name][\"first_created_at\"]:\n",
    "                author_info[author_name][\"first_created_at\"] = created_at\n",
    "\n",
    "    return list(author_info.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69df882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb89147-ab2c-4654-a8d7-637737c90537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "index = start_index\n",
    "user_contribution_data = df\n",
    "current_year = datetime.datetime.now().year\n",
    "all_contributions = {}\n",
    "with open(\"progress.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    spam_prs = data[\"df\"]\n",
    "    unique_users = get_unique_authors_with_creation_time(spam_prs)\n",
    "    for user in unique_users:\n",
    "        username = user[\"author_name\"]\n",
    "        created_at: datetime = user.get(\"first_created_at\")\n",
    "        start_year = created_at.year\n",
    "        for year in range(start_year, current_year + 1):\n",
    "            try:\n",
    "                response = execute_query(\n",
    "                    username,\n",
    "                    start=f\"{year}-01-01T00:00:00Z\",\n",
    "                    end=f\"{year}-12-31T23:59:59Z\",\n",
    "                )\n",
    "                contributionsCollection = response[\"user\"][\"contributionsCollection\"]\n",
    "                df.append(\n",
    "                    {\n",
    "                        \"username\": username,\n",
    "                        \"year\": year,\n",
    "                        \"url\": response[\"user\"][\"url\"],\n",
    "                        \"contributions_count\": contributionsCollection[\n",
    "                            \"contributionCalendar\"\n",
    "                        ][\"totalContributions\"],\n",
    "                        \"account_created_at\": created_at.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                        \"total_commit_contributions\": contributionsCollection[\n",
    "                            \"totalCommitContributions\"\n",
    "                        ],\n",
    "                        \"total_issue_contributions\": contributionsCollection[\n",
    "                            \"totalIssueContributions\"\n",
    "                        ],\n",
    "                        \"total_pull_request_contributions\": contributionsCollection[\n",
    "                            \"totalPullRequestContributions\"\n",
    "                        ],\n",
    "                        \"total_pull_request_review_contributions\": contributionsCollection[\n",
    "                            \"totalPullRequestReviewContributions\"\n",
    "                        ],\n",
    "                        \"total_repositories_with_contributed_commits\": contributionsCollection[\n",
    "                            \"totalRepositoriesWithContributedCommits\"\n",
    "                        ],\n",
    "                        \"total_repositories_with_contributed_issues\": contributionsCollection[\n",
    "                            \"totalRepositoriesWithContributedIssues\"\n",
    "                        ],\n",
    "                        \"total_repositories_with_contributed_pull_request_reviews\": contributionsCollection[\n",
    "                            \"totalRepositoriesWithContributedPullRequestReviews\"\n",
    "                        ],\n",
    "                        \"total_repositories_with_contributed_pull_requests\": contributionsCollection[\n",
    "                            \"totalRepositoriesWithContributedPullRequests\"\n",
    "                        ],\n",
    "                        \"total_repository_contributions\": contributionsCollection[\n",
    "                            \"totalRepositoryContributions\"\n",
    "                        ],\n",
    "                        \"restricted_contributions_count\": contributionsCollection[\n",
    "                            \"restrictedContributionsCount\"\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "                # Extract pr\n",
    "                with open(\"user_contribution_count_progress.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(\n",
    "                        {\"df\": user_contribution_data, \"start_index\": index + 1}, f\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to retrieve data for username '{username}': {e}\")\n",
    "                # Save usercontrib-progress before terminating\n",
    "                with open(\"user_contribution_count_progress.pkl\", \"wb\") as f:\n",
    "                    pickle.dump({\"df\": df, \"start_index\": index}, f)\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bca9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileinput import filename\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_pkl_content(filepath):\n",
    "    \"\"\"\n",
    "    Loads and displays the content of a .pkl (pickle) file in a Jupyter Notebook.\n",
    "    Handles different data structures within the pickle file and provides informative output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        print(f\"Content of {filepath}:\\n\")\n",
    "\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            print(\"Data is a Pandas DataFrame:\")\n",
    "            print(\n",
    "                data.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\")\n",
    "            )  # Display first few rows as a Markdown table\n",
    "            print(\"\\nDataFrame Info:\")\n",
    "            data.info()  # Display DataFrame information\n",
    "        elif isinstance(data, list):\n",
    "            print(\"Data is a List:\")\n",
    "            if all(\n",
    "                isinstance(item, dict) for item in data\n",
    "            ):  # Check if list contains dictionaries\n",
    "                df = pd.DataFrame(data)\n",
    "                print(\n",
    "                    df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\")\n",
    "                )  # Display first few rows as a Markdown table\n",
    "                print(\"\\nDataFrame Info:\")\n",
    "                df.info()  # Display DataFrame information\n",
    "            else:\n",
    "                print(data[:10])  # Print first 10 items if not dictionaries\n",
    "                print(f\"\\nList Length: {len(data)}\")\n",
    "        elif isinstance(data, dict):\n",
    "            print(\"Data is a Dictionary:\")\n",
    "            pd.DataFrame(data[\"df\"]).to_csv(\"user_contribution.csv\", index=True)\n",
    "            filename = \"user_contribution_count.json\"\n",
    "            try:\n",
    "                with open(filename, \"w\") as f:\n",
    "                    json.dump(data['df'], f, indent=4)\n",
    "                print(f\"Data written to {filename} successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "            for key, value in data.items():\n",
    "                print(f\"\\nKey: {key}\")\n",
    "                if isinstance(value, pd.DataFrame):\n",
    "                    print(\"Value is a Pandas DataFrame:\")\n",
    "                    print(\n",
    "                        value.head().to_markdown(\n",
    "                            index=False, numalign=\"left\", stralign=\"left\"\n",
    "                        )\n",
    "                    )\n",
    "                    print(\"\\nDataFrame Info:\")\n",
    "                    value.info()\n",
    "                elif isinstance(value, list):\n",
    "                    print(\"Value is a List:\")\n",
    "                    print(value[:10])\n",
    "                    print(f\"\\nList Length: {len(value)}\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Value: {value}\")\n",
    "        elif isinstance(data, set):\n",
    "            print(\"Data is a Set:\")\n",
    "            print(list(data)[:10])\n",
    "            print(f\"\\nSet Length: {len(data)}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Data is of type: {type(data)}\")\n",
    "            print(data)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "    except pickle.UnpicklingError:\n",
    "        print(\n",
    "            f\"Error: Could not unpickle data from {filepath}. The file might be corrupted or use a different pickle protocol.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "filepath = \"user_contribution_count_progress.pkl\"  # Replace with the actual path to your .pkl file\n",
    "display_pkl_content(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469812ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
