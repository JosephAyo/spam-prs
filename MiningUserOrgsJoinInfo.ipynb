{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4babdda8-11f7-485e-bad8-d8c5b8eebb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import itertools\n",
    "import os\n",
    "from datetime import datetime\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Read tokens from a text file\n",
    "tokens_file = \"./env/tokens.txt\"\n",
    "with open(tokens_file, \"r\") as file:\n",
    "    tokens = file.read().splitlines()\n",
    "\n",
    "# Create an iterator to cycle through the tokens\n",
    "token_iterator = itertools.cycle(tokens)\n",
    "current_token = next(token_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e0c281d1-993b-497b-8984-0e32ac6a61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of User-Agents for randomization\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "]\n",
    "\n",
    "# Define headers to authenticate using the first token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {current_token}\",\n",
    "    \"User-Agent\": random.choice(user_agents),\n",
    "}\n",
    "\n",
    "# Setup GraphQL endpoint and client\n",
    "graphql_url = \"https://api.github.com/graphql\"\n",
    "transport = RequestsHTTPTransport(url=graphql_url, headers=headers, use_json=True)\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062f1b0-a9df-4f94-bb18-56f42774bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all tokens to verify their validity\n",
    "def test_all_tokens():\n",
    "    test_query = gql(\n",
    "        \"\"\"\n",
    "        {\n",
    "          viewer {\n",
    "            login\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    for i, token in enumerate(tokens):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"User-Agent\": random.choice(user_agents),\n",
    "        }\n",
    "        transport = RequestsHTTPTransport(\n",
    "            url=graphql_url, headers=headers, use_json=True\n",
    "        )\n",
    "        client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "        try:\n",
    "            response = client.execute(test_query)\n",
    "            print(\n",
    "                f\"Token {i+1}/{len(tokens)} is valid. Logged in as: {response['viewer']['login']}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Token {i+1}/{len(tokens)} failed with error: {e}\")\n",
    "\n",
    "\n",
    "# Run the token validation\n",
    "test_all_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cdd85df0-71b7-46c8-9ed4-eceb724201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphQL query\n",
    "query_template = gql(\n",
    "    \"\"\"\n",
    "    query GetUserJoinDate($keyword: String!, $afterCursor: String, $first: Int) {\n",
    "      organization(login: $keyword) {\n",
    "        membersWithRole(first: $first, after: $afterCursor) { \n",
    "          totalCount\n",
    "          edges {\n",
    "            node {\n",
    "              login\n",
    "              createdAt\n",
    "            }\n",
    "          }\n",
    "          pageInfo {\n",
    "            hasNextPage\n",
    "            endCursor\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d7b32e41-7d3f-458b-ad0f-0c13d3146813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_contributor_count(repo_owner, repo_name):\n",
    "#     global current_token\n",
    "#     max_retries = 3\n",
    "#     retries = 0\n",
    "#     while retries < max_retries:\n",
    "#         try:\n",
    "#             # Randomize User-Agent for each query\n",
    "#             headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "#             headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "#             url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contributors?per_page=1&anon=true\"\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             if response.status_code == 200:\n",
    "#                 return int(response.headers.get(\"Link\", \"\").split(\",\")[-1].split(\"&page=\")[-1].split(\">\")[0]) if \"Link\" in response.headers else len(response.json())\n",
    "#             elif response.status_code == 403:\n",
    "#                 print(f\"Rate limit exceeded, switching token... (Attempt {retries + 1}/{max_retries})\")\n",
    "#                 current_token = next(token_iterator)\n",
    "#                 retries += 1\n",
    "#             else:\n",
    "#                 response.raise_for_status()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error: {e}, retrying... (Attempt {retries + 1}/{max_retries})\")\n",
    "#             retries += 1\n",
    "#     raise Exception(\"Max retries reached. Unable to complete the request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c2427-7797-4088-a144-6045c87ee349",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.headers = headers\n",
    "# Check rate limit before executing the main query\n",
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "rate_limit_response = client.execute(rate_limit_query)\n",
    "print(f\"Rate limit: {rate_limit_response['rateLimit']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "49acb9fd-7a25-458d-aec7-8f302a653c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      viewer {\n",
    "        login\n",
    "      }\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        used\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def execute_query(keyword, first=100, after_cursor=None):\n",
    "    global current_token\n",
    "    print(\n",
    "        f\"Executing query with keyword: {keyword}, first: {first}, afterCursor: {after_cursor}\"\n",
    "    )\n",
    "    while True:\n",
    "        try:\n",
    "            # Randomize User-Agent for each query\n",
    "            headers[\"User-Agent\"] = random.choice(user_agents)\n",
    "            transport.headers = headers\n",
    "            # Check rate limit before executing the main query\n",
    "            rate_limit_response = client.execute(rate_limit_query)\n",
    "            remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "            if remaining < 100:\n",
    "                print(\n",
    "                    f\"Rate limit remaining ({remaining}) is below threshold. Switching token...\"\n",
    "                )\n",
    "                # Set up to track whether we have cycled through all tokens\n",
    "                all_tokens_checked = False\n",
    "                initial_token = current_token\n",
    "\n",
    "                while not all_tokens_checked:\n",
    "                    # Switch to the next token\n",
    "                    current_token = next(token_iterator)\n",
    "                    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "                    transport.headers = headers\n",
    "\n",
    "                    # Check the rate limit of the new token\n",
    "                    rate_limit_response = client.execute(rate_limit_query)\n",
    "                    remaining = rate_limit_response[\"rateLimit\"][\"remaining\"]\n",
    "\n",
    "                    if remaining >= 100:\n",
    "                        print(\n",
    "                            f\"Switched to a new token with sufficient rate limit ({remaining} remaining).\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "                    # Check if we have cycled through all tokens\n",
    "                    if current_token == initial_token:\n",
    "                        print(\"All tokens are below threshold. Waiting for 1 hour...\")\n",
    "                        time.sleep(3600)\n",
    "                        all_tokens_checked = True\n",
    "\n",
    "                continue\n",
    "            return client.execute(\n",
    "                query_template,\n",
    "                variable_values={\n",
    "                    \"keyword\": keyword,\n",
    "                    \"first\": first,\n",
    "                    \"afterCursor\": after_cursor,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"API rate limit\" in str(e):\n",
    "                print(\n",
    "                    f\"Rate limit reached: {e}, switching token... (Attempt with first {first})\"\n",
    "                )\n",
    "                current_token = next(token_iterator)\n",
    "                headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "            else:\n",
    "                if first > 1:\n",
    "                    first = max(1, first // 2)\n",
    "                    print(\n",
    "                        f\"Error: {e}, reducing number of results and retrying... (Attempt with first {first})\"\n",
    "                    )\n",
    "                else:\n",
    "                    break\n",
    "    print(\"Max retries reached. Sleeping for 60 minutes and switching token...\")\n",
    "    time.sleep(3600)\n",
    "    current_token = next(token_iterator)\n",
    "    headers[\"Authorization\"] = f\"Bearer {current_token}\"\n",
    "    transport.headers = headers\n",
    "    return execute_query(keyword, first, after_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "91781c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def get_unique_authors_with_organizations(pull_requests):\n",
    "\n",
    "    if not pull_requests:  # Handle empty or None input\n",
    "        return []\n",
    "\n",
    "    author_info = {}\n",
    "\n",
    "    for pr in pull_requests:\n",
    "        author_name = pr.get(\"author_name\")\n",
    "        created_at_str = pr.get(\"author_account_created_at\")\n",
    "        # Find the key that matches partially with 'author_organizations_as_at'\n",
    "        organizations_key = next(\n",
    "            (key for key in pr.keys() if key.startswith(\"author_organizations_as_at\")),\n",
    "            None,\n",
    "        )\n",
    "        organizations = pr.get(organizations_key, [])\n",
    "\n",
    "        if not author_name or not created_at_str:\n",
    "            print(f\"Warning: Pull request missing 'author_name' or 'created_at': {pr}\")\n",
    "            continue  # Skip this PR if it's missing required data\n",
    "\n",
    "        try:\n",
    "            created_at = datetime.datetime.strptime(\n",
    "            created_at_str, \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "            ).replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Invalid date format: {created_at_str} for PR: {pr}\")\n",
    "            continue\n",
    "\n",
    "        if author_name not in author_info:\n",
    "            author_info[author_name] = {\n",
    "            \"author_name\": author_name,\n",
    "            \"first_created_at\": created_at,\n",
    "            \"organizations\": organizations,\n",
    "            }\n",
    "        else:\n",
    "            if created_at < author_info[author_name][\"first_created_at\"]:\n",
    "                author_info[author_name][\"first_created_at\"] = created_at\n",
    "                author_info[author_name][\"organizations\"].update(\n",
    "                organizations\n",
    "                )\n",
    "\n",
    "        # Convert the set of organizations back to a list for each author\n",
    "        for author in author_info.values():\n",
    "            author[\"organizations\"] = list(author[\"organizations\"])\n",
    "\n",
    "    return list(author_info.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd2308",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "is_mining_spam = True\n",
    "\n",
    "print(\"param:\", is_mining_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pr_data_pkl_filename = \"progress.pkl\" if is_mining_spam else \"non-spam-progress.pkl\"\n",
    "orgs_data_pkl_filename = \"user_orgs_progress.pkl\" if is_mining_spam else \"non-spam-user_orgs_progress.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if os.path.exists(orgs_data_pkl_filename):\n",
    "    with open(orgs_data_pkl_filename, \"rb\") as f:\n",
    "        progress_data = pickle.load(f)\n",
    "        df = progress_data[\"df\"]\n",
    "        start_index = progress_data[\"start_index\"]\n",
    "else:\n",
    "    df = []\n",
    "    start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b1763c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb89147-ab2c-4654-a8d7-637737c90537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from os import close\n",
    "import pickle\n",
    "\n",
    "with open(pr_data_pkl_filename, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    prs = data[\"df\"]\n",
    "users = get_unique_authors_with_organizations(prs)\n",
    "\n",
    "unique_organizations = set()\n",
    "for user in users:\n",
    "    for org in user[\"organizations\"]:\n",
    "        unique_organizations.add(org[\"login\"])\n",
    "\n",
    "index = start_index\n",
    "org_member_data = df\n",
    "for organization_login in unique_organizations:\n",
    "    try:\n",
    "        after_cursor = None\n",
    "        while True:\n",
    "            response = execute_query(\n",
    "                organization_login, first=10, after_cursor=after_cursor\n",
    "            )\n",
    "            if response.get(\"organization\") is None:\n",
    "                print(\n",
    "                    f\"Warning: organization with login {organization_login} not found, skipping...\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "            members_with_role = response[\"organization\"].get(\"membersWithRole\")\n",
    "            if members_with_role is None or members_with_role.get(\"totalCount\", 0) == 0:\n",
    "                print(\n",
    "                    f\"Warning: organization with login {organization_login} has no member, skipping...\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # Extract pr\n",
    "            for edge in members_with_role.get(\"edges\", []):\n",
    "                node = edge.get(\"node\", {})\n",
    "                df.append(\n",
    "                    {\n",
    "                        \"organization_login\": organization_login,\n",
    "                        \"member_login\": node.get(\"login\"),\n",
    "                        \"joined_at\": node.get(\"createdAt\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Pagination\n",
    "            page_info = response[\"organization\"][\"membersWithRole\"][\"pageInfo\"]\n",
    "            if page_info[\"hasNextPage\"]:\n",
    "                after_cursor = page_info[\"endCursor\"]\n",
    "            else:\n",
    "                break\n",
    "        with open(orgs_data_pkl_filename, \"wb\") as f:\n",
    "            pickle.dump({\"df\": org_member_data, \"start_index\": index + 1}, f)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve data for keywords '{user}': {e}\")\n",
    "        # Save progress before terminating\n",
    "        with open(orgs_data_pkl_filename, \"wb\") as f:\n",
    "            pickle.dump({\"df\": df, \"start_index\": index}, f)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87982a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_pkl_content(filepath):\n",
    "    \"\"\"\n",
    "    Display the content of a pickle file and save it as CSV and JSON files.\n",
    "    This function reads a pickle file from the given filepath, extracts the data,\n",
    "    and saves it as both a CSV and a JSON file. The CSV file is saved with the\n",
    "    filename 'user_orgs.csv' and the JSON file is saved with the filename 'user_orgs.json'.\n",
    "    Args:\n",
    "        filepath (str): The path to the pickle file to be read.\n",
    "    Raises:\n",
    "        Exception: If there is an error reading the pickle file or writing the CSV/JSON files,\n",
    "                   an exception is caught and an error message is printed.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        print(f\"Content of {filepath}:\\n\")\n",
    "        filename = \"user_orgs\" if is_mining_spam else \"non-spam-user_orgs\"\n",
    "        pd.DataFrame(data[\"df\"]).to_csv(f\"{filename}.csv\", index=True)\n",
    "        print(f\"Data written to {filename}.csv successfully.\")\n",
    "        try:\n",
    "            with open(f\"{filename}.json\", \"w\") as f:\n",
    "                json.dump(data[\"df\"], f, indent=4)\n",
    "            print(f\"Data written to {filename}.json successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "filepath = orgs_data_pkl_filename\n",
    "try:\n",
    "    display_pkl_content(filepath)\n",
    "except Exception as e:\n",
    "    print(f\"Error in cell: {e}\")\n",
    "    pass  # Skip the rest of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64756e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from fileinput import filename\n",
    "\n",
    "df = []\n",
    "\n",
    "try:\n",
    "    with open(pr_data_pkl_filename, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        prs = data[\"df\"]\n",
    "\n",
    "    with open(orgs_data_pkl_filename, \"rb\") as f:\n",
    "        org_data = pickle.load(f)\n",
    "        organizations_with_members = org_data[\"df\"]\n",
    "\n",
    "    # Grouping by 'member_login'\n",
    "    orgs_grouped_by_members = defaultdict(list)\n",
    "    for entry in organizations_with_members:\n",
    "        orgs_grouped_by_members[entry[\"member_login\"]].append(entry)\n",
    "\n",
    "    # Convert defaultdict to a normal dict\n",
    "    orgs_grouped_by_members = dict(orgs_grouped_by_members)\n",
    "\n",
    "    for pull_request in prs:\n",
    "        author_name = pull_request.get(\"author_name\")\n",
    "        author_account_created_at = pull_request.get(\"author_account_created_at\")\n",
    "        organizations_key = next(\n",
    "            (\n",
    "                key\n",
    "                for key in pull_request.keys()\n",
    "                if key.startswith(\"author_organizations_as_at\")\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        if author_name is None or author_account_created_at is None:\n",
    "            continue\n",
    "        df.append(\n",
    "            {\n",
    "                **pull_request,\n",
    "                f\"{organizations_key}\": orgs_grouped_by_members.get(author_name, []),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    filename = (\n",
    "        f\"{\"spam_data\" if is_mining_spam else \"non_spam_data\"}_with_org_join_date\"\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(df).to_csv(f\"{filename}.csv\", index=True)\n",
    "    print(f\"Data written to {filename}.csv successfully.\")\n",
    "\n",
    "    with open(f\"{filename}.json\", \"w\") as f:\n",
    "        json.dump(df, f, indent=4)\n",
    "    print(f\"Data written to {filename}.json successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
