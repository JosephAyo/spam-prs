import requests
import random
import itertools
import datetime
import json
import time
import csv

headers = {"Accept": "application/json"}

tokens_file = "./env/hackerone-tokens.txt"

with open(tokens_file, "r") as file:
    tokens = file.read().splitlines()

# Choose a random start index
start_index = random.randint(0, len(tokens) - 1)

rotated_tokens = tokens[start_index:] + tokens[:start_index]
token_iterator = itertools.cycle(rotated_tokens)

current_token = next(token_iterator)


def log_activity(activity: str):
    log = f"{datetime.datetime.now()}: {activity}\n"
    # print(log)
    with open(f"./datasets/fetch-cve-output.log", "a") as log_file:
        log_file.write(log)


report_ids = [
    2199174,
    2298307,
    2819666,
    2823554,
    2871792,
    2887487,
    2905552,
    2912277,
    2981245,
    3100073,
    3101127,
    3116935,
    3117697,
    3125820,
    3125832,
    3137657,
    3158093,
    3230082,
    3231321,
    3242005,
    3249936,
    3250490,
]

headers = {"Accept": "application/json"}

auth = ("painted_", current_token)  # Define current_token before running


# Step 1: Get HackerOne authenticated data
cve_reports = {}
for report_id in report_ids:
    try:
        r = requests.get(
            f"https://api.hackerone.com/v1/hackers/reports/{report_id}",
            auth=auth,
            headers=headers,
        )
        r.raise_for_status()
        response_data = r.json()
        cve_reports[report_id] = response_data.get("data", {})  # safely extract 'data'
        log_activity(f"[Authenticated] ✅ Report {report_id}")
    except Exception as e:
        log_activity(f"[Authenticated] ❌ Report {report_id}: {e}")

# Step 2: Retry until all public data fetched
direct_cve_reports = {}

missing_ids = set(report_ids)

while missing_ids:
    for report_id in list(missing_ids):
        try:
            r = requests.get(f"https://hackerone.com/reports/{report_id}.json")
            r.raise_for_status()
            direct_cve_reports[report_id] = r.json()
            missing_ids.remove(report_id)
            log_activity(f"[Public] ✅ Report {report_id}")
        except Exception as e:
            log_activity(f"[Public] ❌ Report {report_id}: {e}")
    if missing_ids:
        log_activity(f"Retrying for missing IDs: {missing_ids}")
        time.sleep(3)  # wait before retrying

# Step 3: Merge based on "vulnerability_information"
merged_reports = []

for report_id in report_ids:
    base = cve_reports.get(report_id, {})
    extra = direct_cve_reports.get(report_id, {})

    if "vulnerability_information" in extra:
        base["vulnerability_information"] = extra["vulnerability_information"]

    merged_reports.append(base)

# Step 4: Save final merged report
final_merged_report_file = "./datasets/merged_cve_reports.json"
with open(final_merged_report_file, "w", encoding="utf-8") as f:
    json.dump(merged_reports, f, indent=2)

log_activity(f"✅ All merged data saved to {final_merged_report_file}")


csv_file = "./datasets/merged_cve_reports.csv"

# Define the fields you want to include in the CSV
# You can expand this list based on what's important to you
fields = [
    "id",
    "type",
    "attributes.title",
    "attributes.state",
    "vulnerability_information",
]

def extract_field(report, field_path):
    """Safely extract nested fields like 'data.type'"""
    parts = field_path.split(".")
    value = report
    for part in parts:
        if isinstance(value, dict):
            value = value.get(part, "")
        else:
            return ""
    return value

# Write CSV
with open(csv_file, "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(fields)  # write header

    for report in merged_reports:
        row = [extract_field(report, field) for field in fields]
        writer.writerow(row)
