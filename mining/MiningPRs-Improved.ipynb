{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad6a4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "REPO_OWNER = \"JabRef\"\n",
    "REPO_NAME = \"jabref\"\n",
    "REPOSITORY = f\"{REPO_OWNER}/{REPO_NAME}\"\n",
    "\n",
    "# Date format: YYYY-MM-DDTHH:MM (e.g. 2025-07-31T00:00)\n",
    "# Start date for the mining process\n",
    "START_DATE = \"2025-07-31T00:00\"\n",
    "# End date (Optional). Set to None to use the current time.\n",
    "END_DATE = \"2025-08-01T00:00\" \n",
    "\n",
    "# File paths\n",
    "# Assuming this notebook is in /mining, and env is in the root\n",
    "TOKENS_FILE = \"../env/tokens.txt\" \n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "STATE_FILE = os.path.join(OUTPUT_DIR, f\"{REPO_NAME}_mining_state.pkl\")\n",
    "LOG_FILE = os.path.join(OUTPUT_DIR, f\"{REPO_NAME}_mining.log\")\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_DIR, f\"{REPO_NAME}_prs.csv\")\n",
    "OUTPUT_JSON = os.path.join(OUTPUT_DIR, f\"{REPO_NAME}_prs.json\")\n",
    "\n",
    "# Tuning\n",
    "DEFAULT_DAYS_INTERVAL = 60\n",
    "MAX_RESULTS_PER_QUERY = 1000 # GitHub limit is 1000\n",
    "SAFETY_THRESHOLD = 950 # Reduce interval if results > this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "527543c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 tokens.\n"
     ]
    }
   ],
   "source": [
    "# --- Token Management ---\n",
    "\n",
    "def load_tokens(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Tokens file not found at {filepath}\")\n",
    "    with open(filepath, \"r\") as file:\n",
    "        tokens = [t.strip() for t in file.read().splitlines() if t.strip()]\n",
    "    if not tokens:\n",
    "        raise ValueError(\"No tokens found in file.\")\n",
    "    return tokens\n",
    "\n",
    "tokens = load_tokens(TOKENS_FILE)\n",
    "token_iterator = itertools.cycle(tokens)\n",
    "current_token = next(token_iterator)\n",
    "\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "]\n",
    "\n",
    "def get_headers(token):\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"User-Agent\": random.choice(user_agents),\n",
    "    }\n",
    "\n",
    "# Setup GraphQL client\n",
    "graphql_url = \"https://api.github.com/graphql\"\n",
    "transport = RequestsHTTPTransport(url=graphql_url, headers=get_headers(current_token), use_json=True)\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "def log_activity(activity: str):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {activity}\\n\"\n",
    "    print(activity)\n",
    "    with open(LOG_FILE, \"a\") as log_file:\n",
    "        log_file.write(log_entry)\n",
    "\n",
    "log_activity(f\"Loaded {len(tokens)} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc50a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GraphQL Queries ---\n",
    "\n",
    "# Rate limit query\n",
    "rate_limit_query = gql(\n",
    "    \"\"\"\n",
    "    query {\n",
    "      rateLimit {\n",
    "        limit\n",
    "        remaining\n",
    "        resetAt\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Main PR query\n",
    "query_template = gql(\n",
    "    \"\"\"\n",
    "    query searchIssues($keyword: String!, $afterCursor: String, $first: Int) {\n",
    "      search(query: $keyword, type: ISSUE, first: $first, after: $afterCursor) {\n",
    "        issueCount\n",
    "        edges {\n",
    "          cursor\n",
    "          node {\n",
    "            ... on PullRequest {\n",
    "              id\n",
    "              number\n",
    "              title\n",
    "              url\n",
    "              comments {\n",
    "                totalCount\n",
    "              }\n",
    "              state\n",
    "              closed\n",
    "              merged\n",
    "              createdAt\n",
    "              updatedAt\n",
    "              mergeCommit {\n",
    "                oid\n",
    "              }\n",
    "              timeline(last: 100) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    __typename\n",
    "                    ... on ClosedEvent { \n",
    "                      actor {   \n",
    "                        ... on User { login url }\n",
    "                        ... on Mannequin { login url }\n",
    "                        ... on Bot { login url }\n",
    "                      }\n",
    "                      createdAt\n",
    "                    }\n",
    "                    ... on MergedEvent { \n",
    "                      actor {   \n",
    "                        ... on User { login url }\n",
    "                        ... on Mannequin { login url }\n",
    "                        ... on Bot { login url }\n",
    "                      }\n",
    "                      createdAt\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              commits {\n",
    "                totalCount\n",
    "              }\n",
    "              changedFiles\n",
    "              additions\n",
    "              deletions\n",
    "              headRefName\n",
    "              baseRefName\n",
    "              repository {\n",
    "                id\n",
    "                nameWithOwner\n",
    "                stargazerCount\n",
    "                description\n",
    "                isFork\n",
    "                languages(first: 20) {\n",
    "                  edges {\n",
    "                    node {\n",
    "                      name\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              author {\n",
    "                 ... on User { login url }\n",
    "                 ... on Mannequin { login url }\n",
    "                 ... on Bot { login url }\n",
    "              }\n",
    "              labels(first: 20) {\n",
    "                edges {\n",
    "                  node {\n",
    "                    name\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "              body\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        pageInfo {\n",
    "          endCursor\n",
    "          hasNextPage\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33bf7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def check_and_rotate_token():\n",
    "    global current_token, transport, client\n",
    "    \n",
    "    try:\n",
    "        # Check current token\n",
    "        transport.headers = get_headers(current_token)\n",
    "        response = client.execute(rate_limit_query)\n",
    "        remaining = response[\"rateLimit\"][\"remaining\"]\n",
    "        \n",
    "        if remaining < 100:\n",
    "            log_activity(f\"Token rate limit low ({remaining}). Rotating...\")\n",
    "            \n",
    "            # Try finding a good token\n",
    "            initial_token = current_token\n",
    "            while True:\n",
    "                current_token = next(token_iterator)\n",
    "                transport.headers = get_headers(current_token)\n",
    "                \n",
    "                try:\n",
    "                    resp = client.execute(rate_limit_query)\n",
    "                    rem = resp[\"rateLimit\"][\"remaining\"]\n",
    "                    if rem >= 100:\n",
    "                        log_activity(f\"Switched to token with {rem} remaining.\")\n",
    "                        return\n",
    "                except Exception as e:\n",
    "                    if \"Bad credentials\" in str(e) or \"401\" in str(e):\n",
    "                         log_activity(f\"Token invalid (skipped): {current_token[:10]}...\")\n",
    "                    pass # Token might be bad, skip\n",
    "                \n",
    "                if current_token == initial_token:\n",
    "                    log_activity(\"All tokens exhausted. Sleeping for 30 minutes...\")\n",
    "                    time.sleep(1800)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        if \"Bad credentials\" in str(e) or \"401\" in str(e):\n",
    "            log_activity(f\"Current token invalid: {e}. Rotating...\")\n",
    "        else:\n",
    "            log_activity(f\"Error checking rate limit: {e}. Rotating token.\")\n",
    "        \n",
    "        current_token = next(token_iterator)\n",
    "        transport.headers = get_headers(current_token)\n",
    "\n",
    "def execute_query_safe(keyword, first=100, after_cursor=None):\n",
    "    \"\"\"Executes query with retries and token rotation.\"\"\"\n",
    "    global current_token\n",
    "    \n",
    "    retries = 0\n",
    "    max_retries = 5\n",
    "    \n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            check_and_rotate_token()\n",
    "            \n",
    "            return client.execute(\n",
    "                query_template,\n",
    "                variable_values={\n",
    "                    \"keyword\": keyword,\n",
    "                    \"first\": first,\n",
    "                    \"afterCursor\": after_cursor,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log_activity(f\"Query failed: {e}\")\n",
    "            if \"API rate limit\" in str(e) or \"502\" in str(e) or \"500\" in str(e) or \"Bad credentials\" in str(e) or \"401\" in str(e):\n",
    "                log_activity(\"Rate limit, auth error, or server error. Rotating/Sleeping...\")\n",
    "                current_token = next(token_iterator)\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                # Reduce page size if it's a complexity issue\n",
    "                if first > 10:\n",
    "                    first = int(first / 2)\n",
    "                    log_activity(f\"Reducing page size to {first}\")\n",
    "                else:\n",
    "                    log_activity(\"Persistent error. Skipping...\")\n",
    "                    raise e\n",
    "            retries += 1\n",
    "            \n",
    "    raise Exception(\"Max retries reached\")\n",
    "\n",
    "def process_pr_node(node, repository_name):\n",
    "    \"\"\"Extracts relevant fields from the raw GraphQL node.\"\"\"\n",
    "    if not node: return None\n",
    "    \n",
    "    timeline = node.get(\"timeline\", {}).get(\"edges\", [])\n",
    "    \n",
    "    # Helper to find event\n",
    "    def find_event(typename):\n",
    "        return next((x[\"node\"] for x in timeline if x[\"node\"] and x[\"node\"][\"__typename\"] == typename), None)\n",
    "\n",
    "    closed_event = find_event(\"ClosedEvent\")\n",
    "    merged_event = find_event(\"MergedEvent\")\n",
    "    \n",
    "    author = node.get(\"author\") or {}\n",
    "    repo = node.get(\"repository\") or {}\n",
    "    \n",
    "    return {\n",
    "        \"id\": node.get(\"id\"),\n",
    "        \"pull_number\": node.get(\"number\"),\n",
    "        \"title\": node.get(\"title\"),\n",
    "        \"url\": node.get(\"url\"),\n",
    "        \"state\": node.get(\"state\"),\n",
    "        \"comments_count\": node.get(\"comments\", {}).get(\"totalCount\", 0),\n",
    "        \"closed\": node.get(\"closed\"),\n",
    "        \"closed_by\": closed_event.get(\"actor\", {}).get(\"login\") if closed_event and closed_event.get(\"actor\") else None,\n",
    "        \"closed_at\": closed_event.get(\"createdAt\") if closed_event else None,\n",
    "        \"merged\": node.get(\"merged\"),\n",
    "        \"merged_by\": merged_event.get(\"actor\", {}).get(\"login\") if merged_event and merged_event.get(\"actor\") else None,\n",
    "        \"merged_at\": merged_event.get(\"createdAt\") if merged_event else None,\n",
    "        \"body\": node.get(\"body\"),\n",
    "        \"created_at\": node.get(\"createdAt\"),\n",
    "        \"updated_at\": node.get(\"updatedAt\"),\n",
    "        \"repository_name_with_owner\": repo.get(\"nameWithOwner\"),\n",
    "        \"repository_stargazer_count\": repo.get(\"stargazerCount\"),\n",
    "        \"repository_is_fork\": repo.get(\"isFork\"),\n",
    "        \"repository_languages\": [l[\"node\"][\"name\"] for l in repo.get(\"languages\", {}).get(\"edges\", []) if l.get(\"node\")],\n",
    "        \"merge_commit_oid\": node.get(\"mergeCommit\", {}).get(\"oid\") if node.get(\"mergeCommit\") else None,\n",
    "        \"labels\": [l[\"node\"][\"name\"] for l in node.get(\"labels\", {}).get(\"edges\", []) if l.get(\"node\")],\n",
    "        \"commits_count\": node.get(\"commits\", {}).get(\"totalCount\", 0),\n",
    "        \"changed_files_count\": node.get(\"changedFiles\"),\n",
    "        \"additions_count\": node.get(\"additions\"),\n",
    "        \"deletions_count\": node.get(\"deletions\"),\n",
    "        \"author_name\": author.get(\"login\"),\n",
    "        \"author_url\": author.get(\"url\"),\n",
    "        \"search_repository\": repository_name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34ae4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- State Management ---\n",
    "\n",
    "def load_state():\n",
    "    if os.path.exists(STATE_FILE):\n",
    "        try:\n",
    "            with open(STATE_FILE, \"rb\") as f:\n",
    "                state = pickle.load(f)\n",
    "            log_activity(f\"Resuming from state. Collected {len(state.get('data', []))} PRs so far.\")\n",
    "            log_activity(f\"Last processed date: {state.get('last_date')}\")\n",
    "            return state\n",
    "        except Exception as e:\n",
    "            log_activity(f\"Error loading state: {e}. Starting fresh.\")\n",
    "    \n",
    "    return {\n",
    "        \"data\": [],\n",
    "        \"last_date\": START_DATE,\n",
    "        \"params\": {\"repo\": REPOSITORY}\n",
    "    }\n",
    "\n",
    "def save_state(data, last_date):\n",
    "    state = {\n",
    "        \"data\": data,\n",
    "        \"last_date\": last_date,\n",
    "        \"params\": {\"repo\": REPOSITORY},\n",
    "        \"updated_at\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "    with open(STATE_FILE, \"wb\") as f:\n",
    "        pickle.dump(state, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "422ee9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting mining from 2025-07-31 00:00:00 to 2025-08-01 00:00:00\n",
      "Fetching range: 2025-07-31T00:00..2025-08-01T00:00\n",
      "Found 2 PRs in range.\n",
      "Found 2 PRs in range.\n",
      "Mining completed.\n",
      "Mining completed.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Mining Loop ---\n",
    "\n",
    "def mine_prs():\n",
    "    state = load_state()\n",
    "    all_prs = state[\"data\"]\n",
    "    \n",
    "    # Determine start and end times\n",
    "    current_start_str = state[\"last_date\"]\n",
    "    start_dt = datetime.datetime.strptime(current_start_str, \"%Y-%m-%dT%H:%M\")\n",
    "    \n",
    "    if END_DATE:\n",
    "        end_dt = datetime.datetime.strptime(END_DATE, \"%Y-%m-%dT%H:%M\")\n",
    "    else:\n",
    "        end_dt = datetime.datetime.now()\n",
    "        \n",
    "    days_interval = DEFAULT_DAYS_INTERVAL\n",
    "    \n",
    "    log_activity(f\"Starting mining from {start_dt} to {end_dt}\")\n",
    "    \n",
    "    while start_dt < end_dt:\n",
    "        # Calculate next date\n",
    "        next_dt = start_dt + datetime.timedelta(days=days_interval)\n",
    "        if next_dt > end_dt:\n",
    "            next_dt = end_dt\n",
    "            \n",
    "        date_range_str = f\"{start_dt.strftime('%Y-%m-%dT%H:%M')}..{next_dt.strftime('%Y-%m-%dT%H:%M')}\"\n",
    "        search_query = f\"repo:{REPOSITORY} is:pr is:public archived:false created:{date_range_str}\"\n",
    "        \n",
    "        log_activity(f\"Fetching range: {date_range_str}\")\n",
    "        \n",
    "        try:\n",
    "            # Fetch first page to check count\n",
    "            response = execute_query_safe(search_query, first=1)\n",
    "            total_count = response[\"search\"][\"issueCount\"]\n",
    "            log_activity(f\"Found {total_count} PRs in range.\")\n",
    "            \n",
    "            # Adjust interval if too many results\n",
    "            if total_count > SAFETY_THRESHOLD:\n",
    "                new_interval = max(1, days_interval // 2)\n",
    "                if new_interval < days_interval:\n",
    "                    log_activity(f\"Too many results ({total_count}). Reducing interval from {days_interval} to {new_interval} days.\")\n",
    "                    days_interval = new_interval\n",
    "                    continue # Retry with smaller interval\n",
    "            \n",
    "            # If count is good, fetch all pages\n",
    "            after_cursor = None\n",
    "            page_count = 0\n",
    "            \n",
    "            while True:\n",
    "                response = execute_query_safe(search_query, first=100, after_cursor=after_cursor)\n",
    "                edges = response[\"search\"][\"edges\"]\n",
    "                \n",
    "                if not edges:\n",
    "                    break\n",
    "                    \n",
    "                for edge in edges:\n",
    "                    pr_data = process_pr_node(edge[\"node\"], REPOSITORY)\n",
    "                    if pr_data:\n",
    "                        all_prs.append(pr_data)\n",
    "                \n",
    "                page_info = response[\"search\"][\"pageInfo\"]\n",
    "                if page_info[\"hasNextPage\"]:\n",
    "                    after_cursor = page_info[\"endCursor\"]\n",
    "                    page_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Successful interval\n",
    "            # Save state\n",
    "            save_state(all_prs, next_dt.strftime('%Y-%m-%dT%H:%M'))\n",
    "            \n",
    "            # Advance time\n",
    "            start_dt = next_dt\n",
    "            \n",
    "            # Try to increase interval back to default if it was reduced\n",
    "            if days_interval < DEFAULT_DAYS_INTERVAL:\n",
    "                days_interval = min(DEFAULT_DAYS_INTERVAL, days_interval * 2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            log_activity(f\"Critical error in loop: {e}\")\n",
    "            time.sleep(60) # Wait a bit before retrying same block or manual intervention\n",
    "            # We do NOT advance start_dt here, so it retries the same block next time\n",
    "            raise e\n",
    "\n",
    "    log_activity(\"Mining completed.\")\n",
    "    return all_prs\n",
    "\n",
    "# Run the miner\n",
    "collected_data = mine_prs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "697d4f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicated: 2 -> 2 records.\n",
      "Saved to outputs/jabref_prs.csv\n",
      "Saved to outputs/jabref_prs.json\n"
     ]
    }
   ],
   "source": [
    "# --- Post-Processing & Saving ---\n",
    "\n",
    "def save_results(data):\n",
    "    if not data:\n",
    "        log_activity(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Deduplicate by ID\n",
    "    initial_len = len(df)\n",
    "    df = df.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "    log_activity(f\"Deduplicated: {initial_len} -> {len(df)} records.\")\n",
    "    \n",
    "    # Save CSV\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    log_activity(f\"Saved to {OUTPUT_CSV}\")\n",
    "    \n",
    "    # Save JSON\n",
    "    df.to_json(OUTPUT_JSON, orient=\"records\", indent=4)\n",
    "    log_activity(f\"Saved to {OUTPUT_JSON}\")\n",
    "\n",
    "save_results(collected_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
